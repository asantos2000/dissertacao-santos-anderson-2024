TY  - JOUR
T1  - Analysis of Ontology Competency Questions and their formalizations in SPARQL-OWL
AU  - Wiśniewski, Dawid
AU  - Potoniec, Jedrzej
AU  - Ławrynowicz, Agnieszka
AU  - Keet, C. Maria
JO  - Journal of Web Semantics
VL  - 59
SP  - 100534
PY  - 2019
DA  - 2019/12/01/
SN  - 1570-8268
DO  - https://doi.org/10.1016/j.websem.2019.100534
UR  - https://www.sciencedirect.com/science/article/pii/S1570826819300617
KW  - Ontology Authoring
KW  - Competency Questions
KW  - SPARQL-OWL
AB  - Competency Questions (CQs) are natural language questions outlining and constraining the scope of knowledge represented in an ontology. Despite that CQs are a part of several ontology engineering methodologies, the actual publication of CQs for the available ontologies is very limited and even scarcer is the publication of their respective formalizations in terms of, e.g., SPARQL queries. This paper aims to contribute to addressing the myriad of engineering hurdles to using CQs in ontology development. A prerequisite to this is to understand the relation between CQs and the queries over the ontology. We use a new dataset of 234 competency questions and their SPARQL-OWL queries for several ontologies in different domains developed by different groups, and analysed the CQs in two principal ways. The first stage focused on a linguistic analysis of the natural language text itself, i.e., a lexico-syntactic analysis without any presuppositions of ontology elements, and a subsequent step of semantic analysis in order to find patterns. This increased diversity of CQ sources resulted in a 4-5-fold increase of hitherto published patterns, to 106 distinct CQ patterns, which have a limited subset of few patterns shared across the CQ sets from the different ontologies. Next, we analysed the relation between the found CQ patterns and their respective SPARQL-OWL patterns, which revealed that one CQ pattern may be realized by more than one SPARQL-OWL query pattern, and vice versa. These insights may contribute to establishing common practices, templates, automation, and user tools that will support CQ formulation, formalization, execution, and general management.
ER  - 

TY  - JOUR
T1  - Scalable aggregate keyword query over knowledge graph
AU  - Hu, Xin
AU  - Duan, Jiangli
AU  - Dang, Depeng
JO  - Future Generation Computer Systems
VL  - 107
SP  - 588
EP  - 600
PY  - 2020
DA  - 2020/06/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2020.02.011
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X19300251
KW  - Knowledge graph
KW  - Question answering
KW  - Keyword search
KW  - Aggregation
AB  - Existing keyword query systems over knowledge graphs are easy to use and can produce interesting results. However, they cannot address even simple aggregate queries (i.e., a query that needs statistics such as COUNT, SUM, AVG, MAX, MIN, >, < and =), and the sizes of existing schema graphs grow exponentially with the growth of the number of types or predicates in the knowledge graph, so that they have low scalability for building SPARQL statements. Therefore, we propose a framework called SAKQ (scalable aggregate keyword query over knowledge graph) that enables users to pose aggregate queries using simple keywords. First, we propose a scalable schema graph (i.e., type-predicate graph) that consists of the relationships between types and predicates, which has a small data size and contains all information needed for building SPARQL statements. Second, based on the type-predicate graph, we propose two algorithms to build query graphs with aggregation and transform the query graphs into SPARQL statements with aggregation. Finally, the experimental results over the benchmark datasets demonstrate that SAKQ can answer various general aggregate keyword queries.
ER  - 

TY  - JOUR
T1  - XPlaM: A toolkit for automating the acquisition of BDI agent-based Digital Twins of organizations
AU  - Alelaimat, Ahmad
AU  - Ghose, Aditya
AU  - Dam, Hoa Khanh
JO  - Computers in Industry
VL  - 145
SP  - 103805
PY  - 2023
DA  - 2023/02/01/
SN  - 0166-3615
DO  - https://doi.org/10.1016/j.compind.2022.103805
UR  - https://www.sciencedirect.com/science/article/pii/S0166361522002019
KW  - Digital Twin
KW  - BDI agents
KW  - Abductive reasoning
AB  - A Digital Twin ideally manifests the same behaviour (in silico) of its physical counterpart. While considerable attention has been paid to the development of Digital Twins for physical devices/systems, the question of developing Twins for organizations has received relatively little attention. The setting in which we address this problem is very general and, consequently, very challenging. We look at the automatic acquisition of Digital Twins of organizations. To that end, this paper builds on the following two premises: (1) that Digital Twins of organizations can provide value, and (2) that Belief-Desire-Intention (BDI) agents are a particularly effective means for representing Digital Twins. The overall approach is to leverage the externally observable behaviour of the target system and then generate candidate BDI agent programs that best explain (in the sense of formal abduction) the observed behaviour. The candidate agent programs are generated by searching through potentially large hypotheses spaces for possible plans, selection functions and beliefs. The resulting approach suggests that using abduction to generate Digital Twins of organizations in the form of BDI agents can be effective.
ER  - 

TY  - JOUR
T1  - Using long vector extensions for MPI reductions
AU  - Zhong, Dong
AU  - Cao, Qinglei
AU  - Bosilca, George
AU  - Dongarra, Jack
JO  - Parallel Computing
VL  - 109
SP  - 102871
PY  - 2022
DA  - 2022/03/01/
SN  - 0167-8191
DO  - https://doi.org/10.1016/j.parco.2021.102871
UR  - https://www.sciencedirect.com/science/article/pii/S0167819121001137
KW  - Long vector extension
KW  - Vector operation
KW  - Intel AVX2/AVX-512
KW  - Instruction level parallelism
KW  - Single instruction multiple data
KW  - MPI reduction operation
KW  - Scalable Vector Extension (SVE)
AB  - The modern CPU’s design, including the deep memory hierarchies and SIMD/vectorization capability have a more significant impact on algorithms’ efficiency than the modest frequency increase observed recently. The current introduction of wide vector instruction set extensions (AVX and SVE) motivated vectorization to become a critical software component to increase efficiency and close the gap to peak performance. In this paper, we investigate the impact of the vectorization of MPI reduction operations. We propose an implementation of predefined MPI reduction operations using vector intrinsics (AVX and SVE) to improve the time-to-solution of the predefined MPI reduction operations. The evaluation of the resulting software stack under different scenarios demonstrates that the approach is not only efficient but also generalizable to many vector architectures. Experiments conducted on varied architectures (Intel Xeon Gold, AMD Zen 2, and Arm A64FX), show that the proposed vector extension optimized reduction operations significantly reduce completion time for collective communication reductions. With these optimizations, we achieve higher memory bandwidth and an increased efficiency for local computations, which directly benefit the overall cost of collective reductions and applications based on them.
ER  - 

TY  - JOUR
T1  - Natural language processing-enhanced extraction of SBVR business vocabularies and business rules from UML use case diagrams
AU  - Danenas, Paulius
AU  - Skersys, Tomas
AU  - Butleris, Rimantas
JO  - Data & Knowledge Engineering
VL  - 128
SP  - 101822
PY  - 2020
DA  - 2020/07/01/
SN  - 0169-023X
DO  - https://doi.org/10.1016/j.datak.2020.101822
UR  - https://www.sciencedirect.com/science/article/pii/S0169023X1930299X
KW  - SBVR business vocabulary and rules
KW  - UML use case diagram
KW  - Model-to-model transformation
KW  - Controlled natural language
KW  - Natural language processing
KW  - Information extraction
AB  - Discovery, specification and proper representation of various aspects of business knowledge plays crucial part in model-driven information systems engineering, especially when it comes to the early stages of systems development. Being among the most applicable and advanced features of model-driven development, model transformation could help improving one of the most time- and resource-consuming efforts in this process, namely, discovery and specification of business vocabularies and business rules within the problem domain. One of our latest developments in this area was the solution for the automatic extraction of SBVR business vocabularies and business rules from UML use case diagrams, which was arguably one of the most comprehensive developments of this kind currently available in public. In this paper, we present an enhancement to our previous development by introducing a novel natural language processing component to it. This enhancement provides more advanced extraction capabilities (such as recognition of entities, entire noun and verb phrases, multinary associations) and better quality of the extraction results compared to our previous solution. The main contributions presented in this paper are pre- and post-processing algorithms, and two extraction algorithms using custom-trained POS tagger. Based on the related work findings, it is safe to state that the presented solution is novel and original in its approach of combining together M2M transformation of UML and SBVR models with natural language processing techniques in the field of model-driven information systems engineering.
ER  - 

TY  - CHAP
T1  - Index
A2  - Tiwari, Sanju
A2  - Ortiz Rodriguez, Fernando
A2  - Jabbar, M.A.
BT  - Semantic Models in IoT and eHealth Applications
PB  - Academic Press
SP  - 267
EP  - 273
PY  - 2022
DA  - 2022/01/01/
T2  - Intelligent Data-Centric Systems
SN  - 978-0-323-91773-5
DO  - https://doi.org/10.1016/B978-0-32-391773-5.00019-4
UR  - https://www.sciencedirect.com/science/article/pii/B9780323917735000194
ER  - 

TY  - JOUR
T1  - Using ontologies for life science text-based resource organization
AU  - Panzarella, Giulia
AU  - Veltri, Pierangelo
AU  - Alcaro, Stefano
JO  - Artificial Intelligence in the Life Sciences
VL  - 3
SP  - 100059
PY  - 2023
DA  - 2023/12/01/
SN  - 2667-3185
DO  - https://doi.org/10.1016/j.ailsci.2023.100059
UR  - https://www.sciencedirect.com/science/article/pii/S266731852300003X
KW  - Information overload
KW  - Ontology
KW  - Semantic web
KW  - Life science terms
AB  - Ontologies are used to support access to a multitude of databases that cover domains relevant information. Heterogeneity and different semantics can be accessed by using structured texts and descriptions in a hierarchical concept definition. We are interested in Life Sciences (LS) related ontologies including components taken from molecular biology, bioinformatics, physics, chemistry, medicine and other related areas. An Ontology comprises: (i) term connections, (ii) the identification of core concepts, (iii) data management, (iv) knowledge classification and integration to collect key information. An ontology may be very useful in navigating through LS terms. This paper explores some available biomedical ontologies and frameworks. It describes the most common ontology development environments (ODE): Protégé, Topbraid Composer, Ontostudio, Fluent Editor, VocBench, Swoop and Obo-edit, to create ontologies from textual scientific resources for LS plans. It also compares ontology methodologies in terms of Usability, Scalability, Stability, Integration, Documentation and Originality.
ER  - 

TY  - CHAP
T1  - Chapter 3 - Key Concepts
AU  - McGilvray, Danette
A2  - McGilvray, Danette
BT  - Executing Data Quality Projects (Second Edition)
PB  - Academic Press
SP  - 29
EP  - 72
PY  - 2021
DA  - 2021/01/01/
SN  - 978-0-12-818015-0
DO  - https://doi.org/10.1016/B978-0-12-818015-0.00009-8
UR  - https://www.sciencedirect.com/science/article/pii/B9780128180150000098
KW  - Framework for Information Quality (FIQ)
KW  - information life cycle
KW  - data life cycle
KW  - lineage
KW  - data quality dimensions
KW  - business impact techniques
KW  - data categories
KW  - master data
KW  - data specifications
KW  - metadata
KW  - data standards
KW  - reference data
KW  - data models
KW  - business rules
KW  - data governance
KW  - data stewardship
KW  - Ten Steps Process
KW  - data quality improvement cycle
AB  - This chapter introduces fundamental ideas, the understanding of which, will aid data quality work and use of the Ten Steps Process. Information, like financial and human resources, must be properly managed throughout its life cycle to get the full use and benefit from it, so the information life cycle is discussed with the acronym POSMAD as an easy way to remember the six phases of the information life cycle: Plan, Obtain, Store and Share, Maintain, Apply, and Dispose. POSMAD plus several additional concepts are summarized in the Framework for Information Quality (FIQ), which provides an at-a-glance view of the components necessary to have high quality information. Other concepts central to understanding and managing data are discussed, such as data quality dimensions, business impact techniques, data categories, and data specifications (with a focus on metadata, data standards, reference data, data models, and business rules), data governance and stewardship. An overview of each of the steps in the Ten Steps Process is given, along with their relationship to the concepts.
ER  - 

TY  - JOUR
T1  - An integrated socio-technical enterprise modelling: A scenario of healthcare system analysis and design
AU  - Fayoumi, Amjad
AU  - Williams, Richard
JO  - Journal of Industrial Information Integration
VL  - 23
SP  - 100221
PY  - 2021
DA  - 2021/09/01/
SN  - 2452-414X
DO  - https://doi.org/10.1016/j.jii.2021.100221
UR  - https://www.sciencedirect.com/science/article/pii/S2452414X21000212
KW  - Enterprise Modelling
KW  - Socio-technical Systems
KW  - Enterprise Integrated Model
KW  - Conceptual Modelling
KW  - Healthcare System
AB  - One of the crucial issues facing enterprise modelling (EM) practices is that EM is considered technical, and rarely or never has a social focus. Social aspects referred to here are the soft aspects of the organisation that lead to organic organisation development (communication, collaboration, culture, skills and personal goals). There are many EM approaches and enterprise architecture frameworks were proposed recently. These cover different enterprise aspects, perspectives, artefacts and models with different qualities and levels of details. Yet, the imperative determination has overlaid the declarative exploration in EM as a necessity of the design effort. Rethinking the assumptions underlying EM should bring a new and different understanding on how EM can be tackled within the enterprise, in particular the joint development and optimisation of socio-technical systems. This paper discusses EM from a socio-technical systems (STS) perspective, and towards forming a new model of EM that is driven from STS theory and combined with STS practices. Then proposes a conceptual integrated model that incorporates the new concepts of STS toward building an EM framework for balanced socio-technical joint development and optimisation. The approach is illustrated in a scenario from healthcare industry. A combination between modelling and STS practices proved powerful for holistic IT modernisation, future work discussed toward the end of the paper.
ER  - 

TY  - JOUR
T1  - The application of Software Defined Networking on securing computer networks: A survey
AU  - Sahay, Rishikesh
AU  - Meng, Weizhi
AU  - Jensen, Christian D.
JO  - Journal of Network and Computer Applications
VL  - 131
SP  - 89
EP  - 108
PY  - 2019
DA  - 2019/04/01/
SN  - 1084-8045
DO  - https://doi.org/10.1016/j.jnca.2019.01.019
UR  - https://www.sciencedirect.com/science/article/pii/S108480451930027X
KW  - Software Defined Networking
KW  - Attack detection and mitigation
KW  - Network security
KW  - Middlebox management
KW  - Traffic management
KW  - Policy management
KW  - Traffic engineering
KW  - Smart grid security
AB  - Software Defined Networking (SDN) has emerged as a new networking paradigm for managing different kinds of networks ranging from enterprise to home network through software enabled control. The logically centralized control plane and programmability offers a great opportunity to improve network security, like implementing new mechanisms to detect and mitigate various threats, as well as enables deploying security as a service on the SDN controller. Due to the increasing and fast development of SDN, this paper provides an extensive survey on the application of SDN on enhancing the security of computer networks. In particular, we survey recent research studies that focus on applying SDN for network security including attack detection and mitigation, traffic monitoring and engineering, configuration and policy management, service chaining, and middlebox deployment, in addition to smart grid security. We further identify some challenges and promising future directions on SDN security, compatibility and scalability issues that should be addressed in this field.
ER  - 

TY  - JOUR
T1  - The formal verification of the ctm approach to forcing
AU  - Gunther, Emmanuel
AU  - Pagano, Miguel
AU  - Sánchez Terraf, Pedro
AU  - Steinberg, Matías
JO  - Annals of Pure and Applied Logic
VL  - 175
IS  - 5
SP  - 103413
PY  - 2024
DA  - 2024/05/01/
SN  - 0168-0072
DO  - https://doi.org/10.1016/j.apal.2024.103413
UR  - https://www.sciencedirect.com/science/article/pii/S0168007224000101
KW  - Isabelle/ZF
KW  - Countable transitive models
KW  - Continuum hypothesis
KW  - Proof assistants
KW  - Interactive theorem provers
KW  - Generic extension
AB  - We discuss some highlights of our computer-verified proof of the construction, given a countable transitive set-model M of ZFC, of generic extensions satisfying ZFC+¬CH and ZFC+CH. Moreover, let R be the set of instances of the Axiom of Replacement. We isolated a 21-element subset Ω⊆R and defined F:R→R such that for every Φ⊆R and M-generic G, M⊨ZC∪F“Φ∪Ω implies M[G]⊨ZC∪Φ∪{¬CH}, where ZC is Zermelo set theory with Choice. To achieve this, we worked in the proof assistant Isabelle, basing our development on the Isabelle/ZF library by L. Paulson and others.
ER  - 

TY  - JOUR
T1  - Learning software requirements syntax: An unsupervised approach to recognize templates
AU  - Sonbol, Riad
AU  - Rebdawi, Ghaida
AU  - Ghneim, Nada
JO  - Knowledge-Based Systems
VL  - 248
SP  - 108933
PY  - 2022
DA  - 2022/07/19/
SN  - 0950-7051
DO  - https://doi.org/10.1016/j.knosys.2022.108933
UR  - https://www.sciencedirect.com/science/article/pii/S0950705122004518
KW  - Requirements Engineering
KW  - Requirements templates recognition
KW  - Natural Language Processing (NLP)
KW  - Syntax learning
KW  - Graph community detection
AB  - Requirements are textual representations of the desired software capabilities. Many templates have been used to standardize the structure of requirement statements such as Rupps, EARS, and User Stories. Templates provide a good solution to improve different Requirements Engineering (RE) tasks since their well-defined syntax facilitates the different text processing steps in RE automation researches. However, many empirical studies have concluded that there is a gap between these RE researches and their implementation in industrial and real-life projects. The success of RE automation approaches strongly depends on the consistency of the requirements with the syntax of the predefined templates. Such consistency cannot be guaranteed in real projects, especially in large development projects, or when one has little control over the requirements authoring environment. In this paper, we propose an unsupervised approach to recognize templates from the requirements themselves by extracting their common syntactic structures. The resultant templates reflect the actual syntactic structure of requirements; hence it can recognize both standard and non-standard templates. Our approach uses techniques from Natural Language Processing and Graph Theory to handle this problem through three main stages (1) we formulate the problem as a graph problem, where each requirement is represented as a vertex and each pair of requirements has a structural similarity, (2) We detect main communities in the resultant graph by applying a hybrid technique combining limited dynamic programming and greedy algorithms, (3) finally, we reinterpret the detected communities as templates. Our experiments show that the suggested approach can detect templates that follow well-known standards with a 0.90 F1-measure. Moreover, the approach can detect common syntactic features for non-standard templates in more than 73.5% of the cases. Our evaluation indicates that these results are robust regardless of the number and the length of the processed requirements.
ER  - 

TY  - JOUR
T1  - CPN simulation-based test case generation from controlled natural-language requirements
AU  - Silva, Bruno Cesar F.
AU  - Carvalho, Gustavo
AU  - Sampaio, Augusto
JO  - Science of Computer Programming
VL  - 181
SP  - 111
EP  - 139
PY  - 2019
DA  - 2019/07/15/
SN  - 0167-6423
DO  - https://doi.org/10.1016/j.scico.2019.04.001
UR  - https://www.sciencedirect.com/science/article/pii/S0167642319300516
KW  - Model-based testing
KW  - Controlled natural language
KW  - Data-flow reactive system
KW  - Coloured Petri nets
KW  - Model simulation
AB  - We propose a test generation strategy from natural language (NL) requirements via translation into Coloured Petri Nets (CPN), an extension of Petri Nets that supports model structuring and provides a mature theory and powerful tool support. This approach extends our previous work on the NAT2TEST framework, which involves syntactic and semantic analyses of NL requirements and the generation of Data-Flow Reactive Systems (DFRS) as an intermediate representation, from which target formal models can be obtained for the purpose of test case generation. Our contributions include automating a systematic translation of DFRSs into CPN models, an extension to deal with time aspects, besides an empirical analysis of the CPN-based test generation strategy. The analyses considered examples both from the literature (a vending machine and a nuclear power plant control system), and from the aerospace and the automotive domain (a priority command control system and a turn indicator control system, respectively). We analysed performance and the ability to detect defects generated via mutation. The results provide evidence that the contribution proposed here is more efficient, besides being able to detect at least as many defects as our previous efforts.
ER  - 

TY  - JOUR
T1  - Latency-aware failover strategies for containerized web applications in distributed clouds
AU  - Aldwyan, Yasser
AU  - Sinnott, Richard O.
JO  - Future Generation Computer Systems
VL  - 101
SP  - 1081
EP  - 1095
PY  - 2019
DA  - 2019/12/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2019.07.032
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X19304224
KW  - Distributed Clouds
KW  - High availability
KW  - Performance
KW  - Container technologies
KW  - Cloud outages
KW  - Web applications
KW  - Distributed deployment
AB  - Despite advances in Cloud computing, ensuring high availability (HA) remains a challenge due to varying loads and the potential for Cloud outages. Deploying applications in distributed Clouds can help overcome this challenge by geo-replicating applications across multiple Cloud data centers (DCs). However, this distributed deployment can be a performance bottleneck due to network latencies between users and DCs as well as inter-DC latencies incurred during the geo-replication process. For most web applications, both HA and Performance (HAP) are essential and need to meet pre-agreed Service Level Objectives (SLOs). Efficiently placing and managing primary and backup replicas of applications in distributed Clouds to achieve HAP is a challenging task. Existing solutions consider either HA or performance but not both. In this paper we propose an approach for automating the process of providing a latency-aware failover strategy through a server placement algorithm leveraging genetic algorithms that factor in the proximity of users and inter-DC latencies. To facilitate the distributed deployment of applications and avoid the overheads of Clouds, we utilize container technologies. To evaluate our proposed approach, we conduct experiments on the Australia-wide National eResearch Collaboration Tools and Resources (NeCTAR - www.nectar.org.au) Research Cloud. Our results show at least a 23.3% and 22.6% improvement in response times under normal and failover conditions respectively compared to traditional, latency-unaware approaches. Also, the 95th percentile of response times in our approach are at most1.5 ms above the SLO compared to 11–32 ms using other approaches.
ER  - 

TY  - JOUR
T1  - Low-carbon product conceptual design from the perspectives of technical system and human use
AU  - Ai, Xianfeng
AU  - Jiang, Zhigang
AU  - Zhang, Hua
AU  - Wang, Yan
JO  - Journal of Cleaner Production
VL  - 244
SP  - 118819
PY  - 2020
DA  - 2020/01/20/
SN  - 0959-6526
DO  - https://doi.org/10.1016/j.jclepro.2019.118819
UR  - https://www.sciencedirect.com/science/article/pii/S0959652619336893
KW  - Low-carbon conceptual design
KW  - Function matching
KW  - Sustainable use
KW  - TRIZ laws
KW  - Regular expression
KW  - Part-of-speech tagging
AB  - Product conceptual design plays a decisive role in carbon emission of the products. Unfortunately, the traditional design methods based on carbon footprint calculation are not suitable for the conceptual design stage, and the latest low-carbon conceptual design research mainly focus on technology development to reduce carbon emissions at the manufacturing stage and less on carbon emissions caused by the unsustainable human use. This makes low-carbon product conceptual design less effective. To address this, a low-carbon conceptual design method is proposed for improving existing products, in which an improved process of requirements elicitation and analysis is implemented firstly, and then the improvement strategies are proposed from the perspectives of technical system and human use to help establish a low-carbon function structure. The conceptual design of a boiling water dispenser is taken as a case study. As a result, 5 low-carbon design strategies and a low-carbon function structure were comprehensively obtained. Next, by calculating the energy consumption of the assumed ideal scenario, it can be found that the re-designed product can save 39.4% energy compared to the existing product in the use stage. The results showed that the proposed method is effective in the generation of low-carbon design schemes at the conceptual design stage.
ER  - 

TY  - JOUR
T1  - Transformations’ Study Between Requirements Models and Business Process Models in MDA Approach
AU  - Kharmoum, Nassim
AU  - Bouchti, Karim El
AU  - Laaz, Naziha
AU  - Rhalem, Wajih
AU  - Rhazali, Yassine
JO  - Procedia Computer Science
VL  - 170
SP  - 819
EP  - 824
PY  - 2020
DA  - 2020/01/01/
T2  - The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2020.03.150
UR  - https://www.sciencedirect.com/science/article/pii/S1877050920306062
KW  - Model-Driven Architecture
KW  - Model transformation
KW  - Requirements model
KW  - Business Process model
KW  - Computation Independent Model
KW  - Platform Independent Model
AB  - Models transformation has become in the last decade, the primary key in the Model Driven Architecture (MDA) approach. Most of these models’ transformation are made between one or many abstraction levels (higher level, average level, or lower level). For that, the Object Management Group (OMG) offers for the MDA approach three abstraction levels, which are “Computation Independent Model” (CIM: the higher abstraction level), “Platform Independent Model” (PIM: average abstraction level) and “Platform Specific Model” (PSM: lower abstraction level). Hitherto, most researchers focused on the transformation between average abstraction level and lower abstraction level because those levels have multiple common points. However, the transformation between higher abstraction level and average abstraction level is rarely discussed because they are two distinct levels that can contain higher abstraction model nature, such as Requirements models and Business Process models. Therefore, our contribution in this paper is to study different transformations approaches between Requirements and Business Process models in the higher and average MDA abstraction levels. To do so, we study the selected approaches, analyze the results descriptively, then discuss them, plus propose new taxonomies based on the deduced evaluation criteria.
ER  - 

TY  - JOUR
T1  - Automated sign language detection and classification using reptile search algorithm with hybrid deep learning
AU  - Alsolai, Hadeel
AU  - Alsolai, Leen
AU  - Al-Wesabi, Fahd N.
AU  - Othman, Mahmoud
AU  - Rizwanullah, Mohammed
AU  - Abdelmageed, Amgad Atta
JO  - Heliyon
VL  - 10
IS  - 1
SP  - e23252
PY  - 2024
DA  - 2024/01/15/
SN  - 2405-8440
DO  - https://doi.org/10.1016/j.heliyon.2023.e23252
UR  - https://www.sciencedirect.com/science/article/pii/S2405844023104609
KW  - Sign language
KW  - Deep learning
KW  - Computer vision
KW  - Reptile search algorithm
KW  - Intelligent models
AB  - Sign language recognition (SLR) contains the capability to convert sign language gestures into spoken or written language. This technology is helpful for deaf persons or hard of hearing by providing them with a way to interact with people who do not know sign language. It is also be utilized for automatic captioning in live events and videos. There are distinct methods of SLR comprising deep learning (DL), computer vision (CV), and machine learning (ML). One general approach utilises cameras for capturing the signer's hand and body movements and processing the video data for recognizing the gestures. One of challenges with SLR comprises the variability in sign language through various cultures and individuals, the difficulty of certain signs, and require for realtime processing. This study introduces an Automated Sign Language Detection and Classification using Reptile Search Algorithm with Hybrid Deep Learning (SLDC-RSAHDL). The presented SLDC-RSAHDL technique detects and classifies different types of signs using DL and metaheuristic optimizers. In the SLDC-RSAHDL technique, MobileNet feature extractor is utilized to produce feature vectors, and its hyperparameters can be adjusted by manta ray foraging optimization (MRFO) technique. For sign language classification, the SLDC-RSAHDL technique applies HDL model, which incorporates the design of Convolutional Neural Network (CNN) and Long-Short Term Memory (LSTM). At last, the RSA was exploited for the optimal hyperparameter selection of the HDL model, which resulted in an improved detection rate. The experimental result analysis of the SLDC-RSAHDL technique on sign language dataset demonstrates the improved performance of the SLDC-RSAHDL system over other existing DL techniques.
ER  - 

TY  - JOUR
T1  - A survey on the formalisation of system requirements and their validation
AU  - Mokos, Konstantinos
AU  - Katsaros, Panagiotis
JO  - Array
VL  - 7
SP  - 100030
PY  - 2020
DA  - 2020/09/01/
SN  - 2590-0056
DO  - https://doi.org/10.1016/j.array.2020.100030
UR  - https://www.sciencedirect.com/science/article/pii/S2590005620300151
KW  - Requirement specification
KW  - Requirement formalisation
KW  - Semantic analysis
KW  - Model-based design
KW  - Component-based design
KW  - Formal verification
AB  - System requirements define conditions and capabilities to be met by a system under design. They are a partial definition in natural language, with inevitable ambiguities. Formalisation concerns with the transformation of requirements into a specification with unique interpretation, for resolving ambiguities, underspecified references and for assessing whether requirements are consistent, correct (i.e. valid for an acceptable solution) and attainable. Formalisation and validation of system requirements provides early evidence of adequate specification, for reducing the validation tests and high-cost corrective measures in the later system development phases. This article has the following contributions. First, we characterise the specification problem based on an ontology for some domain. Thus, requirements represent a particular system among many possible ones, and their specification takes the form of mapping their concepts to a semantic model of the system. Second, we analyse the state-of-the-art of pattern-based specification languages, which are used to avoid ambiguity. We then discuss the semantic analyses (missing requirements, inconsistencies etc.) supported in such a framework. Third, we survey related research on the derivation of formal properties from requirements, i.e. verifiable specifications that constrain the system’s structure and behaviour. Possible flaws in requirements may render the derived properties unsatisfiable or not realizable. Finally, this article discusses the important challenges for the current requirements analysis tools, towards being adopted in industrial-scale projects.
ER  - 

TY  - JOUR
T1  - Methodology for the design of didactic tools for the future transport engineers’ competencies formation on the ontology basis
AU  - Volegzhanina, Irina
AU  - Adol’f, Vladimir
AU  - Chusovlyanova, Svetlana
JO  - Transportation Research Procedia
VL  - 63
SP  - 1743
EP  - 1751
PY  - 2022
DA  - 2022/01/01/
T2  - X International Scientific Siberian Transport Forum — TransSiberia 2022
SN  - 2352-1465
DO  - https://doi.org/10.1016/j.trpro.2022.06.189
UR  - https://www.sciencedirect.com/science/article/pii/S2352146522004446
KW  - future engineer
KW  - railway transport
KW  - ontology
KW  - knowledge representation
KW  - questionnaire
KW  - didactic tools
AB  - The change in the essence of engineers’ functional activity activates the problem of creating new didactic tools adequate to the conditions of digital transformation of production and university education. The authors consider the ontological-semantic approach based on the use of ontological models and semantic technologies, as well as the principles of interoperability and interdisciplinarity corresponding to this approach as the methodological basis for the development of such tools. The feasibility of developing didactic tools on the ontology basis was determined using the questionnaire survey method, which was conducted among future railway transport engineers from 2016 to 2021. The results of the data analysis indicated a tendency for respondents to use compression tools for better assimilation of new learning material. Among such means students indicated not only tables, drawings and diagrams, but also a fundamentally new form of educational knowledge presentation - ontology. The conclusions made on the results of the questionnaire served as a basis for the development of didactic tools for the formation of communicative competence of future railway transport engineers. The novelty of this educational solution consisted in the complex use of textual, hypertext and ontological forms of branch knowledge representation, including the foreign language studied by students.
ER  - 

TY  - JOUR
T1  - Business process and rule integration approaches—An empirical analysis of model understanding
AU  - Wang, Wei
AU  - Chen, Tianwa
AU  - Indulska, Marta
AU  - Sadiq, Shazia
AU  - Weber, Barbara
JO  - Information Systems
VL  - 104
SP  - 101901
PY  - 2022
DA  - 2022/02/01/
SN  - 0306-4379
DO  - https://doi.org/10.1016/j.is.2021.101901
UR  - https://www.sciencedirect.com/science/article/pii/S0306437921001162
KW  - Business process modeling
KW  - Business rule modeling
KW  - Eye-tracking
KW  - Cognitive process
KW  - Model understanding
KW  - Controlled experiment
AB  - Business process models are widely used in organizations by information systems analysts to represent complex business requirements. They are also used by business users to understand business operations and constraints. This understanding is extracted from graphical process models as well as business rules. Prior research advocated integrating business rules and business process models to improve the effectiveness of various organizational activities, such as developing a shared understanding of practices, process improvement, and mitigating risks of compliance and policy breaches. However, whether such integrated modeling can improve the understanding of business processes, which is a fundamental benefit of integrated modeling, has not been empirically evaluated. In this paper, first, we report on an experiment investigating whether rule linking, a representative integrated modeling method, can improve understanding performance. We use eye tracking technology to understand the cognitive process by which model readers use models to perform understanding tasks. Our results show that rule linking outperforms separated modeling in terms of understanding effectiveness, efficiency, perceived mental effort, and visual attention. Further, cognitive process analysis reveals that the form of rule representation does not affect the extent of deep processing, but rule linking significantly decreases the occurrence of rule scanning and screening processes. Moreover, our results show that rule linking leads to an increase of visual association suggesting improved information integration, leading to improved task performance.
ER  - 

TY  - JOUR
T1  - Grounding of Modal Responses in Question Answering System Equipped with Hierarchical Categorisation
AU  - Lorkiewicz, Wojciech
AU  - Popek, Grzegorz
JO  - Procedia Computer Science
VL  - 176
SP  - 3163
EP  - 3172
PY  - 2020
DA  - 2020/01/01/
T2  - Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2020.09.172
UR  - https://www.sciencedirect.com/science/article/pii/S1877050920320743
KW  - question answering systems
KW  - language grounding
KW  - concept hierarchy
KW  - cognitive agent
AB  - Intelligent and dialogue systems highly utilise natural language interfaces. Such systems do not only process linguistic questions, but also formulate proper linguistic responses. Often neglected and important aspect of such responses lies in the ability to express and communicate systems internal beliefs. The proposed model fills in the current research gap in the grounding theory by enriching empirical experiences with hierarchical semantic structures in establishing agents internal belief stance. Such an extension significantly influences the process of grounding and allows for a dedicated computational mechanism of moving the conversations focus, which is incorporated into grounding mechanisms and fully presented.
ER  - 

TY  - JOUR
T1  - On the declarative paradigm in hybrid business process representations: A conceptual framework and a systematic literature study
AU  - Abbad Andaloussi, Amine
AU  - Burattin, Andrea
AU  - Slaats, Tijs
AU  - Kindler, Ekkart
AU  - Weber, Barbara
JO  - Information Systems
VL  - 91
SP  - 101505
PY  - 2020
DA  - 2020/07/01/
SN  - 0306-4379
DO  - https://doi.org/10.1016/j.is.2020.101505
UR  - https://www.sciencedirect.com/science/article/pii/S0306437920300168
KW  - Hybrid process model
KW  - Understandability of process models
KW  - Process flexibility
KW  - Declarative process modeling
KW  - Business process modeling
AB  - Process modeling plays a central role in the development of today’s process-aware information systems both on the management level (e.g., providing input for requirements elicitation and fostering communication) and on the enactment level (providing a blue-print for process execution and enabling simulation). The literature comprises a variety of process modeling approaches proposing different modeling languages (i.e., imperative and declarative languages) and different types of process artifact support (i.e., process models, textual process descriptions, and guided simulations). However, the use of an individual modeling language or a single type of process artifact is usually not enough to provide a clear and concise understanding of the process. To overcome this limitation, a set of so-called “hybrid” approaches combining languages and artifacts have been proposed, but no common grounds have been set to define and categorize them. This work aims at providing a fundamental understanding of these hybrid approaches by defining a unified terminology, providing a conceptual framework and proposing an overarching overview to identify and analyze them. Since no common terminology has been used in the literature, we combined existing concepts and ontologies to define a “Hybrid Business Process Representation” (HBPR). Afterwards, we conducted a Systematic Literature Review (SLR) to identify and investigate the characteristics of HBPRs combining imperative and declarative languages or artifacts. The SLR resulted in 30 articles which were analyzed. The results indicate the presence of two distinct research lines and show common motivations driving the emergence of HBPRs, a limited maturity of existing approaches, and diverse application domains. Moreover, the results are synthesized into a taxonomy classifying different types of representations. Finally, the outcome of the study is used to provide a research agenda delineating the directions for future work.
ER  - 

TY  - JOUR
T1  - The use of artificial neural networks for extracting actions and actors from requirements document
AU  - Al-Hroob, Aysh
AU  - Imam, Ayad Tareq
AU  - Al-Heisa, Rawan
JO  - Information and Software Technology
VL  - 101
SP  - 1
EP  - 15
PY  - 2018
DA  - 2018/09/01/
SN  - 0950-5849
DO  - https://doi.org/10.1016/j.infsof.2018.04.010
UR  - https://www.sciencedirect.com/science/article/pii/S0950584918300752
KW  - NLP
KW  - ANN
KW  - I-CASE
KW  - Software requirements
KW  - GATE
KW  - MATLAB
AB  - Context
The automatic extraction of actors and actions (i.e., use cases) of a system from natural language-based requirement descriptions, is considered a common problem in requirements analysis. Numerous techniques have been used to resolve this problem. Examples include rule-based (e.g., inference), keywords, query (e.g., bi-grams), library maintenance, semantic business vocabularies, and rules. The question remains: can combination of natural language processing (NLP) and artificial neural networks (ANNs) perform this job successfully and effectively?
Objective
This paper proposes a new approach to automatically identify actors and actions in a natural language-based requirements’ description of a system. Included are descriptions of how NLP plays an important role in extracting actors and actions, and how ANNs can be used to provide definitive identification.
Method
We used an NLP parser with a general architecture for text engineering, producing lexicons, syntaxes, and semantic analyses. An ANN was developed using five different use cases, producing different results due to their complexity and linguistic formation.
Results
Binomial classification accuracy techniques were used to evaluate the effectiveness of this approach. Based on the five use cases, the results were 17–63% for precision, 5–6100% for recall, and 29–71% for F-measure.
Conclusion
We successfully used a combination of NLP and ANN artificial intelligence techniques to reveal specific domain semantics found in a software requirements specification. An Intelligent Technique for Requirements Engineering (IT4RE) was developed to provide a semi-automated approach, classified as Intelligent Computer Aided Software Engineering (I-CASE).
ER  - 

TY  - JOUR
T1  - EMIL: Extracting Meaning from Inconsistent Language: Towards argumentation using a controlled natural language interface
AU  - Strass, Hannes
AU  - Wyner, Adam
AU  - Diller, Martin
JO  - International Journal of Approximate Reasoning
VL  - 112
SP  - 55
EP  - 84
PY  - 2019
DA  - 2019/09/01/
SN  - 0888-613X
DO  - https://doi.org/10.1016/j.ijar.2019.04.010
UR  - https://www.sciencedirect.com/science/article/pii/S0888613X18300793
KW  - Argumentation
KW  - Non-monotonic reasoning
KW  - Controlled natural language
KW  - Defeasible reasoning
AB  - There are well-developed formal and computational theories of argumentation to reason in the face of inconsistency, some with implementations; there are recent efforts to extract arguments from large textual corpora. Both developments are leading towards automated processing and reasoning with inconsistent, linguistically expressed knowledge in order to provide explanations and justifications in a form accessible to humans. However, there remains a gap between the knowledge-bases of computational theories of argumentation, which are generally coarse-grained and semi-structured (e.g. propositional logic), and inferences from knowledge-bases derived from natural language, which are fine-grained and highly structured (e.g. predicate logic). Arguments that occur in textual corpora are very rich, highly various, and incompletely understood. We identify several subproblems which must be addressed in order to bridge the gap, requiring the development of a computational foundation for argumentation coupled with natural language processing. For the computational foundation, we provide a direct semantics, a formal approach for argumentation, which is implemented and suitable to represent and reason with an associated natural language expression for defeasibility. It has attractive properties with respect to expressivity and complexity; we can reason by cases; we can structure higher level argumentation components such as cases and debates. With the implementation, we output experimental results which emphasise the importance of our efficient approach. To motivate our formal approach, we identify a range of issues found in other approaches. For the natural language processing, we adopt and adapt an existing controlled natural language (CNL) to interface with our computational theory of argumentation; the tool takes natural language input and automatically outputs expressions suitable for automated inference engines. A CNL, as a constrained fragment of natural language, helps to control variables, highlights key problems, and provides a framework to engineer solutions. The key adaptation incorporates the expression ‘it is usual that’, which is a plausibly ‘natural’ linguistic expression of defeasibility. This is an important, albeit incremental, step towards the incorporation of linguistic expressions of defeasibility; yet, by engineering such specific solutions, a range of other, relevant issues arise to be addressed. Overall, we can input arguments expressed in a controlled natural language, translate them to a formal knowledge base, represent the knowledge in a rule language, reason with the rules, generate argument extensions, and finally convert the arguments in the extensions into natural language. Our approach makes for fine-grained, highly structure, accessible, and linguistically represented argumentation evaluation. The overall novel contribution of the paper is an integrated, end-to-end argumentation system which bridges a gap between automated defeasible reasoning and a natural language interface. The component novel contributions are the computational theory of ‘direct semantics’, the motivation for our theory, the results with respect to the direct semantics, the implementation, the experimental results, the tie between the formalisation and the CNL, the adaptation of a CNL defeasibility, and an ‘engineering’ approach to fine-grained argument analysis.
ER  - 

TY  - JOUR
T1  - Incorporating sentimental trend into gated mechanism based transformer network for story ending generation
AU  - Mo, Linzhang
AU  - Wei, Jielong
AU  - Huang, Qingbao
AU  - Cai, Yi
AU  - Liu, Qingguang
AU  - Zhang, Xingmao
AU  - Li, Qing
JO  - Neurocomputing
VL  - 453
SP  - 453
EP  - 464
PY  - 2021
DA  - 2021/09/17/
SN  - 0925-2312
DO  - https://doi.org/10.1016/j.neucom.2021.01.040
UR  - https://www.sciencedirect.com/science/article/pii/S0925231221000618
KW  - Story ending generation
KW  - Sentiment trend
KW  - Transformer
KW  - Gated mechanism
AB  - Story ending generation is a challenging and under-explored task, which aims at generating a coherent, reasonable, and logical story ending given a context. Previous studies mainly focus on utilizing the contextual information and commonsense knowledge to generate story endings. However, there are still some issues must be addressed in the story endings generation processing, such as sentimental consistency and interference from secondary information. In this paper, we propose a Gated Mechanism based Transformer Network (GMTF). The GMTF model utilizes the sentimental trend to make story ending generation more sentimentally consistent with the context. For a given story context, we utilize a sentiment analysis tool VADER to obtain the sentimental trend. Then, the sentimental information and contextual information are input jointly into the transformer network to capture the key clues. Furthermore, the gated mechanism is applied to filter irrelative information and the weights of attention layers for encoder and decoder are shared to make the most of the contextual clues. The experimental results on ROCStories dataset demonstrate that the proposed method achieves 27.03% on BLEU-1, 7.62% on BLEU-2, 1.71 on Grammar, and 1.31 on Logicality, respectively. Specifically, our model outperforms the state-of-the-art model IE+MSA by 0.23%, 0.22%, 1.78%, 5.64%, respectively and the Transformer model by 3.06%, 1.05%, 5.55%, 48.86%, respectively. Both automatic and manual evaluations show that our model can generate more reasonable and appropriate story endings compared with the related well-established approaches.
ER  - 

TY  - JOUR
T1  - Automation and consistency analysis of test cases written in natural language: An industrial context
AU  - Arruda, Filipe
AU  - Barros, Flávia
AU  - Sampaio, Augusto
JO  - Science of Computer Programming
VL  - 189
SP  - 102377
PY  - 2020
DA  - 2020/04/01/
SN  - 0167-6423
DO  - https://doi.org/10.1016/j.scico.2019.102377
UR  - https://www.sciencedirect.com/science/article/pii/S0167642319301698
KW  - Test automation
KW  - Controlled natural language
KW  - Alloy
KW  - Test case consistency
AB  - We present here a novel test automation strategy that receives as input a freestyle natural language (NL) test case (consisting of a sequence of test steps) and produces executable test scripts. This strategy relies on a database of previously automated seed test steps, available for reuse. New steps are automated via a capturing process by a tester, without requiring any programming knowledge. Automated tests can be executed by a replay facility. We discuss the reuse improvement, implementation effort, and user feedback regarding the industrial applicability and usability of our capture & replay tool. We then show that restricting the input textual description to obey a proposed Controlled NL (CNL) brings significant advantages: (1) reuse improvement; (2) the possibility of integration with a test generation framework; and (3) definition of consistency notions for test actions and test action sequences, that ensure, respectively, well-formedness of each action and a proper configuration to safely execute a sequence of actions. We formalize these consistency notions in Alloy and use the Alloy Analyzer to carry out the consistency check; the scalability of the analysis is assessed via an evaluation considering a repository with real test cases; the practical context of our work is mobile device testing, involving a partnership with Motorola Mobility, a Lenovo company.
ER  - 

TY  - JOUR
T1  - Optimisation of phonetic aware speech recognition through multi-objective evolutionary algorithms
AU  - Bird, Jordan J.
AU  - Wanner, Elizabeth
AU  - Ekárt, Anikó
AU  - Faria, Diego R.
JO  - Expert Systems with Applications
VL  - 153
SP  - 113402
PY  - 2020
DA  - 2020/09/01/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2020.113402
UR  - https://www.sciencedirect.com/science/article/pii/S0957417420302268
KW  - Speech recognition
KW  - Phoneme classification
KW  - Applied hyperheuristics
KW  - Multi-objective evolutionary computation
AB  - Recent advances in the availability of computational resources allow for more sophisticated approaches to speech recognition than ever before. This study considers Artificial Neural Network and Hidden Markov Model methods of classification for Human Speech Recognition through Diphthong Vowel sounds in the English Phonetic Alphabet rather than the classical approach of the classification of whole words and phrases, with a specific focus on both single and multi-objective evolutionary optimisation of bioinspired classification methods. A set of audio clips are recorded by subjects from the United Kingdom and Mexico and the recordings are transformed into a static dataset of statistics by way of their Mel-Frequency Cepstral Coefficients (MFCC) at sliding window length of 200ms as well as a reshaped MFCC timeseries format for forecast-based models. An deep neural network with evolutionary optimised topology achieves 90.77% phoneme classification accuracy in comparison to the best HMM that achieves 86.23% accuracy with 150 hidden units, when only accuracy is considered in a single-objective optimisation approach. The obtained solutions are far more complex than the HMM taking around 248 seconds to train on powerful hardware versus 160 for the HMM. A multi-objective approach is explored due to this. In the multi-objective approaches of scalarisation presented, within which real-time resource usage is also considered towards solution fitness, far more optimal solutions are produced which train far quicker than the forecast approach (69 seconds) with classification ability retained (86.73%). Weightings towards either maximising accuracy or reducing resource usage from 0.1 to 0.9 are suggested depending on the resources available, since many future IoT devices and autonomous robots may have limited access to cloud resources at a premium in comparison to the GPU used in this experiment.
ER  - 

TY  - JOUR
T1  - Linked open data in authoring virtual exhibitions
AU  - Monaco, Daniele
AU  - Pellegrino, Maria Angela
AU  - Scarano, Vittorio
AU  - Vicidomini, Luca
JO  - Journal of Cultural Heritage
VL  - 53
SP  - 127
EP  - 142
PY  - 2022
DA  - 2022/01/01/
SN  - 1296-2074
DO  - https://doi.org/10.1016/j.culher.2021.11.002
UR  - https://www.sciencedirect.com/science/article/pii/S1296207421001667
KW  - Virtual exhibitions
KW  - Linked open data
KW  - Knowledge graph
KW  - Virtual reality
KW  - Query builder
KW  - Natural language interface
AB  - In the last years, virtual exhibitions have been widely adopted to enhance traditional museums and enable active interaction with culture without posing any physical constraints. Nevertheless, people interested in cultural heritage still behave as visitors. To fully engage them, we propose to let cultural heritage lovers play the role of exhibition curators. In authoring virtual exhibitions, users have to perform a data selection phase that poses several challenges, including finding data sources and extracting data of interest. We aim to take advantage of data published as Knowledge Graphs in the Linked Open Data format. Users can query geographically distributed artworks thanks to their linking nature, manipulate heterogeneous data, and easily customise their exhibitions by exploiting the wide range of available cultural heritage knowledge graphs. However, the complexity of linked open data query languages (such as SPARQL) threatens their exploitation. Consequently, we need to mask SPARQL technical challenges and guide users in naturally posing questions to unlock the potentialities of linked open data to a broader audience. We propose a virtual exhibition authoring tool that guides users from knowledge graphs querying to the automatic generation of virtual experiences. The Knowledge Graph query phase relies on ELODIE, a natural language interface to scaffold users in retrieving data of interest without asking for technical skills in query languages. We introduce our prototype by describing its operating mechanism and by detailing its components. We present a Van Gogh’s experience as a use case by collecting all the artist’s artworks published on DBpedia (a well-known and general purpose knowledge graph) and organise them in a virtual reality-based virtual exhibition. Finally, we conclude by overviewing advantages and technical challenges posed by linked open data in designing and developing knowledge graph exploitation tools.
ER  - 

TY  - JOUR
T1  - Guest editorial for the special issue from the 18th Brazilian Symposium on Formal Methods (SBMF 2015)
AU  - Cornélio, M.L.
AU  - Roscoe, A.W.
JO  - Science of Computer Programming
VL  - 181
SP  - 82
EP  - 83
PY  - 2019
DA  - 2019/07/15/
SN  - 0167-6423
DO  - https://doi.org/10.1016/j.scico.2019.07.001
UR  - https://www.sciencedirect.com/science/article/pii/S0167642319300747
ER  - 

TY  - JOUR
T1  - SiteFinder: A geospatial scoping tool to assist the siting of external water harvesting structures
AU  - Delaney, R.G.
AU  - Blackburn, G.A.
AU  - Whyatt, J.D.
AU  - Folkard, A.M.
JO  - Agricultural Water Management
VL  - 272
SP  - 107836
PY  - 2022
DA  - 2022/10/01/
SN  - 0378-3774
DO  - https://doi.org/10.1016/j.agwat.2022.107836
UR  - https://www.sciencedirect.com/science/article/pii/S0378377422003833
KW  - Water harvesting
KW  - GIS
KW  - Digital elevation model
KW  - Remote sensing
KW  - Drylands
AB  - Water harvesting has a long history, but still plays an important role today by increasing crop productivity, combatting erosion, and improving water supplies. Geographical Information Systems (GIS) are used extensively to assess the suitability of sites for water harvesting but available tools fail to consider the synoptic topography of sites. Here, we report the creation of a novel, automated tool – “SiteFinder” – that evaluates potential locations by automatically calculating site-specific information, including structure parameters (height, length, and volume) and descriptors of the zone affected by the structure (storage capacity and area of influence) and the catchment area. Innovatively, compared to existing tools of this kind, SiteFinder works within a GIS environment. Thus, it allows the possibility of combining its outputs with larger Multi-Criteria Decision-Making processes to consider other bio-physical, socio-economic, and environmental factors. It utilises a Digital Elevation Model (DEM) and automatically analyses thousands of potential sites, computing site characteristics for different barrier heights that are dependent on the surrounding topography. It outputs values of eight parameters to aid planners in assessing the characteristics of sites as to their suitability for water harvesting. We conducted case studies using 30 × 30 m gridded DEMs to automatically evaluate several thousand sites and, by filtering the tool outputs, successfully identified sites with characteristics appropriate for scenarios at three spatial scales: large dams for nationally significant water supply reservoirs (383 sites analysed; 5 filtered sites with barriers up to 30 m in height); large gully erosion control dams for regional-scale interventions (4,586 sites analysed; 6 filtered sites with barriers up to 3.6 m in height); and local, community-based earth embankment projects (801 sites analysed; 6 filtered sites with barriers up to 2 m in height). A higher resolution (1 × 1 m) terrain elevation model, derived from open-source airborne survey data, was used to assess the veracity of these results. Correlations between the barrier length, impounded area and storage volume capacity derived from the two different resolution data sets were all strongly significant (Spearman’s rank correlation, p < 0.001); and normalised root mean square errors were 9%, 15% and 16% for these parameters, respectively.
ER  - 

TY  - JOUR
T1  - Generating BPMN diagram from textual requirements
AU  - Sholiq, Sholiq
AU  - Sarno, Riyanarto
AU  - Astuti, Endang Siti
JO  - Journal of King Saud University - Computer and Information Sciences
VL  - 34
IS  - 10, Part B
SP  - 10079
EP  - 10093
PY  - 2022
DA  - 2022/11/01/
SN  - 1319-1578
DO  - https://doi.org/10.1016/j.jksuci.2022.10.007
UR  - https://www.sciencedirect.com/science/article/pii/S1319157822003585
KW  - BPMN diagram
KW  - Natural language
KW  - Textual requirement
AB  - An interesting challenge in software requirements engineering is converting textual requirements to Business Process Model and Notation (BPMN) diagrams. In this study, the BPMN diagram is used as an intermediate representation before measuring the functional software size from Natural Language (NL) input. The methods currently used for converting NL input to BPMN diagrams are not able to generate complete BPMN diagrams, nor can they handle complex and compound-complex sentences in the NL input. This study proposes conversion from textual requirements to a BPMN diagram for improving the weaknesses of existing methods. The proposed method has two stages: 1) analyzing the textual requirements using natural language processing and 2) generating the BPMN diagram. The output of the first stage is fact types as the basis for generating the BPMN diagram in the second phase. The BPMN diagram is generated using a set of informal mapping rules that were created in this study. The proposed method was applied to ten textual requirements of an enterprise application, which involved simple, compound, complex, and compound-complex sentences. The experiments resulted in a suitable BPMN diagram with higher accuracy than obtained by other methods.
ER  - 

TY  - JOUR
T1  - Flexible runtime support of business processes under rolling planning horizons
AU  - Barba, Irene
AU  - Jiménez-Ramírez, Andrés
AU  - Reichert, Manfred
AU  - Del Valle, Carmelo
AU  - Weber, Barbara
JO  - Expert Systems with Applications
VL  - 177
SP  - 114857
PY  - 2021
DA  - 2021/09/01/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2021.114857
UR  - https://www.sciencedirect.com/science/article/pii/S0957417421002980
KW  - Process flexibility
KW  - Rolling planning horizon
KW  - Declarative process
KW  - Healthcare process
AB  - This work has been motivated by the needs we discovered when analyzing real-world processes from the healthcare domain that have revealed high flexibility demands and complex temporal constraints. When trying to model these processes with existing languages, we learned that none of the latter was able to fully address these needs. This motivated us to design TConDec-R, a declarative process modeling language enabling the specification of complex temporal constraints. Enacting business processes based on declarative process models, however, introduces a high complexity due to the required optimization of objective functions, the handling of various temporal constraints, the concurrent execution of multiple process instances, the management of cross-instance constraints, and complex resource allocations. Consequently, advanced user support through optimized schedules is required when executing the instances of such models. In previous work, we suggested a method for generating an optimized enactment plan for a given set of process instances created from a TConDec-R model. However, this approach was not applicable to scenarios with uncertain demands in which the enactment of newly created process instances starts continuously over time, as in the considered healthcare scenarios. Here, the process instances to be planned within a specific timeframe cannot be considered in isolation from the ones planned for future timeframes. To be able to support such scenarios, this article significantly extends our previous work by generating optimized enactment plans under a rolling planning horizon. We evaluate the approach by applying it to a particularly challenging healthcare process scenario, i.e., the diagnostic procedures required for treating patients with ovarian carcinoma in a Woman Hospital. The application of the approach to this sophisticated scenario allows avoiding constraint violations and effectively managing shared resources, which contributes to reduce the length of patient stays in the hospital.
ER  - 

TY  - JOUR
T1  - Tackling rapid technology changes by applying enterprise engineering theories
AU  - Dvořák, Ondřej
AU  - Pergl, Robert
JO  - Science of Computer Programming
VL  - 215
SP  - 102747
PY  - 2022
DA  - 2022/03/01/
SN  - 0167-6423
DO  - https://doi.org/10.1016/j.scico.2021.102747
UR  - https://www.sciencedirect.com/science/article/pii/S0167642321001404
KW  - ADA
KW  - Component-based systems
KW  - Enterprise engineering
KW  - Technology acceleration
KW  - Evolvability
AB  - Moore's law states that the number of transistors on a chip will double every two years. A similar force appears to drive the progress of information technology (IT). IT companies tend to struggle to keep up with the latest technological developments, and software solutions are becoming increasingly outdated. The ability for software to change easily is defined as evolvability. One of the major fields researching evolvability is enterprise engineering (EE). The EE research paradigm applies theories from other fields to the evolvability of organisations. We argue that such theories can be applied to software engineering (SE) as well, which can contribute to the construction of software with a clear separation of dynamically changing technologies based on a relatively stable description of functions required for a specific user. EE theories introduce notions of function, construction, and affordance. We reify these concepts in terms of SE. Based on this reification, we propose affordance-driven assembling (ADA) as a software design approach that can aid in the construction of more evolvable software solutions. We exemplify the implementation of ADA in a case study on a commercial system and measure its effectiveness in terms of the impact of changes, as defined by the normalised systems theory.
ER  - 

TY  - JOUR
T1  - A mathematical evaluation for measuring correctness of domain ontologies using concept maps
AU  - Iqbal, Rizwan
AU  - Azmi Murad, Masrah Azrifah
AU  - Sliman, Layth
AU  - da Silva, Clay Palmeira
JO  - Measurement
VL  - 118
SP  - 73
EP  - 82
PY  - 2018
DA  - 2018/03/01/
SN  - 0263-2241
DO  - https://doi.org/10.1016/j.measurement.2018.01.009
UR  - https://www.sciencedirect.com/science/article/pii/S0263224118300083
KW  - Ontology engineering
KW  - Concept mapping
KW  - Ontology evaluation
KW  - Closeness index
KW  - Similarity index
AB  - There is a need for further research in the area of ontology evaluation specifically dealing with ontology development exploiting concept maps. The existing literature on ontology evaluation primarily emphasis on ontology formalisation as well as on performing logical inferences, which is usually not directly relevant for concept maps as they are commonly exploited as communication instruments for learning purposes. Commonly used techniques for evaluating concept maps for knowledge assessment may be adopted for a kind of criteria-based evaluation of a domain concept map with respect to a particular aspect. However, this makes its validity limited to a particular aspect or criteria. This paper presents a mathematical ontology evaluation technique to measure the correctness of domain ontologies engineered using concept maps. It is based on the notion of merging two different mathematical measures, namely closeness index and similarity index to come up with a combined index that takes different criteria or aspects into account while performing ontology evaluation. Therefore, the proposed technique makes the evaluation process more reliable and robust. Two case studies were conducted employing the proposed technique for evaluating two different domain ontologies that were engineered using concept maps. Calculations and results from the case studies showed that depending on the correctness of individual ontology, different values of combined Index was calculated manifesting the measure of correctness of each individual ontology in a quantifiable form. Moreover, the results depict that the technique provides in-depth evaluation, it is easy to adopt, requires no special skills, and is conveniently replicable.
ER  - 

TY  - JOUR
T1  - Test case information extraction from requirements specifications using NLP-based unified boilerplate approach
AU  - Lim, Jin Wei
AU  - Chiew, Thiam Kian
AU  - Su, Moon Ting
AU  - Ong, Simying
AU  - Subramaniam, Hema
AU  - Mustafa, Mumtaz Begum
AU  - Chiam, Yin Kia
JO  - Journal of Systems and Software
VL  - 211
SP  - 112005
PY  - 2024
DA  - 2024/05/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2024.112005
UR  - https://www.sciencedirect.com/science/article/pii/S0164121224000487
KW  - Natural language processing
KW  - Test case generation
KW  - Automation
KW  - Software requirements
KW  - Software Testing
KW  - Test Case
AB  - Automated testing which extracts essential information from software requirements written in natural language offers a cost-effective and efficient solution to error-free software that meets stakeholders’ requirements in the software industry. However, natural language can cause ambiguity in requirements and increase the challenges of automated testing such as test case generation. Negative requirements also cause inconsistency and are often neglected. This research aims to extract test case information (actors, conditions, steps, system response) from positive and negative requirements written in natural language (i.e. English) using natural language processing (NLP). We present a unified boilerplate that combines Rupp's and EARS boilerplates, and serves as the grammar guideline for requirements analysis. Extracted information is populated in a test case template, becoming the building blocks for automated test case generation. An experiment was conducted with three public requirements specifications from PURE datasets to investigate the correctness of information extracted using this proposed approach. The results presented correctness of 50 % (Mdot), 61.7 % (Pointis) and 10 % (Npac) on information extracted. The lower correctness on negative over positive requirements was observed. The correctness by specific categories is also analysed, revealing insights into actors, steps, conditions, and system response extracted from positive and negative requirements.
ER  - 

TY  - JOUR
T1  - Human-Generated Web Data Disentanglement for Complex Event Processing
AU  - Blanco, José Miguel
AU  - Ge, Mouzhi
AU  - Pitner, Tomáš
JO  - Procedia Computer Science
VL  - 207
SP  - 1341
EP  - 1349
PY  - 2022
DA  - 2022/01/01/
T2  - Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2022.09.190
UR  - https://www.sciencedirect.com/science/article/pii/S1877050922010730
KW  - Complex Event Processing
KW  - Semantic Web
KW  - Data Disentanglement
KW  - Web Data Preprocessing
AB  - In social media, human-generated web data from real-world events have become exponentially complex due to the chaotic and spontaneous features of natural language. This may create an information overload for the information consumers, and in turn not easily digest a large amount of information in a limited time. To tackle this issue, we propose to use Complex Event Processing (CEP) and semantic web reasoners to disentangle the human-generated data and present users with only relevant and important data. However, one of the key obstacles is that the human-generated data can have no structured meaning sometimes even for the speaker, hindering the output of the CEP. Therefore, in order to adapt to the CEP inputs, we present two different techniques that allow for the discrimination and digestion of value of human-generated data. The first one relies on the Variable Sharing Property that was developed for relevance logics, while the second one is based on semantic equivalence and natural language processing. The results can be given to CEP for further semantic reasoning and generate digested information for users.
ER  - 

TY  - JOUR
T1  - Managing non-trivial internet-of-things systems with conversational assistants: A prototype and a feasibility experiment
AU  - Lago, André Sousa
AU  - Dias, João Pedro
AU  - Ferreira, Hugo Sereno
JO  - Journal of Computational Science
VL  - 51
SP  - 101324
PY  - 2021
DA  - 2021/04/01/
SN  - 1877-7503
DO  - https://doi.org/10.1016/j.jocs.2021.101324
UR  - https://www.sciencedirect.com/science/article/pii/S1877750321000223
KW  - Internet-of-Things
KW  - Conversational assistants
KW  - Software engineering
KW  - Natural language processing
KW  - Visual programming
AB  - Internet-of-Things has reshaped the way people interact with their surroundings and automatize the once manual actions. In a smart home, controlling the Internet-connected lights is as simple as speaking to a nearby conversational assistant. However, specifying interaction rules, such as making the lamp turn on at specific times or when someone enters the space is not a straightforward task. The complexity of doing such increases as the number and variety of devices increases, along with the number of household members. Thus, managing such systems becomes a problem, including finding out why something has happened. This issue lead to the birth of several low-code development solutions that allow users to define rules to their systems, at the cost of discarding the easiness and accessibility of voice interaction. In this paper we extend the previous published work on Jarvis [1], a conversational interface to manage IoT systems that attempts to address these issues by allowing users to specify time-based rules, use contextual awareness for more natural interactions, provide event management and support causality queries. A proof-of-concept is presented, detailing its architecture and natural language processing capabilities. A feasibility experiment was carried with mostly non-technical participants, providing evidence that Jarvis is intuitive enough to be used by common end-users, with participants showcasing an overall preference by conversational assistants over visual low-code solutions.
ER  - 

TY  - JOUR
T1  - A review of the generation of requirements specification in natural language using objects UML models and domain ontology
AU  - Abdalazeim, Alaa
AU  - Meziane, Farid
JO  - Procedia Computer Science
VL  - 189
SP  - 328
EP  - 334
PY  - 2021
DA  - 2021/01/01/
T2  - AI in Computational Linguistics
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2021.05.102
UR  - https://www.sciencedirect.com/science/article/pii/S1877050921012266
KW  - Requirements Specification
KW  - Natural Language Generation
KW  - Object UML Model
KW  - Ontology
AB  - In the software development life cycle, requirements engineering is the main process that is derived from users by informal interviews written in natural language by requirements engineers (analysts). The requirements may suffer from incompleteness and ambiguity when transformed into formal or semi-formal models that are not well understood by stakeholders. Hence, the stakeholder cannot verify if the formal or semi-formal models satisfy their needs and requirements. Another problem faced by requirements is that when code and/or designs are updated, it is often the case that requirements and specifically the requirements document are not updated. Hence ending with a requirements document not reflecting the implemented software. Generating requirements from the design and/or implementation document is seen by many researchers as a way to address the latter issue. This paper presents a survey of some works undertaken in the field of generation natural language specifications from object UML model using the support of an ontology. and analyzing the robustness and limitations of these existing approaches. This includes studying the generation of natural language from a formal model, review the generation of natural language from ontologies, and finally reviews studies about check to generate natural language from OntoUML.
ER  - 

TY  - JOUR
T1  - Digital storytelling for good with Tappetina game
AU  - Gomez, Javier
AU  - Jaccheri, Letizia
AU  - Maragoudakis, Manolis
AU  - Sharma, Kshitij
JO  - Entertainment Computing
VL  - 30
SP  - 100297
PY  - 2019
DA  - 2019/05/01/
SN  - 1875-9521
DO  - https://doi.org/10.1016/j.entcom.2019.100297
UR  - https://www.sciencedirect.com/science/article/pii/S1875952118300909
KW  - Digital storytelling
KW  - Data analysis
KW  - Machine Learning
KW  - Games for Good
AB  - Context
Storytelling is an important asset in today’s society. Digital platforms for storytelling can facilitate collaborative development of stories. The storytelling process, if properly facilitated, can lead to the creation of stories that improve the relations between the players. Moreover, stories convey important information about the players and their interaction. Extended knowledge and better tools are needed about how to facilitate storytelling for good and analysis to exploit the power of the generated data.
Research question
How to facilitate Digital Storytelling for good?
Method
The investigation is based on a case study approach in which participants have been engaged in the creation of stories. The study is based on empirical data collection and analysis: from the stories recorded, we extract the storytelling features and performance. We have provided qualitative (Domain Expert) and quantitative (Machine Learning) analysis of the stories. In total, 58 users played the game in 15 sessions.
Results and conclusions
The main result is a framework for analysing digital stories. The analysis gives an indication of which game building blocks lead to stories for good. Future work will include a redesign of the game and its building blocks which lead to stories for good and further analyses.
ER  - 

TY  - JOUR
T1  - BR4DQ: A methodology for grouping business rules for data quality evaluation
AU  - Caballero, Ismael
AU  - Gualo, Fernando
AU  - Rodríguez, Moisés
AU  - Piattini, Mario
JO  - Information Systems
VL  - 109
SP  - 102058
PY  - 2022
DA  - 2022/11/01/
SN  - 0306-4379
DO  - https://doi.org/10.1016/j.is.2022.102058
UR  - https://www.sciencedirect.com/science/article/pii/S0306437922000485
KW  - Business rules
KW  - Data quality
KW  - Data quality evaluation
KW  - Data quality measurement
KW  - Data quality characteristics
KW  - Data quality properties
KW  - ISO/IEC 25012
KW  - ISO/IEC 25024
AB  - Data quality evaluation is built upon data quality measurement results. “Data quality evaluation” uses the “data quality rules” representing the risk appetite of the organization to decide on the usability of the data; “data quality measurement” uses the business rules describing the “data requirements” or “data specifications” to determine the validity of the data. Consequently, to conduct meaningful and useful data quality evaluations, business rules must be first completely identified and captured at the beginning of the evaluation to perform sound measurements. We propose that the evaluation leads to better and more interpretable and useful results when the potential contribution of these business rules to the measurement of the data quality characteristics is first evaluated, avoiding the inclusion in the evaluation of those not having potential contribution and the resulting waste of resources. Considering this, we feel that for a better management of business rules for data quality evaluation, it makes sense to group all business rules having an important contribution to the evaluation of data quality characteristics, something that other business rules management methodologies have not covered yet. Through our experiences in conducting industrial projects of data quality evaluations we identified six problems when collecting and grouping the business rules. These problems make data quality evaluation processes less efficient and more costly. The main contribution of this paper is a methodology to systematically collect, group and validate the business rules to avoid or to alleviate these problems. For the sake of generalization, comparability, and reusability, we propose to do the grouping for data quality characteristics and properties defined in ISO/IEC 25012 and ISO/IEC 25024, respectively. Lastly, we validate the methodology in three case studies of real projects. From this validation, it is possible to raise the conclusion that the methodology is useful, applicable in the real world, and valid to capture and group the business rules used as a basis for data quality evaluation.
ER  - 

TY  - JOUR
T1  - Applied Machine Learning for Production Planning and Control: Overview and Potentials
AU  - Büttner, Konstantin
AU  - Antons, Oliver
AU  - Arlinghaus, Julia C.
JO  - IFAC-PapersOnLine
VL  - 55
IS  - 10
SP  - 2629
EP  - 2634
PY  - 2022
DA  - 2022/01/01/
T2  - 10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022
SN  - 2405-8963
DO  - https://doi.org/10.1016/j.ifacol.2022.10.106
UR  - https://www.sciencedirect.com/science/article/pii/S2405896322021152
KW  - machine learning
KW  - production planning
KW  - control
KW  - production control
AB  - Manufacturing companies are under constant pressure to increase efficiency and to achieve logistical objectives. Improving production planning and control (PPC) has significant impact on these efforts. At the same time, increasing complexity and dynamics of PPC environments make PPC more difficult. One way to cope with this situation is the application of machine learning (ML) methods. In this article, we therefore address the current state of PPC-ML research and show, based on the Aachen PPC model, in which PPC tasks and subtasks ML is already applied and to what degree the task is covered by ML. The analysis is limited to core and cross-sectional tasks of the Aachen PPC model, procurement and network tasks are not included. Furthermore, a broad analysis of the targeted data mining, business and logistic objectives is conducted. In addition, we also identify motivations which prompted researchers to apply ML in PPC.
ER  - 

TY  - JOUR
T1  - NALDO: From natural language definitions to OWL expressions
AU  - Kacfah Emani, Cheikh
AU  - Ferreira Da Silva, Catarina
AU  - Fiès, Bruno
AU  - Ghodous, Parisa
JO  - Data & Knowledge Engineering
VL  - 122
SP  - 130
EP  - 141
PY  - 2019
DA  - 2019/07/01/
SN  - 0169-023X
DO  - https://doi.org/10.1016/j.datak.2019.06.002
UR  - https://www.sciencedirect.com/science/article/pii/S0169023X18306086
KW  - Ontologies
KW  - Natural language definitions
KW  - Ontology enrichment
KW  - OWL DL
KW  - Semantic web
AB  - Domain ontologies are pivotal for Semantic Web applications. The richness of an ontology goes in hand with its usefulness and efficiency. Unfortunately, manually enriching an ontology is very time-consuming. In this paper, we propose to enrich an ontology automatically by obtaining logical expressions of concepts. We present NALDO, a novel approach that provides an OWL DL (Web Ontology Language Description Logics) expression of a concept from two inputs: (1) the natural language definition of the concept and (2) an ontology describing the domain of this concept. NALDO uses as much as possible entities provided by the domain ontology, however it can suggest, when needed, new entities. The expressiveness of expressions provided by NALDO covers value and cardinality restrictions, subsumption and equivalence. We evaluate our approach against the definitions and the corresponding ontologies of the BEAUFORD benchmark. Our results show that NALDO is able to perform the correct identification of formal entities with an F1-measure up to 0.79.
ER  - 

TY  - JOUR
T1  - Automated Analysis of Assembly Processes in Human-Robot Collaboration: Research Approaches and Challenges
AU  - Jonek, Michael
AU  - Niermann, Dario
AU  - Petzoldt, Christoph
AU  - Manns, Martin
AU  - Freitag, Michael
JO  - Procedia CIRP
VL  - 120
SP  - 1203
EP  - 1208
PY  - 2023
DA  - 2023/01/01/
T2  - 56th CIRP International Conference on Manufacturing Systems 2023
SN  - 2212-8271
DO  - https://doi.org/10.1016/j.procir.2023.09.149
UR  - https://www.sciencedirect.com/science/article/pii/S2212827123008818
KW  - Collaborative robot
KW  - Human-Machine Cooperation
KW  - Process-Planning
KW  - Process-Analysis
KW  - Digital Twin
AB  - Due to the increasing trends of individualization and variant diversity, cobotic production systems are becoming more important. However, cobots are hardly used in small and medium-sized enterprises (SME) because of a lack in expertise, experience or resources. In order to be able to assess whether the use of a cobot is reasonable, there are already many methods in research with different focuses such as ergonomics, productivity or economic efficiency. However, many methods are not targeted for use by SME, thus still require the involvement of experts, and often only consider economic efficiency without consideration of human-centered aspects. Furthermore, these are mostly not integrated into commonly used process planning tools. This work provides an overview of methods for automated analysis of assembly processes in HRC. We present a method for HRC process analysis with an individual weighting of fatigue, time, costs and safety that can be set depending on an application-specific focus defined by the user. It allows non-experts to perform a process analysis based on the individual weighting and a digital twin of the workstation. An evaluation method developed for this purpose calculates the benefit of a cobot in the analyzed process depending on the preset focus. To keep the effort low, the method includes a workflow to easily create a digital twin of the workplace with sufficient accuracy. In addition to the pre-estimation of the human-cobot process, the method allows an easy planning of collaborative production processes, which is why we suggest a conceptual integration into a process planning and control framework. The proposed approach is demonstrated in an use case by analyzing the potential for introduction of collaborative robots for the production of currently manually assembled solar inverters.
ER  - 

TY  - JOUR
T1  - Translating meaning representations to behavioural interface specifications
AU  - Leong, Iat Tou
AU  - Barbosa, Raul
JO  - Journal of Systems and Software
VL  - 211
SP  - 112009
PY  - 2024
DA  - 2024/05/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2024.112009
UR  - https://www.sciencedirect.com/science/article/pii/S0164121224000529
KW  - Software engineering
KW  - Formal specification
KW  - Software verification
KW  - Java modelling language
KW  - Compilers
AB  - Higher-order logic can be used for meaning representation in natural language processing to encode the semantic relationships in text. Alternatively, using a formal specification language for meaning representation is more precise for specifying programs and widely supported by automatic theorem provers, while deductive verification based on higher order logic is less common for mainstream programming languages. This paper addresses the research question of translating higher-order logic meaning representations generated from method-level code comments into a formal specification language that extends first-order logic. Doing so requires resolving possible ambiguities in determining the appropriate semantics for predicates. This is an open challenge in the path toward using natural language processing with formal methods. To address this, the paper proposes an approach and constructs a compiler for translating meaning representations, generated from Java programs with method-level comments, into Java Modelling Language. We evaluate the compiler on a set of representative benchmarks, including programs and specifications from the Java API, by generating Java Modelling Language specifications and statically checking them with a theorem prover. Results show that in 94% of the cases Java Modelling Language is accurately generated and in 97% of those cases it can be automatically checked with a state-of-the-art theorem prover.
ER  - 

TY  - JOUR
T1  - Semantics–informed geological maps: Conceptual modeling and knowledge encoding
AU  - Lombardo, Vincenzo
AU  - Piana, Fabrizio
AU  - Mimmo, Dario
JO  - Computers & Geosciences
VL  - 116
SP  - 12
EP  - 22
PY  - 2018
DA  - 2018/07/01/
SN  - 0098-3004
DO  - https://doi.org/10.1016/j.cageo.2018.04.001
UR  - https://www.sciencedirect.com/science/article/pii/S0098300416302321
KW  - Geologic knowledge encoding
KW  - Geologic unit ontology
KW  - Geodatabase
KW  - Geological map
KW  - Conceptual modeling of geologic knowledge
KW  - Automatic reasoning
AB  - This paper introduces a novel, semantics-informed geologic mapping process, whose application domain is the production of a synthetic geologic map of a large administrative region. A number of approaches concerning the expression of geologic knowledge through UML schemata and ontologies have been around for more than a decade. These approaches have yielded resources that concern specific domains, such as, e.g., lithology. We develop a conceptual model that aims at building a digital encoding of several domains of geologic knowledge, in order to support the interoperability of the sources. We apply the devised terminological base to the classification of the elements of a geologic map of the Italian Western Alps and northern Apennines (Piemonte region). The digitally encoded knowledge base is a merged set of ontologies, called OntoGeonous. The encoding process identifies the objects of the semantic encoding, the geologic units, gathers the relevant information about such objects from authoritative resources, such as GeoSciML (giving priority to the application schemata reported in the INSPIRE Encoding Cookbook), and expresses the statements by means of axioms encoded in the Web Ontology Language (OWL). To support interoperability, OntoGeonous interlinks the general concepts by referring to the upper part level of ontology SWEET (developed by NASA), and imports knowledge that is already encoded in ontological format (e.g., ontology Simple Lithology). Machine-readable knowledge allows for consistency checking and for classification of the geological map data through algorithms of automatic reasoning.
ER  - 

TY  - JOUR
T1  - Investigation on test effort estimation of mobile applications: Systematic literature review and survey
AU  - Kaur, Anureet
AU  - Kaur, Kulwant
JO  - Information and Software Technology
VL  - 110
SP  - 56
EP  - 77
PY  - 2019
DA  - 2019/06/01/
SN  - 0950-5849
DO  - https://doi.org/10.1016/j.infsof.2019.02.003
UR  - https://www.sciencedirect.com/science/article/pii/S095058491930031X
KW  - Mobile applications
KW  - Test effort estimation
KW  - Software engineering
KW  - Systematic literature review (SLR)
KW  - Survey
AB  - Context
In the last few years, the exigency of mobile devices has proliferated to prodigious heights. The process of developing the mobile software/application proceeds amidst testing phase to verify the correctness of the mobile app. The estimation of testing plays a vital role in the effective completion of testing.
Objective
To identify how estimation of test effort for mobile applications is distinct from other software via published literature and from mobile software organizations. Second is to recognize different issues in adapting traditional test estimation methods to the mobile domain and if suggestions from survey results could be helpful in providing an improved test estimation model for mobile applications.
Method
A systematic literature review is conducted followed by a survey through an online questionnaire filled from experienced mobile application developers and testers.
Results
The results from SLR cover identification of mobile app specific characteristics and reports test effort estimation techniques in the mobile domain. Findings from survey corroborate that a) Function Point/Test Point Analysis is highly adapted traditional test estimation technique to mobile domain; b) Challenges like uncertain requirements, no tool support for test estimation, complexity in testing, client miscommunication etc. are reported; c)Suggestions to improve test estimation process include proper test planning, adoption of agile methodology, healthier communication among client, developer, and tester etc.; d) On the basis of responses, Analytical Hierarchical Process (AHP) identifies “Diverse Devices and OS” along with “Type of App” as highly influential mobile app characteristic on the test estimation process.
Conclusion
Results conclude that the importance of identified mobile app characteristics from SLR cannot be ignored in the estimation process of mobile software testing. There might be a possibility to improve existing test estimation techniques for mobile apps by giving weight to mobile app specific characteristics and by considering suggestions from experienced developers and testers.
ER  - 

TY  - JOUR
T1  - Modelling temporal goals in runtime goal models
AU  - Morgan, Rebecca
AU  - Pulawski, Simon
AU  - Selway, Matt
AU  - Ghose, Aditya
AU  - Grossmann, Georg
AU  - Mayer, Wolfgang
AU  - Stumptner, Markus
AU  - Kyprianou, Ross
JO  - Data & Knowledge Engineering
VL  - 147
SP  - 102205
PY  - 2023
DA  - 2023/09/01/
SN  - 0169-023X
DO  - https://doi.org/10.1016/j.datak.2023.102205
UR  - https://www.sciencedirect.com/science/article/pii/S0169023X23000654
KW  - Goal modelling
KW  - Self-adaptive systems
KW  - Context awareness
AB  - Achieving real-time agility and adaptation with respect to changing requirements in existing IT infrastructure can pose a complex challenge. We describe a goal-oriented approach to manage this complexity. We argue that a goal-oriented perspective can form an effective basis for devising and deploying responses to changed requirements at runtime. We offer an extended vocabulary of goal types by presenting two novel conceptions: differential goals and integral goals, which we formalize in both linear-time and branching-time settings. We describe goal lifecycles and interactions and the extended notion of context for the representation of rapidly changing, complex operating environments. We then illustrate the working of the approach by presenting a detailed scenario of adaptation in a Kubernetes setting, in the face of a Distributed Denial-of-Service (DDoS) attack.
ER  - 

TY  - JOUR
T1  - A Functional Classification of Text Annotations for Engineering Design
AU  - Company, Pedro
AU  - Camba, Jorge D.
AU  - Patalano, Stanislao
AU  - Vitolo, Ferdinando
AU  - Lanzotti, Antonio
JO  - Computer-Aided Design
VL  - 158
SP  - 103486
PY  - 2023
DA  - 2023/05/01/
SN  - 0010-4485
DO  - https://doi.org/10.1016/j.cad.2023.103486
UR  - https://www.sciencedirect.com/science/article/pii/S0010448523000180
KW  - Annotations
KW  - Model-based definition
KW  - Text annotations
AB  - Describing and supplementing geometric shapes (parts) and layouts (assemblies) with relevant information is key for successful product design communication. 3D annotation tools are widely available in commercial systems, but they are generally used in the same manner as 2D annotations in traditional engineering drawings. The gap between technology and practices is particularly evident in plain text annotations. In this paper, we introduce a functional classification of text annotations to provide an information framework for shifting traditional annotation practices towards the Model-Based Definition (MBD) paradigm. In our view, the current classification of dimensions, tolerances, symbols, notes, and text does not stress the inherent properties of two broader categories: symbols and text. Symbol-based annotations use a symbolic language (mostly standardized) such as Geometric Dimensioning and Tolerancing (GD&T) to provide precise information about the implications of geometric imperfections in manufacturing, whereas notes and text are based on non-standardized and unstructured plain text, and can be used to convey design information. We advocate that text annotations can be characterized in four different functional types (objectives, requirements, rationale, and intent), which should be classified as such when annotations are added to a model. The identification and definition of a formalized structure and syntax can enable the management of the annotations as separate entities, thus leveraging their individual features, or as a group to gain a global and collective view of the design problem. The proposed classification was tested with a group of users in a redesign task that involved a series of geometric changes to an annotated assembly model.
ER  - 

TY  - JOUR
T1  - A note on the applications of artificial intelligence in the hospitality industry: preliminary results of a survey
AU  - Citak, Joanna
AU  - Owoc, Mieczysław L.
AU  - Weichbroth, Paweł
JO  - Procedia Computer Science
VL  - 192
SP  - 4552
EP  - 4559
PY  - 2021
DA  - 2021/01/01/
T2  - Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2021.09.233
UR  - https://www.sciencedirect.com/science/article/pii/S1877050921019724
KW  - Artificial Intelligence
KW  - Hotel Industry
KW  - Applications
AB  - Intelligent technologies are widely implemented in different areas of modern society but specific approaches should be applied in services. Basic relationships refer to supporting customers and people responsible for services offering for these customers. The aim of the paper is to analyze and evaluate the state-of-the art of artificial intelligence (AI) applications in the hospitality industry. Our findings show that the major deployments concern in-person customer services, chatbots and messaging tools, business intelligence tools powered by machine learning, and virtual reality & augmented reality. Moreover, we performed a survey (n = 178), asking respondents about their perceptions and attitudes toward AI, including its implementation within a hotel space. The paper attempts to discuss how the hotel industry can be motivated by potential customers to apply selected AI solutions. In our opinion, these results provide useful insights for understanding the phenomenon under investigation. Nevertheless, since the results are not conclusive, more research is still needed on this topic. Future studies may concern both qualitative and quantitative methods, devoted to developing models that: a) quantify the potential benefits and risks of AI implementations, b) determine and evaluate the factors affecting the AI adoption by the customers, and c) measure the user (guest) experience of the hotel services, fueled by AI-based technologies.
ER  - 

TY  - JOUR
T1  - Mapping singly-linked rules to linear temporal logic formulas
AU  - Mackey, Isaac
AU  - Su, Jianwen
JO  - Information Systems
VL  - 117
SP  - 102222
PY  - 2023
DA  - 2023/07/01/
SN  - 0306-4379
DO  - https://doi.org/10.1016/j.is.2023.102222
UR  - https://www.sciencedirect.com/science/article/pii/S0306437923000583
KW  - Business process management
KW  - Service provisioning
KW  - Runtime monitoring
KW  - Rules
KW  - Linear temporal logic
KW  - Violation detection
AB  - Business services are provided by enacting interrelated business processes. Service providers must ensure enactments comply with policies, regulations, and business rules, including rules with quantitative time constraints. Enforcing such rules at design-time may be too restrictive, so effective service provisioning includes expressing rules in a formal specification language and detecting violations of these rules at runtime. Many specification languages do not include quantitative time constraints; for languages with such constraints, it is often unknown if they have runtime monitors whose auxiliary data storage is of bounded size. In this paper, we formulate a technical model of services, a logic language with quantitative time constraints for specifying rules, and develop techniques for automatically generating monitors to detect rule violations. This approach involves two steps, translating: (1) rules to formulas in linear temporal logic (LTL) on finite traces, and (2) LTL formulas to finite state machines. Since algorithms exist for step (2), we focus on step (1), i.e., mapping rules to equivalent LTL formulas. We present and establish the correctness of two translation techniques for “singly-linked” rules. We also compare the size of formulas produced by these techniques with a method of translation derived from Kamp’s Theorem, showing an improvement from hyper-exponential to exponential size.
ER  - 

TY  - JOUR
T1  - The evolution of a toolkit for smart-thing design with children through action research
AU  - Gennari, Rosella
AU  - Matera, Maristella
AU  - Melonio, Alessandra
AU  - Rizvi, Mehdi
AU  - Roumelioti, Eftychia
JO  - International Journal of Child-Computer Interaction
VL  - 31
SP  - 100359
PY  - 2022
DA  - 2022/03/01/
SN  - 2212-8689
DO  - https://doi.org/10.1016/j.ijcci.2021.100359
UR  - https://www.sciencedirect.com/science/article/pii/S2212868921000660
KW  - Smart thing
KW  - Toolkit
KW  - Action research
KW  - Card
KW  - Game
KW  - Design
KW  - Reflection
KW  - Child
AB  - Several workshops use toolkits to engage children in the design of smart things, that is, everyday things like toys enhanced with computing devices and capabilities. In general, the toolkits focus on one design stage or another, e.g., ideation or programming. Few toolkits are created to guide children through an entire design process. This paper presents a toolkit for smart-thing design with children. It revolves around SNaP, a card-based board game for children. The toolkit serves to frame the entire design process and guide them through their exploration, ideation, programming and prototyping of their own smart things. By embracing action research, the toolkit was adopted in actions with children, namely, design workshops. Results of actions were reflected over by considering children’s benefits, and they were used to make the toolkit evolve across cycles of action, reflection and development. The paper reports on the latest evolution cycles, ending with the 2020 cycle for continuing smart-thing design during COVID-19 times. The paper concludes with general reflections concerning action research and design with children, toolkits for framing smart-thing design with children, on-going and future work.
ER  - 

TY  - JOUR
T1  - DMN4DQ: When data quality meets DMN
AU  - Valencia-Parra, Álvaro
AU  - Parody, Luisa
AU  - Varela-Vaca, Ángel Jesús
AU  - Caballero, Ismael
AU  - Gómez-López, María Teresa
JO  - Decision Support Systems
VL  - 141
SP  - 113450
PY  - 2021
DA  - 2021/02/01/
SN  - 0167-9236
DO  - https://doi.org/10.1016/j.dss.2020.113450
UR  - https://www.sciencedirect.com/science/article/pii/S0167923620302050
KW  - Data usability
KW  - Data quality
KW  - Decision model and notation
KW  - Data quality rule
KW  - Data quality assessment
KW  - Data quality measurement
AB  - To succeed in their business processes, organizations need data that not only attains suitable levels of quality for the task at hand, but that can also be considered as usable for the business. However, many researchers ground the potential usability of the data on its quality. Organizations would benefit from receiving recommendations on the usability of the data before its use. We propose that the recommendation on the usability of the data be supported by a decision process, which includes a context-dependent data-quality assessment based on business rules. Ideally, this recommendation would be generated automatically. Decision Model and Notation (DMN) enables the assessment of data quality based on the evaluation of business rules, and also, provides stakeholders (e.g., data stewards) with sound support for the automation of the whole process of generation of a recommendation regarding usability based on data quality. The main contribution of the proposal involves designing and enabling both DMN-driven mechanisms and a guiding methodology (DMN4DQ) to support the automatic generation of a decision-based recommendation on the potential usability of a data record in terms of its level of data quality. Furthermore, the validation of the proposal is performed through the application of a real dataset.
ER  - 

TY  - JOUR
T1  - CyberShip-IoT: A dynamic and adaptive SDN-based security policy enforcement framework for ships
AU  - Sahay, Rishikesh
AU  - Meng, Weizhi
AU  - Estay, D.A. Sepulveda
AU  - Jensen, Christian D.
AU  - Barfod, Michael Bruhn
JO  - Future Generation Computer Systems
VL  - 100
SP  - 736
EP  - 750
PY  - 2019
DA  - 2019/11/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2019.05.049
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X1930367X
KW  - Internet-of-Things
KW  - OpenFlow
KW  - SDN
KW  - Ship system
KW  - Policy language and enforcement
KW  - SCADA system
AB  - With the wide adoption of Information and Communication Technology (ICT) in the marine environment, ship systems are increasingly similar to other networked computing systems. The integration of positioning systems with navigational and propulsion control systems and the increasing reliance on Supervisory Control And Data Acquisition (SCADA) systems for monitoring the ship’s performance makes modern ships vulnerable to a wide range of cyber security issues. Moreover, frequent or permanent onshore connection makes the ship’s communication network a potential target for cyber-criminals. Such attacks can incapacitate the vessel, i.e., through a ransomware attack, or greatly degrade the performance of the ship systems, i.e., causing delays in the propagation of control messages between critical components within the ship. Furthermore, crew members and marine engineers are challenged with the task of configuring security policies for networked devices, using low-level device specific syntax, which is an error prone and time consuming process. In addition to this, crew members must also be familiar with the specific syntax for low-level network management task, which exacerbates the problem. The emergence of Software-Defined Networking (SDN) helps reduce the complexity of the network management tasks and we believe that a similar approach may be used to address the larger problem. We therefore propose the CyberShip-IoT framework to provide a network level defense for the communication network component of ship systems. CyberShip-IoT offers a high-level policy language and a translation mechanism for automated policy enforcement in the ship’s communication network. The modular design of the framework provides flexibility to deploy detection mechanism according to their requirements. To evaluate the feasibility and effectiveness of this framework, we develop a prototype for a scenario involving the communication network of a typical ship. The experimental results demonstrate that our framework can effectively translate high-level security policies into OpenFlow rules of the switches without incurring much latency, ultimately leading to efficient attack mitigation and reduced collateral damage.
ER  - 

TY  - JOUR
T1  - An impact of ontology-based service-oriented ecosystems on digital transformation of railway transport and engineering education
AU  - Khabarov, Valeriy
AU  - Volegzhanina, Irina
JO  - Transportation Research Procedia
VL  - 63
SP  - 1899
EP  - 1908
PY  - 2022
DA  - 2022/01/01/
T2  - X International Scientific Siberian Transport Forum — TransSiberia 2022
SN  - 2352-1465
DO  - https://doi.org/10.1016/j.trpro.2022.06.210
UR  - https://www.sciencedirect.com/science/article/pii/S2352146522004653
KW  - railway transport
KW  - intelligent tutoring agent
KW  - education course ontology
KW  - industry-related university
KW  - service-oriented business ecosystem
KW  - digital transformation
AB  - The scientific novelty of the article is to ground the digital transformation of railway universities through the development of an ontology-based service-oriented ecosystem. The analysis of scientific publications and results of technical and technological initiatives in the field of railway transport revealed that the industry business ecosystem based on ontologies would determine the nature of digital transformation in railway universities. This idea, however, has not yet been sufficiently developed in education studies. It is obvious that ontologisation will require new conceptual solutions and tools to be implemented. The authors propose a new didactic concept called "knowledge factory". The implementation of this didactic concept requires the development of ontology-based didactic tools. In particular, standardisation of education content through the concepts and relations in ontologies makes it possible to develop a web-application such as an intelligent tutoring agent. The study involved two stages. The first stage - conceptual - grounded the new "knowledge factory" didactic concept and the ontological model of interaction between the railway industry and railway universities. For this purpose, the authors applied the methods of ecosystem and ontological-semantic approaches. At the second stage - practical - an attempt was made to develop a prototype of an intelligent tutoring agent. With this purpose a fragment of ontology for the education course "Artificial Intelligence Systems in Transport: Agent-Based Approach" was developed. The top-down method was applied to develop this ontology. What is also emphasized is that the findings are to a large extent transferable to many industries.
ER  - 

TY  - JOUR
T1  - Temporal relation identification in functional requirements
AU  - Onishi, Maiko
AU  - Ogata, Shinpei
AU  - Okano, Kozo
AU  - Bekki, Daisuke
JO  - Procedia Computer Science
VL  - 225
SP  - 1161
EP  - 1170
PY  - 2023
DA  - 2023/01/01/
T2  - 27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2023.10.104
UR  - https://www.sciencedirect.com/science/article/pii/S1877050923012620
KW  - Software Engineering
KW  - Natural Language Processing
KW  - Temporal Relation Identification
KW  - Requirements Specification
AB  - In this study, we propose a method for applying a temporal relation identification model to functional requirements. We discuss the limited availability of data in the requirements engineering domain compared to other fields when used for supervised learning, and therefore employ a corpus from the news domain for training. The experimental results demonstrate that the types of temporal relations present in functional requirements are limited, indicating that focusing on learning with a narrowed set of labels is effective. Additionally, We incorporate Dependency Path (DP) into the temporal relation identification model and report, through comparative experiments, that leveraging DP is effective, but minor modifications to DP do not lead to significant improvements in accuracy. By demonstrating specific application methods of temporal relation identification in requirements engineering, we anticipate contributing to the analysis of functional requirements in software development.
ER  - 

TY  - JOUR
T1  - Ambiguity in user stories: A systematic literature review
AU  - Amna, Anis R.
AU  - Poels, Geert
JO  - Information and Software Technology
VL  - 145
SP  - 106824
PY  - 2022
DA  - 2022/05/01/
SN  - 0950-5849
DO  - https://doi.org/10.1016/j.infsof.2022.106824
UR  - https://www.sciencedirect.com/science/article/pii/S0950584922000040
KW  - Requirements engineering
KW  - Agile software development
KW  - User story
KW  - Ambiguity
KW  - Systematic Literature Review
AB  - Context
Ambiguity in user stories is a problem that has received little research attention. Due to the absence of review studies, it is not known how and to what extent this problem, which impacts the effectiveness of user stories in supporting systems development, has been solved.
Objectives
We review the studies that investigate or develop solutions for problems related to ambiguity in user stories. We investigate how these problems manifest themselves, what their causes and consequences are, what solutions have been proposed and what evidence of their effectiveness has been presented. Based on the insights we obtain from this review, we identify research gaps and suggest opportunities for future research.
Methods
We followed Systematic Literature Review guidelines to review problems investigated, solutions proposed, and validation/evaluation methods used. We classified the reviewed studies according to the four linguistic levels of ambiguity (i.e., lexical, syntactic, semantic, pragmatic) proposed by Berry and Kamsties to obtain insights from patterns that we observe in the classification of problems and solutions.
Results
A total of 36 studies published in 2001–2020 investigated ambiguity in user stories. Based on four patterns we discern, we identify three research gaps. First, we need more research on human behaviors and cognitive factors causing ambiguity. Second, ambiguity is seldom studied as a problem of a set of related user stories, like a theme or epic in Scrum. Third, there is a lack of holistic solution approaches that consider ambiguity at multiple linguistic levels.
Conclusion
Ambiguity in user stories is a known problem. However, a comprehensive solution for addressing ambiguity in a set of related user stories as it manifests itself at different linguistic levels as a cognitive problem is lacking.
ER  - 

TY  - JOUR
T1  - A trait-based typification of urban forests as nature-based solutions
AU  - Scheuer, Sebastian
AU  - Jache, Jessica
AU  - Kičić, Martina
AU  - Wellmann, Thilo
AU  - Wolff, Manuel
AU  - Haase, Dagmar
JO  - Urban Forestry & Urban Greening
VL  - 78
SP  - 127780
PY  - 2022
DA  - 2022/12/01/
SN  - 1618-8667
DO  - https://doi.org/10.1016/j.ufug.2022.127780
UR  - https://www.sciencedirect.com/science/article/pii/S1618866722003235
KW  - Urban forest
KW  - Nature-based solution
KW  - Typology
KW  - Trait-based modelling
KW  - Semantics
KW  - Ontology
AB  - Urban forests as nature-based solutions (UF-NBS) are important tools for climate change adaptation and sustainable development. However, achieving both effective and sustainable UF-NBS solutions requires diverse knowledge. This includes knowledge on UF-NBS implementation, on the assessment of their environmental impacts in diverse spatial contexts, and on their management for the long-term safeguarding of delivered benefits. A successful integration of such bodies of knowledge demands a systematic understanding of UF-NBS. To achieve such an understanding, this paper presents a conceptual UF-NBS model obtained through a semantic, trait-based modelling approach. This conceptual model is subsequently implemented as an extendible, re-usable and interoperable ontology. In so doing, a formal, trait-based vocabulary on UF-NBS is created, that allows expressing spatial, morphological, physical, functional, and institutional UF-NBS properties for their typification and a subsequent integration of further knowledge and data. Thereby, ways forward are opened for a more systematic UF-NBS impact assessment, management, and decision-making.
ER  - 

TY  - JOUR
T1  - Extensible Worker Assistance (EWA): Presenting a Comprehensive Framework for Context-Aware Assistance in Manual Assembly
AU  - Binder, Eva
AU  - Romer, Michael
AU  - Engesser, Patrick
AU  - Lehwald, Jannes
JO  - Procedia CIRP
VL  - 112
SP  - 501
EP  - 506
PY  - 2022
DA  - 2022/01/01/
T2  - 15th CIRP Conference on Intelligent Computation in ManufacturingEngineering, 14-16 July 2021
SN  - 2212-8271
DO  - https://doi.org/10.1016/j.procir.2022.09.055
UR  - https://www.sciencedirect.com/science/article/pii/S2212827122012094
KW  - Worker Assistance
KW  - Manual Assembly
KW  - Framework
KW  - Context-Aware
AB  - To cope with the complexification of manual assembly, new assistance methods are developed continuously. However, those hardware-dependent methods are not deployed context-aware. Hence, workers are not supported situationally and new methods have to be implemented at great expense on a heterogeneous system landscape, evoking an inappropriate maintenance effort. As known from the plant engineering, standardized encapsulation of specific methods provides a solution to integrate heterogeneous applications into one generic system. Therefore, besides the propose of a novel Extensible Worker Assistance (EWA) framework, the underlying novel concepts of so-called Assistance Model Units (AMUs) is utilized as a standardized way to abstract from specific implementations and thus, enable the integration of various assistance methods into one generic system. Furthermore, the applicability of the EWA framework with its underlying core concepts is shown by a use case-specific implementation within the bus assembly. Hence, a first step towards the provision of an optimal worker assistance tailored to the individual needs is done, by the presentation of the EWA framework with the ability to integrate different assistance methods, devices and consider various contextual knowledge within the context-aware assistance selection. Future work has to be done to further develop and investigate the single components of the comprehensive framework.
ER  - 

TY  - JOUR
T1  - Smart-thing design by children at a distance: How to engage them and make them learn
AU  - Roumelioti, Eftychia
AU  - Pellegrino, Maria Angela
AU  - Rizvi, Mehdi
AU  - D’Angelo, Mauro
AU  - Gennari, Rosella
JO  - International Journal of Child-Computer Interaction
VL  - 33
SP  - 100482
PY  - 2022
DA  - 2022/09/01/
SN  - 2212-8689
DO  - https://doi.org/10.1016/j.ijcci.2022.100482
UR  - https://www.sciencedirect.com/science/article/pii/S2212868922000198
KW  - Design
KW  - Programming
KW  - At a distance
KW  - Online
KW  - Child
KW  - Smart thing
KW  - Learning
KW  - Engagement
KW  - Guidelines
KW  - Challenges
KW  - Framework
KW  - Toolkit
KW  - COVID-19 pandemic
KW  - Software metrics
AB  - In recent years, research in Child–ComputerInteraction has shifted the focus from design with children, giving them a voice in the design process, to design by children to bring child participants different benefits, such as engagement and learning. However, design workshops, encompassing different stages, are challenging in terms of engagement and learning, e.g., they require prolonged commitment and concentration. They are potentially more challenging when held at a distance, as in recent years due to the COVID-19 pandemic. This paper explores at-a-distance smart-thing design by children, how it can engage different children and support their learning in programming. The paper reports a series of design workshops with 20 children, aged from 8 to 16 years old, all held at a distance. They were all organised with the DigiSNaP design framework and toolkit. The first workshop enabled children to explore what smart things are, to start ideating their own smart things and to scaffold their programming. The other workshops enabled children to evolve their own smart-thing ideas and programs. Data were gathered in relation to children’s engagement and learning from different sources. Results are promising for future editions of smart-thing design at a distance or in a hybrid modality. They are discussed along with guidelines for smart-thing design by children at a distance.
ER  - 

TY  - JOUR
T1  - Flexible entity marks and a fine-grained style control for knowledge based natural answer generation
AU  - Huang, Yongjie
AU  - Yang, Meng
AU  - Yang, Ni
JO  - Knowledge-Based Systems
VL  - 243
SP  - 108248
PY  - 2022
DA  - 2022/05/11/
SN  - 0950-7051
DO  - https://doi.org/10.1016/j.knosys.2022.108248
UR  - https://www.sciencedirect.com/science/article/pii/S0950705122000752
KW  - Entity marks
KW  - Style control
KW  - Natural answer generation
AB  - Knowledge-based natural answer generation systems are generally difficult to train because numerous entities rarely appear. One way is to replace the entities with their respective types. However, the entity type requires additional recognition with limited accuracy and ignores semantic meanings. Consequently, we propose a question answering system with flexible marks, copying, and retrieving mechanisms (MarkCRQA) to generate natural and accurate answers. By requiring random marks to be shared among all entity types, MarkCRQA attaches the marks to the entities in questions, answers, and knowledge base, which avoids the additional recognition process for entity types and reduces the training difficulty. In addition, we propose to finely control the naturalness and knowledge level of each answer for different real-world scenarios and user needs. Experiments show that MarkCRQA achieves state-of-the-art performance on two open-domain question answering datasets.
ER  - 

TY  - CHAP
T1  - Chapter 14 - Rental Cars-R-Us case study
AU  - Harmon, Paul
A2  - Harmon, Paul
BT  - Business Process Change (Fourth Edition)
PB  - Morgan Kaufmann
SP  - 343
EP  - 366
PY  - 2019
DA  - 2019/01/01/
SN  - 978-0-12-815847-0
DO  - https://doi.org/10.1016/B978-0-12-815847-0.00014-5
UR  - https://www.sciencedirect.com/science/article/pii/B9780128158470000145
KW  - As-Is diagram
KW  - Business Process Model and Notation (BPMN) diagram
KW  - Cause-effect diagram
KW  - Concept model
KW  - Human performance problems
KW  - Organization analysis
KW  - Policies and business rules
KW  - Problem analysis worksheet
KW  - Process management problems
KW  - Scope diagramming
KW  - Stakeholder analysis
KW  - Subprocess analysis
KW  - To-Be diagram
KW  - Use case diagram
AB  - This chapter walks us through the phases of an auto rental business redesign project showing how a process redesign team applies the redesign methodology to a real process problem.
ER  - 

TY  - JOUR
T1  - Formalisms of Representing Knowledge
AU  - Patel, Archana
AU  - Jain, Sarika
JO  - Procedia Computer Science
VL  - 125
SP  - 542
EP  - 549
PY  - 2018
DA  - 2018/01/01/
T2  - The 6th International Conference on Smart Computing and Communications
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2017.12.070
UR  - https://www.sciencedirect.com/science/article/pii/S187705091732834X
KW  - Knowledge base
KW  - Knowledge Representation
KW  - Semantic Web
KW  - OWL
KW  - Logic
AB  - Incomplete, imprecise and large volume of data generates the concept of knowledge base. Knowledge base which is collection of facts, procedures and meaning is much better than database because it provides the power of reasoning, with the help of which the complicated questions are solved. Knowledge representation is a method to encode knowledge, beliefs, action, feeling, goals, desires, preferences and all other mental states in the Knowledge base. Semantic web defines standards for exchanging knowledge via coherent knowledge base. To develop a good knowledge base it is necessary to have good knowledge representation. For this reason, knowledge representation is our main consideration. This paper gives an overview on knowledge representation aspects in the context of semantic web.
ER  - 

TY  - JOUR
T1  - Towards an automatic model-based Scrum Methodology
AU  - Chantit, Salima
AU  - ESSEBAA, Imane
JO  - Procedia Computer Science
VL  - 184
SP  - 797
EP  - 802
PY  - 2021
DA  - 2021/01/01/
T2  - The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2021.03.099
UR  - https://www.sciencedirect.com/science/article/pii/S1877050921007407
KW  - Model Driven Engineering
KW  - Model Driven Architecture
KW  - Model-Based Testing
KW  - Agile Methodologies
KW  - Scrum
KW  - V development Life Cycle
AB  - Software systems evolve continuously and must be developed quickly to fit user requirements and new advances in technology. This has led the software engineering to propose several methods and approaches to overcome the development and maintenance of these software systems. In this regard, Agile Methodologies and Model-Driven Engineering (MDE) are two main approaches that have emerged in recent years and suggest a solution to some of the issues associated with Software systems developments. MDE focuses on software reuse through models and on generative approaches based on separation of concerns whereas Agile Methods promote the use of simpler models and best practices for programming to achieve quick feedback from clients within a development process. However, these two approaches have evolved separately and there are only a few works related to their combination. This paper presents a customized V development life cycle based on models which combines the two MDE variants: The MDA approach in the V left branch with the MBT approach to generate tests of the V right branch. In addition, we integrate this customized V life cycle in the agile Scrum methodology to facilitate the management of each Scrum sprint.
ER  - 

TY  - JOUR
T1  - Construction of domain ontology utilizing formal concept analysis and social media analytics
AU  - Jindal, Rajni
AU  - Seeja, K.R.
AU  - Jain, Shivani
JO  - International Journal of Cognitive Computing in Engineering
VL  - 1
SP  - 62
EP  - 69
PY  - 2020
DA  - 2020/06/01/
SN  - 2666-3074
DO  - https://doi.org/10.1016/j.ijcce.2020.11.003
UR  - https://www.sciencedirect.com/science/article/pii/S2666307420300103
KW  - Ontology
KW  - Semantic web
KW  - Formal concept analysis
KW  - Fluent editor
KW  - Twitter
KW  - Social media analysis
AB  - Semantic Web, deals with the meaning of information in a defined domain and Ontologies are the backbone of Semantic Web. Domain Ontologies are crucial source of information for knowledge-based system. Still, domain ontology development is a labor- intensive process and is highly dependent on developer's knowledge. In this work, a novel semiautomatic method is proposed to build an ontology on terrorism domain. Terrorism activities provide crucial information to enhance security system for any country worldwide. Social media data, namely, Twitter text data is extracted to attain latest information related to domain and next, concepts and relationships are identified and mapped using formal concept analysis. Several user-defined relationships are presented through fluent editor tool. Also, knowledge is extracted with the help of query-based system through a reasoner window of fluent editor. The developed domain ontology is published on web using ontology web language which can be utilized in other related application areas. The proposed work is significant as it develops a wide-coverage domain ontology for terrorism domain using a tool named Fluent Editor, in place of standard tool protégé and semantic information is extracted similar to query-based system with 100% accuracy.
ER  - 

TY  - JOUR
T1  - Importing participatory practices of the socio-environmental systems community to the process system engineering community: An application to supply chain
AU  - Roth, Anastasia
AU  - Pinta, François
AU  - Negny, Stéphane
AU  - Montastruc, Ludovic
JO  - Computers & Chemical Engineering
VL  - 155
SP  - 107530
PY  - 2021
DA  - 2021/12/01/
SN  - 0098-1354
DO  - https://doi.org/10.1016/j.compchemeng.2021.107530
UR  - https://www.sciencedirect.com/science/article/pii/S0098135421003082
KW  - Supply chain
KW  - Multi agent modeling
KW  - Design
AB  - The Process System Engineering community has an extensive knowledge and skills on supply chain design: from the time dimension (production and flow planning) to the space dimension (geographic position of facilities). Nevertheless, supply chains are also social networks where multiple stakeholders have to collaborate while they have different, and sometimes, diverging objectives. For this reason, having a more realistic model representing collaboration between the various stakeholders involved is necessary and new methods that facilitate the development of a shared representation of the system must be introduced. We propose to import a participatory method, PARDI (Problematic, Actors, Resources, Dynamics and Interactions), from the Socio-Environmental System community to the practices of the Process System Engineering community. Based on this method, we develop a participatory process in order to collect the necessary knowledge on the supply chain and its context. Following this participatory process, we then develop an Agent-Based Model as a simulation and decision making tool to support collective scenario analyses and collectively draw solutions with stakeholders. Our participatory modeling approach necessarily imposes a multi stakeholders vision (within the modeling but also in the result analyses) and therefore the search for a modeling consensus. Thus, it brings a better inclusion of social aspects in problem solving which are usually poorly considered leading to implementation failure sometimes. By comparing our approach with the classic one of the Process Systems Engineering community, we highlight the strengths and weaknesses of both and how complementary they can be. A case study on the already existing supply chain of the chestnut wood in Cévennes area (France) illustrates the capabilities of our participatory methodology. It focuses on the socio-economic model design of the first two steps (forestry activities to harvest) in the supply chain as the latter is locked because of economic and social organisation issues. The objective is to find the best action levers to unlock the resistance that forest plot owners have to remove declining wood from their land.
ER  - 

TY  - JOUR
T1  - Product-line assurance cases from contract-based design
AU  - Nešić, Damir
AU  - Nyberg, Mattias
AU  - Gallina, Barbara
JO  - Journal of Systems and Software
VL  - 176
SP  - 110922
PY  - 2021
DA  - 2021/06/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2021.110922
UR  - https://www.sciencedirect.com/science/article/pii/S0164121221000194
KW  - Assurance cases
KW  - Product line engineering
KW  - Contract-based design
AB  - Assurance cases are used to argue in a structured, and evidence-supported way, that a property such as safety or security is satisfied by a system. In some domains however, instead of single systems, product lines with many system-variants are engineered, to satisfy the needs of different customers. In such context, single-system methods for assurance-case creation suffer from scalability issues because the underlying assumption is that the evidence and arguments can be created per system variant. This paper presents a novel method for product-line assurance-case creation where all the arguments and the evidence are created without analyzing each system variant. Consequently, the effort to create an assurance case scales with the complexity of system variants, instead with their number. The method is based on a contract-based design framework for cyber–physical systems, which is extended to define the conditions under which all system variants satisfy a particular property. These conditions are used to define an assurance-case pattern, which can be instantiated for arbitrary product lines. Moreover, the defined pattern is modular to enable step-wise assurance-case creation. Finally, an exploratory case study is performed on a real product-line from the heavy-vehicle manufacturer Scania to evaluate the applicability of the presented method.
ER  - 

TY  - JOUR
T1  - Identifying human intention during assembly operations using wearable motion capturing systems including eye focus
AU  - Manns, Martin
AU  - Tuli, Tadele Belay
AU  - Schreiber, Florian
JO  - Procedia CIRP
VL  - 104
SP  - 924
EP  - 929
PY  - 2021
DA  - 2021/01/01/
T2  - 54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0
SN  - 2212-8271
DO  - https://doi.org/10.1016/j.procir.2021.11.155
UR  - https://www.sciencedirect.com/science/article/pii/S2212827121010532
KW  - Operator 4.0
KW  - human motion capture
KW  - Eye tracker
KW  - human-robot collaboration
AB  - Simulating human motion behavior in assembly operations helps to create efficient collaboration plans for humans and robots. However, identifying human intention may require high quality human motion capture data in order to discriminate micro-actions and human attention. In this regard, a human motion capture setup that combines various systems such as joint body, finger, and eye trackers is proposed in combination with a methodology of identifying the intention of human operators as well as for predicting sequences of activities. The approach may lead to safer human-robot collaboration.
ER  - 

TY  - JOUR
T1  - Extracting Decision Model and Notation models from text using deep learning techniques
AU  - Goossens, Alexandre
AU  - De Smedt, Johannes
AU  - Vanthienen, Jan
JO  - Expert Systems with Applications
VL  - 211
SP  - 118667
PY  - 2023
DA  - 2023/01/01/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2022.118667
UR  - https://www.sciencedirect.com/science/article/pii/S0957417422017043
KW  - Deep learning
KW  - Decision Model and Notation
KW  - DMN
KW  - Decision model extraction
AB  - Companies and organizations often use manuals and guidelines to communicate and execute operational decisions. Decision Model and Notation (DMN) models can be used to model and automate these decisions. Modeling a decision from a textual source, however, is a time intensive and complex activity hence a need for shorter modeling times. This paper studies how NLP deep learning techniques can extract decision models from text faster. In this paper, we study and evaluate an automatic sentence classifier and a decision dependency extractor using NLP deep learning models (BERT and Bi-LSTM-CRF). A large labeled and tagged dataset was collected from real use cases to train these models. We conclude that BERT can be used for the (semi)-automatic extraction of decision models from text.
ER  - 

TY  - JOUR
T1  - Integration of Business Applications with the Blockchain: Odoo and Hyperledger Fabric Open Source Proof of Concept
AU  - Belhi, Abdelhak
AU  - Gasmi, Houssem
AU  - Bouras, Abdelaziz
AU  - Aouni, Belaid
AU  - Khalil, Ibrahim
JO  - IFAC-PapersOnLine
VL  - 54
IS  - 1
SP  - 817
EP  - 824
PY  - 2021
DA  - 2021/01/01/
T2  - 17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021
SN  - 2405-8963
DO  - https://doi.org/10.1016/j.ifacol.2021.08.185
UR  - https://www.sciencedirect.com/science/article/pii/S2405896321009551
KW  - Blockchain
KW  - Supply Chain
KW  - ERP
KW  - Odoo
KW  - Hyperledger Fabric
KW  - Software Integration
AB  - Since its revolution in the financial sector, the blockchain technology disrupted the majority of collaboration-based applications including supply chain management. The supply chain is one of the most important sectors benefiting from all the advantages of blockchain. Through this paper, we are mainly focusing on the practical aspects of integrating blockchain technology with traditional and existing business applications. Indeed, most businesses and corporations will find it hard to shift from traditional architectures to a decentralized one with fears related to upgrading risks, unknown tools, and resistance to change. Thus, we mainly propose several scenarios for blockchain integration focusing on the most used Enterprise Resource Planning (ERP) platforms. Besides, we present our proof of concept integration that uses the HyperLedger Fabric blockchain platform and the Odoo ERP framework. The selection of these two solutions for our study was mainly influenced by the fact that Hyperledger is the most used open-source blockchain platform in the industry and that Odoo is the most used open-source ERP framework. Nevertheless, we believe that our proposed solution can easily be used with proprietary ERP platforms and business applications.
ER  - 

TY  - JOUR
T1  - ESPRET: A tool for execution time estimation of manual test cases
AU  - Tahvili, Sahar
AU  - Afzal, Wasif
AU  - Saadatmand, Mehrdad
AU  - Bohlin, Markus
AU  - Ameerjan, Sharvathul Hasan
JO  - Journal of Systems and Software
VL  - 146
SP  - 26
EP  - 41
PY  - 2018
DA  - 2018/12/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2018.09.003
UR  - https://www.sciencedirect.com/science/article/pii/S0164121218301778
KW  - Software testing
KW  - Execution time
KW  - Test specification
KW  - Optimization
KW  - Manual testing
KW  - Regression analysis
AB  - Manual testing is still a predominant and an important approach for validation of computer systems, particularly in certain domains such as safety-critical systems. Knowing the execution time of test cases is important to perform test scheduling, prioritization and progress monitoring. In this work, we present, apply and evaluate ESPRET (EStimation and PRediction of Execution Time) as our tool for estimating and predicting the execution time of manual test cases based on their test specifications. Our approach works by extracting timing information for various steps in manual test specification. This information is then used to estimate the maximum time for test steps that have not previously been executed, but for which textual specifications exist. As part of our approach, natural language parsing of the specifications is performed to identify word combinations to check whether existing timing information on various test steps is already available or not. Since executing test cases on the several machines may take different time, we predict the actual execution time for test cases by a set of regression models. Finally, an empirical evaluation of the approach and tool has been performed on a railway use case at Bombardier Transportation (BT) in Sweden.
ER  - 

TY  - CHAP
T1  - Chapter Three - Advances in Applications of Object Constraint Language for Software Engineering
AU  - Jilani, Atif A.
AU  - Iqbal, Muhammad Z.
AU  - Khan, Muhammad U.
AU  - Usman, Muhammad
A2  - Memon, Atif M.
BT  - Advances in Computers
PB  - Elsevier
VL  - 112
SP  - 135
EP  - 184
PY  - 2019
DA  - 2019/01/01/
SN  - 0065-2458
DO  - https://doi.org/10.1016/bs.adcom.2017.12.003
UR  - https://www.sciencedirect.com/science/article/pii/S0065245817300554
KW  - Object Constraint Language
KW  - Model-driven engineering
KW  - Secondary study
KW  - Software engineering
AB  - Object Constraint Language (OCL) is a standard language defined by Object Management Group for specifying constraints on models. Since its introduction as part of Unified Modeling Language, OCL has received significant attention by researchers with works in the literature ranging from temporal extensions of OCL to automated test generation by solving OCL constraints. In this chapter, we provide a survey of the various works discussed in literature related to OCL with the aim of highlighting the advances made in the field. We classify the literature into five broad categories and provide summaries for various works in the literature. The chapter also provides insights and highlights the potentials areas of further research in the field.
ER  - 

TY  - JOUR
T1  - Towards the adoption of OMG standards in the development of SOA-based IoT systems
AU  - Costa, Bruno
AU  - Pires, Paulo F.
AU  - Delicato, Flávia C.
JO  - Journal of Systems and Software
VL  - 169
SP  - 110720
PY  - 2020
DA  - 2020/11/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2020.110720
UR  - https://www.sciencedirect.com/science/article/pii/S0164121220301588
KW  - Internet of Things
KW  - Application
KW  - Model-Driven Development
KW  - Service-Oriented Architecture
AB  - A common feature of the Internet of Things (IoT) is the high heterogeneity, regarding network protocols, data formats, hardware and software platforms. Aiming to deal with such a degree of heterogeneity, several frameworks have applied the Model-Driven Development (MDD) to build IoT applications. On the software architecture viewpoint, the literature has shown that the Service-Oriented Architecture (SOA) is a promising style to address the interoperability of entities composing these solutions. Some features of IoT make it challenging to analyze the impact of design decisions on the SOA-based IoT applications behavior. Thus, it is a key requirement to simulate the model to verify whether the system performs as expected before its implementation. Although the literature has identified that the SOA style is suitable for addressing the interoperability, existing modeling languages do not consider SOA elements as first-class citizens when designing IoT applications. Furthermore, although existing MDD frameworks provide modeling languages comprising well-defined syntax, they lack execution semantics, thus, are not suitable for model execution and analysis. This work aims at addressing these issues by introducing IoTDraw. The framework provides a fully OMG-compliant executable modeling language for SOA-based IoT systems; thus, its specifications can be implemented by any tool implementing OMG standards.
ER  - 

TY  - JOUR
T1  - Recent progress in digital image restoration techniques: A review
AU  - Wali, Aamir
AU  - Naseer, Asma
AU  - Tamoor, Maria
AU  - Gilani, S.A.M.
JO  - Digital Signal Processing
VL  - 141
SP  - 104187
PY  - 2023
DA  - 2023/09/01/
SN  - 1051-2004
DO  - https://doi.org/10.1016/j.dsp.2023.104187
UR  - https://www.sciencedirect.com/science/article/pii/S1051200423002828
KW  - Digital image
KW  - Image restoration
KW  - Degradation
KW  - Noise
KW  - Transformation
AB  - Digital images are playing a progressively important role in almost all the fields such as computer science, medicine, communications, transmission, security, surveillance, and many more. Digital images are susceptible to a number of distortions due to faulty imaging instruments, transmission channels, atmospheric and environmental conditions, etc. resulting in degraded images. Degradation can be of different types such as noise, backscattering, low saturation, low contrast, tilt, spectral absorption, blurring, etc. The degradation reduces digital images' effectiveness and therefore needs to be restored. In this paper, we present an extensive review of image restoration tasks. It addresses problems like image deblurring, denoising, dehazing and super-resolution. Image restoration is fundamentally an image processing problem, but deep learning techniques, based mainly on convolutional neural networks have received a lot of attention in almost all areas of computer science. Along with deep learning, other machine learning methods have also been tried for restoring digital images. In this review, we have therefore categorized digital image restoration techniques as either image processing-based, machine learning-based or deep learning-based. For each category, a variety of approaches presented in recent years have been reviewed. This review also includes a summary of the data sets used for image restoration along with a baseline reference that can be used by future researchers to compare and improve their results. We also suggest some interesting research directions for future work in this area.
ER  - 

TY  - JOUR
T1  - Towards naturalistic programming: Mapping language-independent requirements to constrained language specifications
AU  - Mefteh, Mariem
AU  - Bouassida, Nadia
AU  - Ben-Abdallah, Hanêne
JO  - Science of Computer Programming
VL  - 166
SP  - 89
EP  - 119
PY  - 2018
DA  - 2018/11/15/
SN  - 0167-6423
DO  - https://doi.org/10.1016/j.scico.2018.05.006
UR  - https://www.sciencedirect.com/science/article/pii/S0167642318301941
KW  - Natural language processing
KW  - Patterns
KW  - Semantics
KW  - Multilingual requirements
KW  - Use case scenarios
AB  - This research paper presents a new approach that constitutes a first step towards programming using language-independent requirements. To leverage the needed programming effort, our approach takes requirements in the form of language-independent use case scenarios. Then, it generates the inputs of a code generator which, in turn, produces the corresponding code. To provide for the language-independence, our approach uses an enriched version of the semantic model, as a means to represent similar ideas possibly in different ways and in different natural languages. The enrichment consists of a set of patterns that it implements as XML code representing the information embedded in the use case scenarios. This intermediate representation can be processed to derive the inputs required by any code generator to produce code in a particular programming language. This paper illustrates the approach and its tool support for use case scenarios written in English and French, and semantic model patterns implemented as XML code that can be processed by the ReDSeeDS code generator. In addition, it presents the results of an experimental evaluation of the approach on use case scenarios (written in English and in French) belonging to five different systems. This evaluation quantitatively shows the ability of our approach: to extract ReDSeeDS inputs conforming to the expert's inputs with a high precision; to generate XML code elements conforming to the input with an encouraging performance as evaluated by the participating students (an F-measure ranging between 87.43% and 92.31%); and to generate Java code judged efficient by the participating programmers (an F-measure ranging between 66.4% and 93.43%).
ER  - 

TY  - JOUR
T1  - Extracting SBVR business vocabularies and business rules from UML use case diagrams
AU  - Skersys, Tomas
AU  - Danenas, Paulius
AU  - Butleris, Rimantas
JO  - Journal of Systems and Software
VL  - 141
SP  - 111
EP  - 130
PY  - 2018
DA  - 2018/07/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2018.03.061
UR  - https://www.sciencedirect.com/science/article/pii/S016412121830061X
KW  - SBVR business vocabulary
KW  - SBVR business rules
KW  - UML use case diagram
KW  - Model-to-model transformation
AB  - In model-driven information systems engineering, model transformations reside at the very core of this paradigm. Indeed, model transformations (in particular, model-to-model, or M2M) are a must-have feature of any modern model-driven approach supported by CASE technology. Model transformations are intended to raise quality of the models under development, and also speed-up the modeling itself by bringing in certain level of automation into the development process. Nevertheless, due to certain objective reasons, the level of such automation is spread unevenly throughout the development process – in this respect, Business Modeling and System Analysis are, arguably, the most underdeveloped phases of the model-driven information systems development life cycle. In this paper, we show how M2M transformation technology was used to extract well-structured business vocabularies and business rules from formal use case models represented through a set of use case diagrams; Object Management Group's (OMG) standards Semantics for Business Vocabulary and Rules (SBVR) and Unified Modeling Language (UML) were used for this purpose. The proposed solution consists of two concurrent approaches, namely, automatic and semi-automatic, which may be used selectively to achieve the best expected result. Basic implementation aspects of the solution integrating both approaches are also briefly presented in the paper. While UML use case models is the main subject in this research, the proposed solution may be adopted for other UML and MOF-based models as well.
ER  - 

TY  - JOUR
T1  - Editorial: Defeasible and Ampliative Reasoning
AU  - Booth, Richard
AU  - Casini, Giovanni
AU  - Klarman, Szymon
AU  - Richard, Gilles
AU  - Varzinczak, Ivan
JO  - International Journal of Approximate Reasoning
VL  - 112
SP  - 1
EP  - 3
PY  - 2019
DA  - 2019/09/01/
SN  - 0888-613X
DO  - https://doi.org/10.1016/j.ijar.2019.05.009
UR  - https://www.sciencedirect.com/science/article/pii/S0888613X19302944
ER  - 

TY  - JOUR
T1  - Simulation Modeling of Assembly Processes in Digital Manufacturing
AU  - Kutin, Andrey
AU  - Dolgov, Vitaly
AU  - Podkidyshev, Alexey
AU  - Kabanov, Alexander
JO  - Procedia CIRP
VL  - 67
SP  - 470
EP  - 475
PY  - 2018
DA  - 2018/01/01/
T2  - 11th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 19-21 July 2017, Gulf of Naples, Italy
SN  - 2212-8271
DO  - https://doi.org/10.1016/j.procir.2017.12.246
UR  - https://www.sciencedirect.com/science/article/pii/S2212827117311927
KW  - Simulation modeling
KW  - Assembly processes
KW  - Digital manufacturing
AB  - Method is proposed for simulation modeling of assembly processes in digital manufacturing, allowing considering influence of intersections of main material flows of different products at same workplaces, component supply interruptions, as well as non-productive time losses due to organizational or technical causes on performance parameters of assembly processes. Simulation modeling has shown 34% increase of manufacturing system productivity.
ER  - 

TY  - JOUR
T1  - BIGOWL4DQ: Ontology-driven approach for Big Data quality meta-modelling, selection and reasoning
AU  - Barba-González, Cristóbal
AU  - Caballero, Ismael
AU  - Varela-Vaca, Ángel Jesús
AU  - Cruz-Lemus, José A.
AU  - Gómez-López, María Teresa
AU  - Navas-Delgado, Ismael
JO  - Information and Software Technology
VL  - 167
SP  - 107378
PY  - 2024
DA  - 2024/03/01/
SN  - 0950-5849
DO  - https://doi.org/10.1016/j.infsof.2023.107378
UR  - https://www.sciencedirect.com/science/article/pii/S0950584923002331
KW  - Data quality evaluation and measurement
KW  - Data quality information model
KW  - Big Data
KW  - Ontology
KW  - Decision model and notation
AB  - Context:
Data quality should be at the core of many Artificial Intelligence initiatives from the very first moment in which data is required for a successful analysis. Measurement and evaluation of the level of quality are crucial to determining whether data can be used for the tasks at hand. Conscientious of this importance, industry and academia have proposed several data quality measurements and assessment frameworks over the last two decades. Unfortunately, there is no common and shared vocabulary for data quality terms. Thus, it is difficult and time-consuming to integrate data quality analysis within a (Big) Data workflow for performing Artificial Intelligence tasks. One of the main reasons is that, except for a reduced number of proposals, the presented vocabularies are neither machine-readable nor processable, needing human processing to be incorporated.
Objective:
This paper proposes a unified data quality measurement and assessment information model. This model can be used in different environments and contexts to describe data quality measurement and evaluation concerns.
Method:
The model has been developed as an ontology to make it interoperable and machine-readable. For better interoperability and applicability, this ontology, BIGOWL4DQ, has been developed as an extension of a previously developed ontology for describing knowledge management in Big Data analytics.
Conclusions:
This extended ontology provides a data quality measurement and assessment framework required when designing Artificial Intelligence workflows and integrated reasoning capacities. Thus, BIGOWL4DQ can be used to describe Big Data analysis and assess the data quality before the analysis.
Result:
Our proposal has been validated with two use cases. First, the semantic proposal has been assessed using an academic use case. And second, a real-world case study within an Artificial Intelligence workflow has been conducted to endorse our work.
ER  - 

TY  - JOUR
T1  - Characterizing and evaluating the quality of software process modeling language: Comparison of ten representative model-based languages
AU  - García-García, J.A.
AU  - Enríquez, J.G.
AU  - Domínguez-Mayo, F.J.
JO  - Computer Standards & Interfaces
VL  - 63
SP  - 52
EP  - 66
PY  - 2019
DA  - 2019/03/01/
SN  - 0920-5489
DO  - https://doi.org/10.1016/j.csi.2018.11.008
UR  - https://www.sciencedirect.com/science/article/pii/S0920548918302757
KW  - Quality Evaluation Framework (QuEF)
KW  - Quality Model
KW  - Software Process Modeling Language
KW  - comparative study
AB  - Software organizations are very conscious that deployments of well-defined software processes improve software product development and its quality. Over last decade, many Software Process Modeling Languages (SPMLs) have been proposed to describe and manage software processes. However, each one presents advantages and disadvantages. The main challenge for an organization is to choose the best and most suitable SPML to meet its requirements. This paper proposes a Quality Model (QM) which has been defined conforms to QuEF (Quality Evaluation Framework). This QM allows to compare model-based SPMLs and it could be used by organizations to choose the most useful model-based SPML for their particular requirements. This paper also instances our QM to evaluate and compare 10 representative SPMLs of the various alternative approaches (metamodel-level approaches; SPML based on UML and approaches based on standards). Finally, this paper concludes there are many model-based proposals for SPM, but it is very difficult to establish with could be the commitment to follow. Some non-considered aspects until now have been identified (e.g., validation within enterprise environments, friendly support tools, mechanisms to carry out continuous improvement, mechanisms to establish business rules and elements for software process orchestrating).
ER  - 

TY  - JOUR
T1  - Rapid Trust Calibration through Interpretable and Uncertainty-Aware AI
AU  - Tomsett, Richard
AU  - Preece, Alun
AU  - Braines, Dave
AU  - Cerutti, Federico
AU  - Chakraborty, Supriyo
AU  - Srivastava, Mani
AU  - Pearson, Gavin
AU  - Kaplan, Lance
JO  - Patterns
VL  - 1
IS  - 4
SP  - 100049
PY  - 2020
DA  - 2020/07/10/
SN  - 2666-3899
DO  - https://doi.org/10.1016/j.patter.2020.100049
UR  - https://www.sciencedirect.com/science/article/pii/S266638992030060X
KW  - DSML 1: Concept: Basic principles of a new data science output observed and reported
AB  - Artificial intelligence (AI) systems hold great promise as decision-support tools, but we must be able to identify and understand their inevitable mistakes if they are to fulfill this potential. This is particularly true in domains where the decisions are high-stakes, such as law, medicine, and the military. In this Perspective, we describe the particular challenges for AI decision support posed in military coalition operations. These include having to deal with limited, low-quality data, which inevitably compromises AI performance. We suggest that these problems can be mitigated by taking steps that allow rapid trust calibration so that decision makers understand the AI system's limitations and likely failures and can calibrate their trust in its outputs appropriately. We propose that AI services can achieve this by being both interpretable and uncertainty-aware. Creating such AI systems poses various technical and human factors challenges. We review these challenges and recommend directions for future research.
ER  - 

TY  - JOUR
T1  - Automated business rules and requirements to enrich product-centric information
AU  - Fortineau, Virginie
AU  - Paviot, Thomas
AU  - Lamouri, Samir
JO  - Computers in Industry
VL  - 104
SP  - 22
EP  - 33
PY  - 2019
DA  - 2019/01/01/
SN  - 0166-3615
DO  - https://doi.org/10.1016/j.compind.2018.10.001
UR  - https://www.sciencedirect.com/science/article/pii/S0166361518301477
KW  - Business rule
KW  - Requirements
KW  - Ontology
KW  - PLM
KW  - BIM
AB  - Current PLM or BIM based information systems suffer from a lack of checking components for business rules. One reason is the misunderstanding of the role and nature of business rules, and how they should be treated in a product-centric information system. This paper intends to provide both a process and a related model to build such a component and enrich future systems. Rules and requirements process management enables the unambiguous formalization of implicit knowledge contained in business rules, generally expressed in easily understandable language, and leads to the formal expression of requirements. In this paper, the requirements are considered a consequence of the application of a business rule. A conceptual model is then introduced, called DALTON (DAta Linked Through Occurrences Network), which supports this process. In this ontology, concepts and product data, coming for instance from an existing product database, are represented using instances and occurrences, connected together with triples built from business rules and requirements according to previous management processes. An experiment involving a set of SWRL rules is conducted in the Protégé environment that validates the model and the process.
ER  - 

TY  - JOUR
T1  - CrossPrune: Cooperative pruning for camera–LiDAR fused perception models of autonomous driving
AU  - Lu, Yantao
AU  - Jiang, Bo
AU  - Liu, Ning
AU  - Li, Yilan
AU  - Chen, Jinchao
AU  - Zhang, Ying
AU  - Wan, Zifu
JO  - Knowledge-Based Systems
VL  - 289
SP  - 111522
PY  - 2024
DA  - 2024/04/08/
SN  - 0950-7051
DO  - https://doi.org/10.1016/j.knosys.2024.111522
UR  - https://www.sciencedirect.com/science/article/pii/S0950705124001576
KW  - Camera–LiDAR fusion
KW  - DNN pruning
KW  - Perception of autonomous driving
AB  - Deep neural network pruning is effective in enabling high-performance perception models to be deployed on autonomous driving platforms with limited computation and memory resources. With their rapid development, state-of-the-art autonomous driving perception (ADP) models have advocated the use of multimodal sensors to extract diverse feature categories. However, existing pruning studies in the ADP area focus on single-modal models and neglect multimodal models. Compared with conventional pruning, multimodal pruning presents a new type of redundancy, namely modal-wise redundancy that is caused by multimodal branches extracting similar perception information. When a specific type of information is extracted by more than one modal branch, although this extraction is deemed essential from an individual single-modal standpoint, it becomes redundant when viewed from a modal-wise perspective. Therefore, modal branches must be handled cooperatively to eliminate modal-wise redundancy while concurrently preserving the original perception accuracy. Building on this, we propose CrossPrune, a modal cooperative pruning framework designed for camera–LiDAR fused perception in autonomous driving. The primary objective of CrossPrune is to effectively eliminate redundancy and achieve nondestructive pruning for multimodal ADP models. This was accomplished by approaching the problem as a multi-objective optimization task, encompassing both weight pruning and the restriction of feature distortions caused by pruning. Experiments conducted on the nuScenes and KITTI datasets demonstrated that CrossPrune attained superior pruning ratios while minimizing accuracy loss, surpassing the performance of the baselines. The key results indicated that the proposed CrossPrune achieved relative improvements of 9.6% in mAP and 11.5% in NDS under 89.8% pruning sparsity.
ER  - 

TY  - JOUR
T1  - Preface – 22nd Brazilian Symposium on Formal Methods – SBMF 2019
AU  - Duran, Adolfo
AU  - Wadler, Philip
JO  - Science of Computer Programming
VL  - 201
SP  - 102565
PY  - 2021
DA  - 2021/01/01/
SN  - 0167-6423
DO  - https://doi.org/10.1016/j.scico.2020.102565
UR  - https://www.sciencedirect.com/science/article/pii/S0167642320301738
ER  - 

TY  - JOUR
T1  - A comprehensive review on resolving ambiguities in natural language processing
AU  - Yadav, Apurwa
AU  - Patel, Aarshil
AU  - Shah, Manan
JO  - AI Open
VL  - 2
SP  - 85
EP  - 92
PY  - 2021
DA  - 2021/01/01/
SN  - 2666-6510
DO  - https://doi.org/10.1016/j.aiopen.2021.05.001
UR  - https://www.sciencedirect.com/science/article/pii/S2666651021000127
KW  - Natural language processing
KW  - Requirement engineering
KW  - Machine learning
KW  - Ambiguity
KW  - Disambiguation
AB  - Natural language processing is a known technology behind the development of some widely known AI assistants such as: SIRI, Natasha, and Watson. However, NLP is a diverse technology used for numerous purposes. NLP based tools are widely used for disambiguation in requirement engineering which will be the primary focus of this paper. A requirement document is a medium for the user to deliver one's expectations from the software. Hence, an ambiguous requirement document may eventually lead to misconceptions in a software. Various tools are available for disambiguation in RE based on different techniques. In this paper, we analyzed different disambiguation tools in order to compare and evaluate them. In our survey, we noticed that even though some disambiguation tools reflect promising results and can supposedly be relied upon, they fail to completely eliminate the ambiguities. In order to avoid ambiguities, the requirement document has to be written using formal language, which is not preferred by users due to its lack of lucidity and readability. Nevertheless, some of the tools we mentioned in this paper are still under development and in future might become capable of eliminating ambiguities. In this paper, we attempt to analyze some existing research work and present an elaborative review of various disambiguation tools.
ER  - 

TY  - JOUR
T1  - Validating, verifying and testing timed data-flow reactive systems in Coq from controlled natural-language requirements
AU  - Carvalho, Gustavo
AU  - Meira, Igor
JO  - Science of Computer Programming
VL  - 201
SP  - 102537
PY  - 2021
DA  - 2021/01/01/
SN  - 0167-6423
DO  - https://doi.org/10.1016/j.scico.2020.102537
UR  - https://www.sciencedirect.com/science/article/pii/S0167642320301453
KW  - Timed data-flow reactive system
KW  - Interactive theorem proving
KW  - Property-based testing
KW  - Controlled natural language
AB  - Data-flow reactive systems (DFRSs) form a class of embedded systems whose inputs and outputs are always available as signals. Input signals can be seen as data provided by sensors, whereas the output data are provided to system actuators. In previous works, verifying well-formedness properties of DFRS models was accomplished in a programmatic way, with no formal guarantees, and test cases were generated by translating these models into other notations. Here, we use Coq as a single framework to specify, validate and verify DFRS models. Moreover, the specification of DFRSs in Coq is automatically derived from controlled natural-language requirements, and well-formedness properties are formally verified with no user intervention. System validation is supported by bounded exploration of models; general and domain-specific system property verification is supported by the development of proof scripts, and test generation is achieved with the aid of the QuickChick tool. Considering examples from the literature, but also from the aerospace (Embraer) and the automotive (Mercedes) industries, our automatic testing strategy was evaluated in terms of performance and the ability to detect defects generated by mutation. Within seconds, test cases were generated automatically from the requirements, achieving an average mutation score of about 75%.
ER  - 

TY  - JOUR
T1  - On Keyword-Based Ad-Hoc Querying of Hospital Data Stored in Semistar Data Ontologies
AU  - Rencis, Edgars
JO  - Procedia Computer Science
VL  - 138
SP  - 27
EP  - 32
PY  - 2018
DA  - 2018/01/01/
T2  - CENTERIS 2018 - International Conference on ENTERprise Information Systems / ProjMAN 2018 - International Conference on Project MANagement / HCist 2018 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2018.10.005
UR  - https://www.sciencedirect.com/science/article/pii/S1877050918316363
KW  - Keyword-Based Querying
KW  - Query Languages
KW  - Semistar Ontologies
KW  - Natural Language Querying
KW  - Query Translation
KW  - Hospital Management
AB  - This paper sketches a possible solution to the problem of the currently growing necessity in various domains for domain experts to be able to query the database of the organization in a convenient manner. The paper focuses on the domain of hospital management where the normal practice is to involve a programmer as an intermediary between the managers and the database. This is an error-prone and cumbersome solution. The decision-making process of domain experts would hugely benefit if they could retrieve the information from the database themselves. There have been attempts to develop natural language-based query languages for this exact purpose, but the ultimate goal of the simplicity of querying has not yet been reached. The approach presented in this paper involves letting users define their queries in a very weakly-controlled natural language and then to choose among the offered query translations into a more strongly-controlled natural language which already has an efficient implementation for semistar ontologies. Experiments show that the implemented query language is very readable for non-programmers, because it lacks technical details (thanks to the nature of semistar ontologies), and because it is very intuitive. This phenomenon creates a conviction that the proposed approach is highly viable.
ER  - 

TY  - JOUR
T1  - Research on Vehicle Service Simulation Dispatching Telephone System Based on Natural Language Processing
AU  - Huang, Da Ji
AU  - Lin, Hai Xiang
JO  - Procedia Computer Science
VL  - 166
SP  - 344
EP  - 349
PY  - 2020
DA  - 2020/01/01/
T2  - Proceedings of the 3rd International Conference on Mechatronics and Intelligent Robotics (ICMIR-2019)
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2020.02.087
UR  - https://www.sciencedirect.com/science/article/pii/S187705092030209X
KW  - Vehicle simulation
KW  - NLP
KW  - Voice interaction
KW  - System recognition
AB  - In view of the current situation that the training process of the existing vehicle simulation training system is not complete and quite different under abnormal circumstances, a vehicle simulation dispatching telephone system based on natural language processing (NLP) is developed. The system uses ASR and TTS, combined with embedded design, to simulate the actual on-site traffic attendant’s voice interaction with each post under abnormal conditions, and secondly recognize the received speech recognition text through NLP which is converted to railway standard terminology for system evaluation. Through the actual case application, it is verified that the system can better identify railway terminology, and ensure that the single person completes the whole process of training and assessment. The system realizes system identification and human-computer interaction, which is important for the efficient and reliable training of the vehicle duty attendant and the expansion of the training system function.
ER  - 

TY  - JOUR
T1  - Implementing the map task in applied linguistics research: What, how, and why
AU  - Berríos, Juan
AU  - Swain, Angela
AU  - Fricke, Melinda
JO  - Research Methods in Applied Linguistics
VL  - 2
IS  - 3
SP  - 100081
PY  - 2023
DA  - 2023/12/01/
SN  - 2772-7661
DO  - https://doi.org/10.1016/j.rmal.2023.100081
UR  - https://www.sciencedirect.com/science/article/pii/S2772766123000411
KW  - Map task
KW  - Written communication
KW  - Informal language
KW  - Corpus design
KW  - Computer-mediated communication
AB  - The “map task” is an interactive, goal-driven, real-time conversational task used to elicit semi-controlled natural language production data. We present recommendations for creating a bespoke map task that can be tailored to individual research projects and administered online using a chat interface. As proof of concept, we present a case study exemplifying our own implementation, designed to elicit informal written communication in either English or Spanish. Eight experimental maps were created, manipulating linguistic factors including lexical frequency, cognate status, and semantic ambiguity. Participants (N = 40) completed the task in pairs and took turns (i) providing directions based on a pre-traced route, or (ii) following directions to draw the route on an empty map. Computational measures of image similarity (e.g., structural similarity index) between pre-traced and participant-traced routes showed that participants completed the task successfully; we describe use of this method for measuring task success quantitatively. We also provide a comparative analysis of the language elicited in English and Spanish. The most frequently used words were roughly equivalent in both languages, encompassing primarily commands and items on the maps. Similarly, abbreviations, swear words, and slang present in both datasets indicated that the task successfully elicited informal communication. Interestingly, Spanish turns were longer and displayed a wider range of morphologically complex forms. English, conversely, displayed strategies mostly absent in Spanish, such as the use of cardinal directions as a communicative strategy. We consider the online map task as a promising method for examining a variety of phenomena in applied linguistics research.
ER  - 

TY  - JOUR
T1  - Enhancing students’ English language learning via M-learning: Integrating technology acceptance model and S-O-R model
AU  - Yao-Ping Peng, Michael
AU  - Xu, Yunying
AU  - Xu, Cheng
JO  - Heliyon
VL  - 9
IS  - 2
SP  - e13302
PY  - 2023
DA  - 2023/02/01/
SN  - 2405-8440
DO  - https://doi.org/10.1016/j.heliyon.2023.e13302
UR  - https://www.sciencedirect.com/science/article/pii/S2405844023005091
KW  - Curiosity
KW  - Perceived convenience
KW  - Perceived ease of use
KW  - Perceived usefulness
KW  - Self-efficacy
KW  - SOR model
KW  - Technology acceptance model
AB  - With the impact of COVID-19, many university students may not be able to learn English in the physical classroom in a traditional way. Students' English learning effectiveness and outcome were threatened when English learning was forced to turn online. Thus, a variety of technological media and platforms to improve their learning outcomes are in need. Mobile learning (M-learning) that involves interacting with other devices through mobile devices and wireless networks can also be a solution to improve students' online English learning effectiveness. In order to explore the learning behaviors and attitudes of university students when learning English with M-learning, this study integrated technology acceptance model and Stimulus Organism Response model including the concepts of perceived convenience, curiosity and self-efficacy in addition to the original technology acceptance model to verify university students’ usage cognition and attitude toward English M-learning. This study disseminated surveys to 10 targeted universities/colleges and collected 1432 valid surveys. This study implemented Smart-PLS 4.0 to examine structural model and verify the hypotheses. Results indicated that perceived convenience have positive impact on perceived ease of use, perceived usefulness and attitude toward using; there is a significant and positive relationship among perceived ease of use, perceived usefulness, attitude toward using and intention to using; curiosity and self-efficacy have positive impact on intention to using. Based on the findings, this study further provides abundant theoretical insights and practical significance on language learning.
ER  - 

TY  - JOUR
T1  - A benchmark for end-user structured data exploration and search user interfaces
AU  - García, Roberto
AU  - Gil, Rosa
AU  - Bakke, Eirik
AU  - Karger, David R.
JO  - Journal of Web Semantics
VL  - 65
SP  - 100610
PY  - 2020
DA  - 2020/12/01/
SN  - 1570-8268
DO  - https://doi.org/10.1016/j.websem.2020.100610
UR  - https://www.sciencedirect.com/science/article/pii/S1570826820300469
KW  - Benchmark
KW  - User experience
KW  - Usability
KW  - Semantic data
KW  - Exploration
KW  - Relational data
AB  - During the years, it has been possible to assess significant improvements in the computational efficiency of Semantic Web search and exploration systems. However, it has been much harder to assess how well different semantic systems’ user interfaces help their users. One of the key factors facilitating the advancement of research in a particular field is the ability to compare the performance of different approaches. Though there are many such benchmarks in Semantic Web fields that have experienced significant improvements, this is not the case for Semantic Web user interfaces for data exploration. We propose and demonstrate the use of a benchmark for evaluating such user interfaces, which includes a set of typical user tasks and a well-defined procedure for assigning a measure of performance on those tasks to a semantic system. We have applied the benchmark to four such systems. Moreover, all the required resources to apply the benchmark are openly available online. We intend to initiate a community conversation that will lead to a generally accepted framework for comparing systems and for measuring, and thus encouraging, progress towards better semantic search and exploration tools.
ER  - 

TY  - JOUR
T1  - Towards augmented reality manuals for industry 4.0: A methodology
AU  - Gattullo, Michele
AU  - Scurati, Giulia Wally
AU  - Fiorentino, Michele
AU  - Uva, Antonio Emmanuele
AU  - Ferrise, Francesco
AU  - Bordegoni, Monica
JO  - Robotics and Computer-Integrated Manufacturing
VL  - 56
SP  - 276
EP  - 286
PY  - 2019
DA  - 2019/04/01/
SN  - 0736-5845
DO  - https://doi.org/10.1016/j.rcim.2018.10.001
UR  - https://www.sciencedirect.com/science/article/pii/S0736584518301236
KW  - Augmented reality
KW  - Industry 4.0
KW  - Maintenance support
KW  - Technical documentation
AB  - Augmented Reality (AR), is one of the most promising technology for technical manuals in the context of Industry 4.0. However, the implementation of AR documentation in industry is still challenging because specific standards and guidelines are missing. In this work, we propose a novel methodology for the conversion of existing “traditional” documentation, and for the authoring of new manuals in AR in compliance to Industry 4.0 principles. The methodology is based on the optimization of text usage with the ASD Simplified Technical English, the conversion of text instructions into 2D graphic symbols, and the structuring of the content through the combination of Darwin Information Typing Architecture (DITA) and Information Mapping (IM). We tested the proposed approach with a case study of a maintenance manual of hydraulic breakers. We validated it with a user test collecting subjective feedbacks of 22 users. The results of this experiment confirm that the manual obtained using our methodology is clearer than other templates.
ER  - 

TY  - JOUR
T1  - Towards user-centered and legally relevant smart-contract development: A systematic literature review
AU  - Dixit, Abhishek
AU  - Deval, Vipin
AU  - Dwivedi, Vimal
AU  - Norta, Alex
AU  - Draheim, Dirk
JO  - Journal of Industrial Information Integration
VL  - 26
SP  - 100314
PY  - 2022
DA  - 2022/03/01/
SN  - 2452-414X
DO  - https://doi.org/10.1016/j.jii.2021.100314
UR  - https://www.sciencedirect.com/science/article/pii/S2452414X21001072
KW  - Blockchain
KW  - Smart contract
KW  - Ricardian contract
KW  - Business collaboration
KW  - Legal relevance
AB  - Smart contracts (SC) run on blockchain technology (BCT) to implement agreements between several parties. As BCT grows, organizations aim to automate their processes and engage in business collaborations using SCs. The translation of contract semantics into SC language semantics is difficult due to ambiguous contractual interpretation by the several parties and the developers. Also, an SC language itself misses the language constructs needed for semantically expressing collaboration terms. This leads to SC coding errors that result in contractual conflicts over transactions during the performance of SCs and thus, novel SC solutions incur high development and maintenance costs. Various model-based and no/low code development approaches address this issue by enabling higher abstractions in SC development. Still, the question remains unanswered how contractual parties, i.e., end-users with non-IT skills, manage to develop legally relevant SCs with ease. This study aims to (1) identify and categorize the state of the art of SC automation models, in terms of their technical features, and their legal significance, and to (2) identify new research opportunities. The review has been conducted as a systematic literature review (SLR) that follows the guidelines proposed by Kitchenham for performing SLRs in software-engineering. As a result of the implementation of the review protocol, 1367 papers are collected, and 33 of them are selected for extraction and analysis. The contributions of this article are threefold: (1) 10 different SC automation models/frameworks are identified and classified according to their technical and implementation features; (2) 11 different legal contract parameters are identified and categorized into 4 legal criteria classes; (3) a comparative analysis of SC-automation models in the context of their legal significance is conducted that identifies the degrees to which the SC-automation models are considered legally relevant. As a conclusion, we produce a comprehensive and replicable overview of the state of the art of SC automation models and a systematic measure of their legal significance to benefit practitioners in the field.
ER  - 

TY  - JOUR
T1  - Automated modelling assistance by integrating heterogeneous information sources
AU  - Ángel, Mora Segura
AU  - de Lara, Juan
AU  - Neubauer, Patrick
AU  - Wimmer, Manuel
JO  - Computer Languages, Systems & Structures
VL  - 53
SP  - 90
EP  - 120
PY  - 2018
DA  - 2018/09/01/
SN  - 1477-8424
DO  - https://doi.org/10.1016/j.cl.2018.02.002
UR  - https://www.sciencedirect.com/science/article/pii/S1477842417301690
KW  - Modelling
KW  - (Meta-)modelling
KW  - Modelling assistance
KW  - Domain-specific languages
KW  - Language engineering
AB  - Model-Driven Engineering (MDE) uses models as its main assets in the software development process. The structure of a model is described through a meta-model. Even though modelling and meta-modelling are recurrent activities in MDE and a vast amount of MDE tools exist nowadays, they are tasks typically performed in an unassisted way. Usually, these tools cannot extract useful knowledge available in heterogeneous information sources like XML, RDF, CSV or other models and meta-models. We propose an approach to provide modelling and meta-modelling assistance. The approach gathers heterogeneous information sources in various technological spaces, and represents them uniformly in a common data model. This enables their uniform querying, by means of an extensible mechanism, which can make use of services, e.g., for synonym search and word sense analysis. The query results can then be easily incorporated into the (meta-)model being built. The approach has been realized in the Extremo tool, developed as an Eclipse plugin. Extremo has been validated in the context of two domains – production systems and process modelling – taking into account a large and complex industrial standard for classification and product description. Further validation results indicate that the integration of Extremo in various modelling environments can be achieved with low effort, and that the tool is able to handle information from most existing technological spaces.
ER  - 

TY  - JOUR
T1  - GeoReservoir: An ontology for deep-marine depositional system geometry description
AU  - Cicconeto, Fernando
AU  - Vieira, Lucas Valadares
AU  - Abel, Mara
AU  - Alvarenga, Renata dos Santos
AU  - Carbonera, Joel Luis
AU  - Garcia, Luan Fonseca
JO  - Computers & Geosciences
VL  - 159
SP  - 105005
PY  - 2022
DA  - 2022/02/01/
SN  - 0098-3004
DO  - https://doi.org/10.1016/j.cageo.2021.105005
UR  - https://www.sciencedirect.com/science/article/pii/S0098300421002880
KW  - Ontology
KW  - Deep-marine depositional system
KW  - Turbidite
KW  - Artificial intelligence
AB  - An ontology is a logical theory that accounts for a domain vocabulary’s intended meaning, allowing us to develop a computational artifact that explicitly and formally represents the community’s conceptualization related to a vocabulary. This paper presents the GeoReservoir ontology — the result of the ontological analysis of the terminology adopted by geologists in sedimentological studies about deep-marine depositional systems, which is one of the most relevant types of oil and gas reservoirs around the world. Despite the variety of studies describing the patterns of productive reservoirs, this domain demanded new approaches in conceptual modeling methodologies because the available terminology presents issues such as ambiguity and contamination of different interpretations in the terms’ definitions. Previous work has dealt with these issues in geology, resulting in the GeoCore, an ontology that explicitly defines the most generic entities in geology. However, the domain in focus still demanded a specialized ontology containing the particular terms found in reports about deep-marine deposits. GeoReservoir is an extension of GeoCore, which, in its turn, extends the Basic Formal Ontology (BFO), a foundational ontology for scientific domains. GeoReservoir takes advantage of both BFO and GeoCore’s foundations, allowing us to focus our effort on defining the domain-specific entities. A team of professional reservoir geologists supported the knowledge acquisition process. We developed the ontology in iterative steps from an initial prototype to a complete artifact containing the taxonomy of entities and the relations between the entities, being increasingly refined by the experts. We further present a case study demonstrating how one could describe a depositional system in GeoReservoir terms. Moreover, we validated the ontology against defined competency questions (CQs). This work’s final result is offering a sound, consistent and unambiguous terminology to support the integration of data and knowledge about deep-marine depositional system geometry and lithology.
ER  - 

TY  - JOUR
T1  - Automatic creation of acceptance tests by extracting conditionals from requirements: NLP approach and case study
AU  - Fischbach, Jannik
AU  - Frattini, Julian
AU  - Vogelsang, Andreas
AU  - Mendez, Daniel
AU  - Unterkalmsteiner, Michael
AU  - Wehrle, Andreas
AU  - Henao, Pablo Restrepo
AU  - Yousefi, Parisa
AU  - Juricic, Tedi
AU  - Radduenz, Jeannette
AU  - Wiecher, Carsten
JO  - Journal of Systems and Software
VL  - 197
SP  - 111549
PY  - 2023
DA  - 2023/03/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2022.111549
UR  - https://www.sciencedirect.com/science/article/pii/S0164121222002254
KW  - Acceptance testing
KW  - Automatic test case creation
KW  - Requirements engineering
KW  - Natural language processing
KW  - Causality extraction
AB  - Acceptance testing is crucial to determine whether a system fulfills end-user requirements. However, the creation of acceptance tests is a laborious task entailing two major challenges: (1) practitioners need to determine the right set of test cases that fully covers a requirement, and (2) they need to create test cases manually due to insufficient tool support. Existing approaches for automatically deriving test cases require semi-formal or even formal notations of requirements, though unrestricted natural language is prevalent in practice. In this paper, we present our tool-supported approach CiRA (Conditionals in Requirements Artifacts) capable of creating the minimal set of required test cases from conditional statements in informal requirements. We demonstrate the feasibility of CiRA in a case study with three industry partners. In our study, out of 578 manually created test cases, 71.8 % can be generated automatically. Additionally, CiRA discovered 80 relevant test cases that were missed in manual test case design. CiRA is publicly available at www.cira.bth.se/demo/.
ER  - 

TY  - JOUR
T1  - Image-based thickener mud layer height prediction with attention mechanism-based CNN
AU  - Fang, Chenyu
AU  - He, Dakuo
AU  - Li, Kang
AU  - Liu, Yan
AU  - Wang, Fuli
JO  - ISA Transactions
VL  - 128
SP  - 677
EP  - 689
PY  - 2022
DA  - 2022/09/01/
SN  - 0019-0578
DO  - https://doi.org/10.1016/j.isatra.2021.11.004
UR  - https://www.sciencedirect.com/science/article/pii/S0019057821005620
KW  - Convolutional neural network
KW  - Attention mechanism
KW  - Thickener mud layer height
KW  - Image processing
AB  - Mud layer height of thickener is the key quality index of thickening process which is difficult to achieve real-time detection with existing methods in reality. While the need of developing a soft sensor model which can be used for real-time detection of mud layer height, we proposed an end-to-end mud layer height prediction method with attention mechanism-based convolutional neural network (CNN). The dynamic features are firstly extracted from the image samples based on CNN, and then two types of attention mechanism are embedded sequentially to contribute to more precise prediction results. Compared with the traditional spatial attention mechanism, the regional spatial attention mechanism we proposed selectively divides the spatial feature map into regions, while regions containing important features are assigned larger weights. Adding the channel and regional spatial attention mechanism in CNN not only effectively improve both the precision and calculation speed, but also affect the dimension of the output feature map, so as to avoid the loss of channel or spatial attention information of the feature map. To verify the validity of the proposed method, different attention mechanisms are embedded in the CNN, and the corresponding experiments are carried out on the dataset of the thickener mud layer. The experimental results demonstrate the feasibility and effectiveness of the mud layer height prediction method.
ER  - 

TY  - JOUR
T1  - AuMixDw: Towards an automated hybrid approach for building XML data warehouses
AU  - Ouaret, Zoubir
AU  - Boukraa, Doulkifli
AU  - Boussaid, Omar
AU  - Chalal, Rachid
JO  - Data & Knowledge Engineering
VL  - 120
SP  - 60
EP  - 82
PY  - 2019
DA  - 2019/03/01/
SN  - 0169-023X
DO  - https://doi.org/10.1016/j.datak.2019.01.004
UR  - https://www.sciencedirect.com/science/article/pii/S0169023X18302477
KW  - XML
KW  - SBVR
KW  - BIR
KW  - Data warehouse
KW  - User requirement
KW  - Dimensional modelling
AB  - In this paper, we present a mixed approach for building XML data warehouses from both XML data sources and user requirements. Our proposed approach aims at obtaining a unique multidimensional schema of theXML data warehouse. The approach follows three steps. During the first step, an intermediate SBVR model extended with template rules is used to accommodate a data warehousing system and to facilitate the automatic identification of facts and dimensions from the user requirements. After modelling XML data sources in UML, the second step corresponds to identifying candidate DW schemata from such data sources. The third step compares these candidate schemata with the reference model obtained from the user requirements. In this step, we propose to adapt similarity metric-extended Boolean models (BIR) and to use them in order to measure, rank and select the most appropriate data warehouse schema. Such a schema should best describe the data sources and exhaustively cover all the needed user requirements. To demonstrate our approach, we present a case study of the bibliographic database dblp.
ER  - 

TY  - JOUR
T1  - Test case generation, selection and coverage from natural language
AU  - Nogueira, Sidney
AU  - Araujo, Hugo
AU  - Araujo, Renata
AU  - Iyoda, Juliano
AU  - Sampaio, Augusto
JO  - Science of Computer Programming
VL  - 181
SP  - 84
EP  - 110
PY  - 2019
DA  - 2019/07/15/
SN  - 0167-6423
DO  - https://doi.org/10.1016/j.scico.2019.01.003
UR  - https://www.sciencedirect.com/science/article/pii/S0167642319300024
KW  - Test generation
KW  - Natural language
KW  - Use case models
AB  - In Model-based Testing (MBT), test cases are automatically generated from a formal model of the system. A disadvantage of MBT is that developers must deal with formal notations. This limitation is addressed in this paper, where use cases are used to model the system. In previous work, we have proposed an automatic strategy for generating test cases from use cases written in a Controlled Natural Language (CNL), which is an English textual notation with a well-defined grammar. Due to its precise syntax, it can be processed and translated into a formal representation for the purpose of automatic test case generation. This paper extends our previous work by proposing a state-based CNL for describing use case control flows enriched with state and data operations. We translate state-based use case descriptions into CSP processes from which test cases can be automatically generated. In addition, we show how a similar notation can be used to specify test selection via the definition of state-based test purposes, which are also translated into CSP processes. Test generation and selection are mechanised by refinement checking using the CSP tool FDR. Despite the fact that we work at a purely process algebraic level to define a test generation strategy, we are able to address model coverage criteria. Particularly, by using FDR, it is possible to have access to the underlying LTS models; we then implemented algorithms for covering events or transitions, possibly combined with selection using test purposes. We also discuss several ways of improving the efficiency of the test generation strategy. As far as we are aware, this integration between an algebraic approach to test case generation with an operational approach for coverage criteria is an original and promising insight. All steps of the strategy are integrated into a tool that provides a GUI for authoring use cases and test purposes described in the proposed CNL, so the formal CSP notation is completely hidden from the test designer. We illustrate our tool and techniques with a running example and a more elaborate case study taken from an industrial setting.
ER  - 

TY  - JOUR
T1  - Loki – the semantic wiki for collaborative knowledge engineering
AU  - Kutt, Krzysztof
AU  - Nalepa, Grzegorz J.
JO  - Expert Systems with Applications
VL  - 224
SP  - 119968
PY  - 2023
DA  - 2023/08/15/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2023.119968
UR  - https://www.sciencedirect.com/science/article/pii/S0957417423004700
KW  - Knowledge engineering
KW  - Semantic wiki
KW  - Software engineering
KW  - Unit tests
KW  - Prolog
AB  - We present Loki, a semantic wiki designed to support the collaborative knowledge engineering process with the use of software engineering methods. Designed as a set of DokuWiki plug-ins, it provides a variety of knowledge representation methods, including semantic annotations, Prolog clauses, and business processes and rules oriented to specific tasks. Knowledge stored in Loki can be retrieved via SPARQL queries, in-line Semantic MediaWiki-like queries, or Prolog goals. Loki includes a number of useful features for a group of experts and knowledge engineers developing the wiki, such as knowledge visualization, ontology storage, or code hint and completion mechanism. Reasoning unit tests are also introduced to validate knowledge quality. The paper is complemented by the formulation of the collaborative knowledge engineering process and the description of experiments performed during Loki development to evaluate its functionality. Loki is available as free software at https://loki.re.
ER  - 

TY  - JOUR
T1  - Token-level disentanglement for unsupervised text style transfer
AU  - Hu, Yahao
AU  - Tao, Wei
AU  - Xie, Yifei
AU  - Sun, Yi
AU  - Pan, Zhisong
JO  - Neurocomputing
VL  - 560
SP  - 126823
PY  - 2023
DA  - 2023/12/01/
SN  - 0925-2312
DO  - https://doi.org/10.1016/j.neucom.2023.126823
UR  - https://www.sciencedirect.com/science/article/pii/S0925231223009463
KW  - Natural language generation
KW  - Text style transfer
KW  - Unsupervised learning
KW  - Disentanglement
KW  - Adversarial training
AB  - Text style transfer is the task of altering the style of a source text to a desired style while preserving the style-independent content. A common approach involves disentangling a given sentence into a style-agnostic content representation within the latent space and then generating the text in the desired style, guided by a separate style embedding. However, previous methods have a limitation in that they assume the input sentence is encoded by a fix-sized latent vector at the sentence level, making it challenging to capture rich semantic information at the token level, especially when dealing with longer texts. Consequently, this leads to suboptimal preservation of non-stylistic semantic content. In this paper, we address this challenge, and propose TED, a fine-grained model for disentangling content and style representation at the token level to enhance content preservation. Specifically, TED use tf–idfs to estimate the pivot tokens for different styles and incorporates multi-task and adversarial objectives to disentangle the content and style information of each token within the latent space. Experimental results on two popular text style transfer datasets show that our proposed model significantly outperforms state-of-the-art baselines, particularly in terms of content preservation. Moreover, the quantitative and qualitative experiments demonstrate the effectiveness of our model in achieving token-level disentanglement within the latent space.
ER  - 
