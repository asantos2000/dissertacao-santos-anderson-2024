@inproceedings{10.1145/3368640.3368641,
author = {Danenas, Paulius and Skersys, Tomas and Butleris, Rimantas},
title = {Enhancing the extraction of SBVR business vocabularies and business rules from UML use case diagrams with natural language processing},
year = {2019},
isbn = {9781450372923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368640.3368641},
doi = {10.1145/3368640.3368641},
abstract = {Being among the best-selling and most advanced features of model-driven development, model-to-model transformation could help improving one of the most time- and resource-consuming efforts in the process of model-driven information systems engineering, namely, discovery and specification of business vocabularies and business rules within the problem domain. Nonetheless, despite the relatively high levels of automation throughout the whole systems' model-driven development process, business modeling stage remains among the most under re-searched areas throughout the whole process. In this paper, we introduce a novel natural language processing (NLP) technique to one of our latest developments for the automatic extraction of SBVR business vocabularies and business rules from UML use case diagrams. This development remains arguably the most comprehensive development of this kind currently available in public. The experiment provided proof that the developed NLP enhancement delivered even better extraction results compared to the already satisfactory performance of the previous development. This work contributes to the research in the areas of model transformations and NLP within the model-driven development of information systems, and beyond.},
booktitle = {Proceedings of the 23rd Pan-Hellenic Conference on Informatics},
pages = {1–8},
numpages = {8},
keywords = {SBVR, UML, business rules, business vocabulary, model transformation, natural language processing, use case diagram},
location = {Nicosia, Cyprus},
series = {PCI '19}
}

@article{10.1145/3397619.3397623,
author = {Calvanese, Diego and Fodor, Paul and Montali, Marco},
title = {Report on the 3rd International Joint Conference on Rules and Reasoning (RuleML+RR 2019)},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
url = {https://doi.org/10.1145/3397619.3397623},
doi = {10.1145/3397619.3397623},
abstract = {The annual International Joint Conference on Rules and Reasoning (RuleML+RR) is an international conference on research, applications, languages and standards for rule technologies, rule-based programming and rule-based systems including production rules systems, logic programming rule engines, as well as business-rule engines and management systems; Semantic Web rule languages and rule standards, including RuleML (e.g., Datalog+ RuleML, Reaction RuleML and LegalRuleML), SWRL, RIF, Common Logic, PRR, decision rules and Decision Model and Notation (DMN), as well as Semantics of Business Vocabulary and Business Rules (SBVR); rule-based Event Processing Languages (EPLs) and technologies; and foundational research on inference rules, transformation rules, decision rules, production rules, and Event-Condition-Action (ECA) rules. In 2017, RuleML+RR joined the efforts of two wellestablished conference series: the International Web Rule Symposia (RuleML) and the Web Reasoning and Rule Systems (RR) conferences, and it is now the leading conference to build bridges between academia and industry in the field of Web rules and its applications, especially as part of the Semantic Technology stack. RuleML+RR is commonly listed together with and related to other major high-impact Artificial Intelligence conferences worldwide, starting with IJCAI in 2011 and 2016, ECAI in 2012, AAAI in 2013, ECAI in 2014, the AI Summit London in 2017, and GCAI in 2018 and 2019. The RuleML symposia and RR conferences have been held since 2002 and 2007, respectively. The RR conferences have been a forum for discussion and dissemination of new results on Web reasoning and rule systems, with an emphasis on rule-based approaches and languages. The RuleML symposia have been devoted to disseminating research, applications, languages, and standards for rule technologies, with attention to both theoretical and practical developments, to challenging new ideas, and to industrial applications. Building on the tradition of both, RuleML and RR, the joint conference series RuleML+RR aims at bridging academia and industry in the field of rules, and at fostering the cross-fertilization between the different communities focused on the research, development, and applications of rule-based systems. RuleML+RR aims at being the leading conference series for all subjects concerning theoretical advances, novel technologies, and innovative applications about knowledge representation and reasoning with rules.},
journal = {ACM SIGLOG News},
month = {apr},
pages = {16–18},
numpages = {3}
}

@inproceedings{10.1145/3167132.3167331,
author = {Camilleri, John J. and Haghshenas, Mohammad Reza and Schneider, Gerardo},
title = {A web-based tool for analysing normative documents in english},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167331},
doi = {10.1145/3167132.3167331},
abstract = {Our goal is to use formal methods to analyse normative documents written in English, such as privacy policies and regulations. This requires the combination of a number of different elements, including information extraction from natural language, formal languages for model representation, and an interface for property specification and verification. A number of components for performing these tasks have separately been developed: a natural language extraction tool, a suitable formalism for representing such documents, an interface for building models in this formalism, and methods for answering queries asked of a given model. In this work, each of these concerns is brought together in a web-based tool, providing a single interface for analysing normative texts in English. Through the use of a running example, we describe each component and demonstrate the workflow established by our tool.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1865–1872},
numpages = {8},
keywords = {contract analysis, controlled natural language, information extraction, model checking, normative texts},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3299771.3299786,
author = {Chittimalli, Pavan Kumar and Bhattacharyya, Abhidip},
title = {SBVR-based Business Rule Creation for Legacy Programs using Variable Provenance},
year = {2019},
isbn = {9781450362153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299771.3299786},
doi = {10.1145/3299771.3299786},
abstract = {Functionality of a software system that implements business operations can be captured using business processes and rules. To understand the 'as-is' processes and rules, the source-code is arguably the best source of knowledge. We present a novel method that combines program analysis and domain knowledge to create the descriptions for "IT rules", as a critical step towards extracting business rules automatically. We introduce and use the concept of 'variable provenance' to propagate the domain descriptions into the source code to create Semantics of Business Vocabularies and Rules (SBVR) rules. In our experiments on sample, near-real-life systems, we could successfully annotate very large percentage (&gt; 90%) of IT rules and enable to create SBVR rules. We present and describe the ProgAnnotator tool which is based on variable provenance and generates descriptions for IT rules in the source code and subsequently create SBVR rules automatically.},
booktitle = {Proceedings of the 12th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {16},
numpages = {11},
keywords = {Business Rule Extraction, Rule Annotation, SBVR, Static Program Analysis, Variable Provenance},
location = {<conf-loc>, <city>Pune</city>, <country>India</country>, </conf-loc>},
series = {ISEC '19}
}

@inproceedings{10.1145/3385032.3385046,
author = {Chittimalli, Pavan Kumar and Prakash, Chandan and Naik, Ravindra and Bhattacharyya, Abhidip},
title = {An Approach to Mine SBVR Vocabularies and Rules from Business Documents},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385046},
doi = {10.1145/3385032.3385046},
abstract = {Enterprises model the behavior of their business to prepare a communication standard for business analysts and to specify requirements to Information Technology (IT) people. The communication gap between IT group and business analysts, who lie on the opposite end of the business spectrum exists due to the different terminologies used in their respective fields regarding the same context. This gap has led to major software failures which prompted the OMG group has come up with a new standard - Semantic of Business Vocabulary and Business Rules (SBVR). Declarative models are provided by SBVR to represent Business Vocabulary and Business Rules which can be understood by everyone working throughout the business spectrum. Each business is governed by business rules which are constrained by the regulation policy set up by the policy guidelines of the organization and government regulations set up on the organization. Business rules are specified in documents like user guides, requirement documents, terms and conditions, do's and don'ts. Typically a Business Analyst interprets the document and manually extracts rules based on his understanding which leads to potential discrepancies, ambiguities and quality issues in the software system. To minimize such errors, in this paper we present an unsupervised approach to automatically extract SBVR vocabularies and rules from domain-specific business documents. We also present our initial results and comparative study with our earlier approach.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {12},
numpages = {11},
keywords = {Business Rules Extraction, Natural Language Processing, Rule Components, Rule Document, SBVR, Text Mining},
location = {<conf-loc>, <city>Jabalpur</city>, <country>India</country>, </conf-loc>},
series = {ISEC '20}
}

@inproceedings{10.1145/3452383.3452396,
author = {Prakash, Chandan and Chittimalli, Pavan Kumar and Naik, Ravindra},
title = {Open Information Extraction Using Dependency Parser for Business Rule Mining in SBVR Format},
year = {2021},
isbn = {9781450390460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452383.3452396},
doi = {10.1145/3452383.3452396},
abstract = {Business Rules exists at the core of any Business Organization. For efficient execution of the business system, all the business rules must be in machine-interpretable format. There is an absence of such a system that can convert the business rule sentences into corresponding structured format automatically. We present BRMiner, a system which automatically converts business rules represented as Natural Language sentences to the corresponding SBVR format which is a structured representation that can be further converted to the machine-interpretable format. BRMiner is based on the idea of Open Information Extraction (OIE). We have shown that existing OIE systems are not suitable for SBVR rule formation that leads to the development of a new OIE system BRMiner, with more accurate prediction and additional capabilities. BRMiner uses the state of the art dependency parser to convert an unstructured business rule to the corresponding structured format. We have used internal as well as publically available datasets for our system evaluation and the results are encouraging which we have shown in the paper.},
booktitle = {Proceedings of the 14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {13},
numpages = {11},
keywords = {Business Rules, Controlled Natural Language, Dependency Parser, Open Information Extraction, SBVR},
location = {<conf-loc>, <city>Bhubaneswar, Odisha</city>, <country>India</country>, </conf-loc>},
series = {ISEC '21}
}

@inproceedings{10.1109/ASE.2019.00134,
author = {Chittimalli, Pavan Kumar and Anand, Kritika and Pradhan, Shrishti and Mitra, Sayandeep and Prakash, Chandan and Shere, Rohit and Naik, Ravindra},
title = {BuRRiTo: a framework to extract, specify, verify and analyze business rules},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00134},
doi = {10.1109/ASE.2019.00134},
abstract = {An enterprise system operates business by providing various services that are guided by set of certain business rules (BR) and constraints. These BR are usually written using plain Natural Language in operating procedures, terms and conditions, and other documents or in source code of legacy enterprise systems. For implementing the BR in a software system, expressing them as UML use-case specifications, or preparing for Merger &amp; Acquisition (M&amp;A) activity, analysts manually interpret the documents or try to identify constraints from the source code, leading to potential discrepancies and ambiguities. These issues in the software system can be resolved only after testing, which is a very tedious and expensive activity. To minimize such errors and efforts, we propose BuRRiTo framework consisting of automatic extraction of BR by mining documents and source code, ability to clean them of various anomalies like inconsistency, redundancies, conflicts, etc. and able to analyze the functional gaps present and performing semantic querying and searching.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1190–1193},
numpages = {4},
keywords = {SBVR, business rules extraction, graphs, match and gap, natural language processing, rule components, rule document, search and query, source code, text mining},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1007/s00165-020-00512-5,
author = {Williams, David M. and Darwish, Salaheddin and Schneider, Steve and Michael, David R.},
title = {Legislation-driven development of a Gift Aid system using Event-B},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {2–3},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-020-00512-5},
doi = {10.1007/s00165-020-00512-5},
abstract = {This work presents our approach to formally model the Swiftaid system design, a digital platform that enables donors to automatically add Gift Aid to donations made via card payments. Following principles of Behaviour-Driven Development, we use Gherkin to capture requirements specified in legislation, specifically the UK Charity (Gift Aid Declarations) Regulations 2016. The Gherkin scenarios provide a basis for subsequent formal modelling and analysis using Event-B, Rodin and ProB. Interactive model simulations assist communication between domain experts, software architects and other stakeholders during requirements capture and system design, enabling the emergent system behaviour to be validated. Our approach was employed within the development of the real Swiftaid product, launched by Streeva in February 2019. Our analysis helped conclude that there was not a strong enough business case for one of the features, whichwas shown to provide nominal user convenience at the expense of increased complexity. This work provides a case study in allying formal and agile software development to enable rapid development of robust software.},
journal = {Form. Asp. Comput.},
month = {jul},
pages = {251–273},
numpages = {23},
keywords = {Behaviour-driven development, Formal modelling, Gherkin, Event-B, Gift Aid, Swiftaid}
}

@inproceedings{10.1145/3172871.3172884,
author = {Bhattacharyya, Abhidip and Chittimalli, Pavan Kumar and Naik, Ravindra},
title = {Relation Identification in Business Rules for Domain-specific Documents},
year = {2018},
isbn = {9781450363983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172871.3172884},
doi = {10.1145/3172871.3172884},
abstract = {This paper focuses on an approach to mine business rules from documents and facilitates a methodology to represent them in a formal notation. Businesses are operated abiding by some rules and complying with respect to regulation and guidelines. The business rules are often written using English in operating procedures, terms and conditions, and various other supporting documents. The manual analysis of these rules for activities like impact analysis, maintenance, business transformation leads to potential discrepancies, ambiguities, and quality issues. In this paper, we discuss our approach of mining relations among the rule intents (atomic facts) defined for business rules. We also present our preliminary studies on a couple of openly available documents.},
booktitle = {Proceedings of the 11th Innovations in Software Engineering Conference},
articleno = {14},
numpages = {5},
keywords = {Business Rule Extraction, Document Mining, Maximum Entropy, Natural Language Processing},
location = {Hyderabad, India},
series = {ISEC '18}
}

@inproceedings{10.1145/3460620.3460768,
author = {M. Maatuk, Abdelsalam and A. Abdelnabi, Esra},
title = {Generating UML Use Case and Activity Diagrams Using NLP Techniques and Heuristics Rules},
year = {2021},
isbn = {9781450388382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460620.3460768},
doi = {10.1145/3460620.3460768},
abstract = {The process of generating Unified Modeling Language (UML) Diagrams from Natural Language (NL) requirements is considered a complex and challenging task. Software requirements specification is often written in NL format, which causes potential problems. Requirement's analysts analyze and process natural language requirements manually to extract the UML elements. The manual analysis takes a lot of time and effort, which justifies the need for automated support. This paper proposes an approach to facilitate the NL requirements analysis process and UML diagrams extraction from NL textual requirements using natural language processing (NLP) techniques and heuristics rules. This approach focuses on generating use-case and activity diagrams. The approach has been applied to a case study and evaluated through an experimental. The evaluation of the approach will be conducted through a comparative study. The experimental results prove that the proposed approach is considerably improved as compared to the other approaches.},
booktitle = {International Conference on Data Science, E-Learning and Information Systems 2021},
pages = {271–277},
numpages = {7},
keywords = {NLP, Requirement Analysis, Software Requirement Specification, UML diagrams},
location = {Ma'an, Jordan},
series = {DATA'21}
}

@inproceedings{10.1145/3220228.3220255,
author = {Apaza, Ren\'{a}n Dar\'{\i}o Gonzales and Barrios, Jhon Edilberto Monroy and Becerra, Diego Alonso Iquira and Quispe, Jos\'{e} Alfredo Herrera},
title = {ERS-TOOL: hybrid model for software requirements elicitation in Spanish language},
year = {2018},
isbn = {9781450364454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220228.3220255},
doi = {10.1145/3220228.3220255},
abstract = {The nature of the software requirements is subjective and varied. For this reason the level of complexity increases according to the volume, especially when the requirements are made in a natural language. Therefore obtain quality software requirements that are understandable and unambiguous in the Spanish language becomes a necessity. First, a controlled syntax was proposed to express software requirements taking into account the static and dynamic behavior among the different actors of the system, where the expressions are elaborated based on the Backus-Naur form (BNF). Then a set of writing rules were adapted to the Spanish language, creating four additional rules. Finally, the results of the case study had high accuracy in understandability; also the ambiguity of requirements elicitation was reduced. In addition to improving the development of software engineering activities, since there are no tools available for the elicitation of software requirements with language Spanish.},
booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
pages = {27–30},
numpages = {4},
keywords = {ambiguity of requirements, controlled syntax, requirements engineering},
location = {Prague, Czech Republic},
series = {ICGDA '18}
}

@inproceedings{10.1145/3229345.3229373,
author = {de Almeida Bordignon, Ana Cl\'{a}udia and Thom, Lucin\'{e}ia Heloisa and Silva, Thanner Soares and Dani, Vinicius Stein and Fantinato, Marcelo and Ferreira, Renato Cesar Borges},
title = {Natural Language Processing in Business Process Identification and Modeling: A Systematic Literature Review},
year = {2018},
isbn = {9781450365598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229345.3229373},
doi = {10.1145/3229345.3229373},
abstract = {Business Process Management (BPM) has been receiving increasing attention in recent years. Many organizations have been adapting their business to a process-centered view since they started noticing its potential to reduce costs, improve productivity and achieve higher levels of quality. However, implementing BPM in organizations requires time, making the automation of process identification and discovery highly desirable. To achieve this expectation, the application of Natural Language Processing (NLP) techniques and tools has emerged to generate process models from unstructured text. In this paper, we provide the results of a systematic literature review conducted in preparation and processing of natural language text aiming the extraction of business processes and process quality assurance. The study presents techniques applied to the BPM life-cycle phases of process identification, process discovery and process analysis as well as tools to support process discovery. This review covered papers from 2009 up to 2016 and identifies 518 articles of which 33 were selected as relevant to our work. The results of the present study may be valuable to support research in extraction of business process models from natural language text.},
booktitle = {Proceedings of the XIV Brazilian Symposium on Information Systems},
articleno = {25},
numpages = {8},
keywords = {Business Process Management, Natural Language Processing, Process Analysis, Process Discovery, Systematic Literature Review},
location = {Caxias do Sul, Brazil},
series = {SBSI '18}
}

@article{10.1145/3632857,
author = {Popescu, Andrei},
title = {Nominal Recursors as Epi-Recursors},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {POPL},
url = {https://doi.org/10.1145/3632857},
doi = {10.1145/3632857},
abstract = {We study nominal recursors from the literature on syntax with bindings and compare them with respect to expressiveness. The term "nominal" refers to the fact that these recursors operate on a syntax representation where the names of bound variables appear explicitly, as in nominal logic. We argue that nominal recursors can be viewed as epi-recursors, a concept that captures abstractly the distinction between the constructors on which one actually recurses, and other operators and properties that further underpin recursion. We develop an abstract framework for comparing epi-recursors and instantiate it to the existing nominal recursors, and also to several recursors obtained from them by cross-pollination. The resulted expressiveness hierarchies depend on how strictly we perform this comparison, and bring insight into the relative merits of different axiomatizations of syntax. We also apply our methodology to produce an expressiveness hierarchy of nominal corecursors, which are principles for defining functions targeting infinitary non-well-founded terms (which underlie lambda-calculus semantics concepts such as B\"{o}hm trees). Our results are validated with the Isabelle/HOL theorem prover.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {15},
numpages = {32},
keywords = {epi-(co)recuror, formal reasoning, nominal logic, nominal recursion and corecursion, syntax with bindings, theorem proving}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00024,
author = {Rajbhoj, Asha and Nistala, Padmalata and Kulkarni, Vinay and Soni, Shivani and Pathan, Ajim},
title = {DocToModel: Automated Authoring of Models from Diverse Requirements Specification Documents},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00024},
doi = {10.1109/ICSE-SEIP58684.2023.00024},
abstract = {Early stages of Software Development Life Cycle (SDLC) namely requirement elicitation and requirements analysis have remained document-centric in the industry for market-driven, complex, large-scale business applications and products. The documentation typically runs into hundreds of Natural Language (NL) text documents which requirements engineers need to sift looking for the relevant information and also maintain these documents in-sync over time - a time-consuming and error-prone activity. Much of this difficulty can be overcome if the information is available in a structured form that is amenable to automated processing. Purposive models offer a way out. However, for easy adoption by industry practitioners, these models must be populated from NL text documents in a largely automated manner. This task is characterized by high variability with several documents containing different information conforming to different structures and styles. As a result, purposive information extractors need to be developed for each project/ product. Moreover, being an open-ended space there is no upper bound on the information extractors that need to be developed. To overcome this difficulty, we propose a document structure agnostic and meta-model agnostic tool, DocToModel, for the automated authoring of models from NL text documents. It provides a pattern mapping language to specify a mapping of structured and unstructured document information to meta-model elements, and a pattern interpreter to automate model authoring. The configurable and extensible architecture of DocToModel makes it generic and amenable to easy repurposing for other NL documents. This paper, describes the approach and illustrates its utility and efficacy on multiple real-world case studies.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {199–210},
numpages = {12},
keywords = {meta-model, automated model authoring, model extraction, document parser, NLP, meta-model pattern, pattern interpreter},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3368691.3368709,
author = {Alsaadi, Mohmood and Lisitsa, Alexei and Qasaimeh, Malik},
title = {Minimizing the ambiguities in medical devices regulations based on software requirement engineering techniques},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3368709},
doi = {10.1145/3368691.3368709},
abstract = {Trusted medical software devices, must comply with one of the healthcare regulations such as Food and Drug Administration (FDA), EU Medical Device Regulation (MDR), and Health Insurance Portability and Accountability Act (HIPAA). Since these regulations are enacted by law legislators and written in a legal text, these regulations are typically written with ambiguities. However, people have a different interpretation for the legal text for example, software developers faced challenges in identifying and understanding the regulatory requirements that are related to the software development process. Moreover, ambiguous in regulatory requirements play a big role in software compliance with regulatory particularly, when the requirements are legal text.In this paper, we intend to minimize the ambiguities in EU MDR requirements based on requirement engineering techniques in order to make MDR requirements clear and precise for software developers. In our previous work, we extracted the requirements of MDR that would affect the software development life cycle (SDLC) directly or indirectly. Herein, we will identify the ambiguity types in the extracted requirements of MDR. Moreover, this paper will present a method to minimize the ambiguities in MDR requirements based on requirement engineering techniques (user story and use case diagram). Finally, this work will be evaluated by the critical-to-quality tree measurement technique.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {18},
numpages = {5},
keywords = {CTQ tree, MDR software requirements, ambiguity of MD requirements, medical devices regulations, requirement engineering techniques},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@inproceedings{10.1145/3550356.3561542,
author = {Cabot, Jordi and Delgado, David and Burgue\~{n}o, Lola},
title = {Combining OCL and natural language: a call for a community effort},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561542},
doi = {10.1145/3550356.3561542},
abstract = {The growing popularity and availability of pretrained natural language models opens the door to many interesting applications combining natural language (NL) with software artefacts. A couple of examples are the generation of code excerpts from NL instructions or the verbalization of programs in NL to facilitate their comprehension.Many of these language models have been trained with open source software datasets and therefore "understand" a variety of programming languages, but not OCL.We argue that OCL needs to jump into the machine learning bandwagon or it will risk losing its appeal as a constraint specification language. For that, the key first task is to create together an OCL corpus dataset amenable for natural language processing.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {908–912},
numpages = {5},
keywords = {OCL, community, corpus, dataset, natural language},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3229147.3229166,
author = {Giunchi, Daniele and James, Stuart and Steed, Anthony},
title = {3D sketching for interactive model retrieval in virtual reality},
year = {2018},
isbn = {9781450358927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229147.3229166},
doi = {10.1145/3229147.3229166},
abstract = {We describe a novel method for searching 3D model collections using free-form sketches within a virtual environment as queries. As opposed to traditional sketch retrieval, our queries are drawn directly onto an example model. Using immersive virtual reality the user can express their query through a sketch that demonstrates the desired structure, color and texture. Unlike previous sketch-based retrieval methods, users remain immersed within the environment without relying on textual queries or 2D projections which can disconnect the user from the environment. We perform a test using queries over several descriptors, evaluating the precision in order to select the most accurate one. We show how a convolutional neural network (CNN) can create multi-view representations of colored 3D sketches. Using such a descriptor representation, our system is able to rapidly retrieve models and in this way, we provide the user with an interactive method of navigating large object datasets. Through a user study we demonstrate that by using our VR 3D model retrieval system, users can perform search more quickly and intuitively than with a naive linear browsing method. Using our system users can rapidly populate a virtual environment with specific models from a very large database, and thus the technique has the potential to be broadly applicable in immersive editing systems.},
booktitle = {Proceedings of the Joint Symposium on Computational Aesthetics and Sketch-Based Interfaces and Modeling and Non-Photorealistic Animation and Rendering},
articleno = {1},
numpages = {12},
keywords = {CNN, HCI, sketch, virtual reality},
location = {Victoria, British Columbia, Canada},
series = {Expressive '18}
}

