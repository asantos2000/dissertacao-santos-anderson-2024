@inproceedings{10.1145/3167132.3167331,
author = {Camilleri, John J. and Haghshenas, Mohammad Reza and Schneider, Gerardo},
title = {A web-based tool for analysing normative documents in english},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167331},
doi = {10.1145/3167132.3167331},
abstract = {Our goal is to use formal methods to analyse normative documents written in English, such as privacy policies and regulations. This requires the combination of a number of different elements, including information extraction from natural language, formal languages for model representation, and an interface for property specification and verification. A number of components for performing these tasks have separately been developed: a natural language extraction tool, a suitable formalism for representing such documents, an interface for building models in this formalism, and methods for answering queries asked of a given model. In this work, each of these concerns is brought together in a web-based tool, providing a single interface for analysing normative texts in English. Through the use of a running example, we describe each component and demonstrate the workflow established by our tool.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1865–1872},
numpages = {8},
keywords = {normative texts, model checking, information extraction, controlled natural language, contract analysis},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3486949.3486963,
author = {Bugayenko, Yegor},
title = {Combining object-oriented paradigm and controlled natural language for requirements specification},
year = {2021},
isbn = {9781450391252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486949.3486963},
doi = {10.1145/3486949.3486963},
abstract = {Natural language is the dominant form of writing software requirements. Its essential ambiguity causes inconsistency of requirements, which leads to scope creep. On the other hand, formal requirements specification notations such as Z, Petri Nets, SysML, and others are difficult to understand by non-technical project stakeholders. They often become a barrier between developers and requirements providers. The article presents a controlled natural language that looks like English but is a strongly typed object-oriented language compiled to UML/XMI. Thus, it is easily understood, at the same time, by non-technical people, programmers, and computers. Moreover, it is formally verifiable and testable. It was designed, developed, and tested in three commercial software projects in order to validate the assumption that object-oriented design can be applied to requirements engineering at the level of specifications writing. The article outlines key features of the language and summarizes the experience obtained during its practical application.},
booktitle = {Proceedings of the 1st ACM SIGPLAN International Workshop on Beyond Code: No Code},
pages = {11–17},
numpages = {7},
keywords = {Requirements, Natural Language Processing},
location = {Chicago, IL, USA},
series = {BCNC 2021}
}

@inproceedings{10.1145/3401895.3402064,
author = {Henarejos-Blasco, Jos\'{e} and Garc\'{\i}a-D\'{\i}az, Jos\'{e} Antonio and Apolinario-Arzube, \'{O}scar and Valencia-Garc\'{\i}a, Rafael},
title = {CNL-RDF-query: a controlled natural language interface for querying ontologies and relational databases},
year = {2021},
isbn = {9781450377119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401895.3402064},
doi = {10.1145/3401895.3402064},
abstract = {The most commonly used search engines do not always show the information that the user needs, because they do not take into account factors such as the context or natural language ambiguity. Therefore, other types of search engines that considered these factors emerged in last few years, such as question-answering systems or semantic web-based search engines. In this work, we present CNL-RDF-Query, a controlled natural language interface for querying rdf-based ontologies and relational databases. The system guides users in the construction of queries with the knowledge of domain ontology. Our proposal has been tested in the domain of the IMDb movie repository.},
booktitle = {Proceedings of the 10th Euro-American Conference on Telematics and Information Systems},
articleno = {35},
numpages = {5},
keywords = {user interfaces, ontologies, natural language interface, SPARQL},
location = {Aveiro, Portugal},
series = {EATIS '20}
}

@inproceedings{10.1145/3452383.3452396,
author = {Prakash, Chandan and Chittimalli, Pavan Kumar and Naik, Ravindra},
title = {Open Information Extraction Using Dependency Parser for Business Rule Mining in SBVR Format},
year = {2021},
isbn = {9781450390460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452383.3452396},
doi = {10.1145/3452383.3452396},
abstract = {Business Rules exists at the core of any Business Organization. For efficient execution of the business system, all the business rules must be in machine-interpretable format. There is an absence of such a system that can convert the business rule sentences into corresponding structured format automatically. We present BRMiner, a system which automatically converts business rules represented as Natural Language sentences to the corresponding SBVR format which is a structured representation that can be further converted to the machine-interpretable format. BRMiner is based on the idea of Open Information Extraction (OIE). We have shown that existing OIE systems are not suitable for SBVR rule formation that leads to the development of a new OIE system BRMiner, with more accurate prediction and additional capabilities. BRMiner uses the state of the art dependency parser to convert an unstructured business rule to the corresponding structured format. We have used internal as well as publically available datasets for our system evaluation and the results are encouraging which we have shown in the paper.},
booktitle = {Proceedings of the 14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {13},
numpages = {11},
keywords = {Business Rules, Controlled Natural Language, Dependency Parser, Open Information Extraction, SBVR},
location = {<conf-loc>, <city>Bhubaneswar, Odisha</city>, <country>India</country>, </conf-loc>},
series = {ISEC '21}
}

@inproceedings{10.1145/3344948.3344981,
author = {Schr\"{o}der, Sandra and Buchgeher, Georg},
title = {Applicability of controlled natural languages for architecture analysis and documentation: an industrial case study},
year = {2019},
isbn = {9781450371421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3344948.3344981},
doi = {10.1145/3344948.3344981},
abstract = {Formal approaches are a prerequisite for automated quality control. However, such approaches are often difficult to learn and apply and thus have not found their way into mainstream software development. Controlled natural languages (CNLs) seek to overcome the drawbacks of formal approaches by hiding their formality behind a restricted set of natural language, which is intended to be easier to use. In this paper, we analyze the applicability of CNLs for automated architecture analysis and as a means for architecture documentation in an industrial case study. We evaluated the CNL with 12 experienced practitioners in a focus group. The results show that practitioners perceive CNLs as an appropriate and an understandable means for formalizing and documenting architectural rules. Even without verifying rules automatically, a project can benefit from such languages, since they allow to find a common sense about architecture concepts and relations and to use those concepts and relations consistently throughout the development and evolution phase.},
booktitle = {Proceedings of the 13th European Conference on Software Architecture - Volume 2},
pages = {190–196},
numpages = {7},
keywords = {empirical study, controlled natural language, architecture documentation, architecture analysis},
location = {Paris, France},
series = {ECSA '19}
}

@inproceedings{10.1145/3640115.3640224,
author = {Wu, Bing and Song, Yuanbin and Cao, Jinhao},
title = {Automatic Generation of GIM Data Audit Rules Based on Sentence Embedding Vectors},
year = {2024},
isbn = {9798400708299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640115.3640224},
doi = {10.1145/3640115.3640224},
abstract = {The digital design results of power substations establish the foundation for their running and maintaining. Currently, the digital design of substations is delivered in the format of Grid Information Model (GIM), an alternative Building Information Modeling (BIM) format for describing power grid infrastructure in China. Since the correctness, compliance, and consistency of GIM data are necessary conditions for information sharing and business decision support, the GIM data must be audited before sharing among the stakeholders. The traditional manual review of GIM data is too inefficient and costly to execute, and thus the power grid industry seeks automatic review approaches. However, one challenge for automated auditing of GIM data is the lack of auditing rules. In order to establish such a rule base for GIM data auditing, this study first categorizes the audit rules, and proposes an XML encoding method for the audit rules. Meanwhile, the methodology of converting the rules described in natural language into XML is also rules proposed using the SBERT model. The application of the developed tool is demonstrated and verified through case studies.},
booktitle = {Proceedings of the 6th International Conference on Information Technologies and Electrical Engineering},
pages = {668–673},
numpages = {6},
keywords = {Controlled natural language, Grid information model, Model audit, Natural language processing, Rule representation, Sentence BERT},
location = {<conf-loc>, <city>Changde, Hunan</city>, <country>China</country>, </conf-loc>},
series = {ICITEE '23}
}

@inproceedings{10.1145/3365438.3410953,
author = {Veizaga, Alvaro and Alferez, Mauricio and Torre, Damiano and Sabetzadeh, Mehrdad and Briand, Lionel and Pitskhelauri, Elene},
title = {Leveraging natural-language requirements for deriving better acceptance criteria from models},
year = {2020},
isbn = {9781450370196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365438.3410953},
doi = {10.1145/3365438.3410953},
abstract = {In many software and systems development projects, analysts specify requirements using a combination of modeling and natural language (NL). In such situations, systematic acceptance testing poses a challenge because defining the acceptance criteria (AC) to be met by the system under test has to account not only for the information in the (requirements) model but also that in the NL requirements. In other words, neither models nor NL requirements per se provide a complete picture of the information content relevant to AC. Our work in this paper is prompted by the observation that a reconciliation of the information content in NL requirements and models is necessary for obtaining precise AC. We perform such reconciliation by devising an approach that automatically extracts AC-related information from NL requirements and helps modelers enrich their model with the extracted information. An existing AC derivation technique is then applied to the model that has now been enriched by the information extracted from NL requirements.Using a real case study from the financial domain, we evaluate the usefulness of the AC-related model enrichments recommended by our approach. Our evaluation results are very promising: Over our case study system, a group of five domain experts found 89% of the recommended enrichments relevant to AC and yet absent from the original model (precision of 89%). Furthermore, the experts could not pinpoint any additional information in the NL requirements which was relevant to AC but which had not already been brought to their attention by our approach (recall of 100%).},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {218–228},
numpages = {11},
keywords = {requirements validation and verification, gherkin, controlled natural language, acceptance testing, acceptance criteria, UML},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1145/3385032.3385046,
author = {Chittimalli, Pavan Kumar and Prakash, Chandan and Naik, Ravindra and Bhattacharyya, Abhidip},
title = {An Approach to Mine SBVR Vocabularies and Rules from Business Documents},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385046},
doi = {10.1145/3385032.3385046},
abstract = {Enterprises model the behavior of their business to prepare a communication standard for business analysts and to specify requirements to Information Technology (IT) people. The communication gap between IT group and business analysts, who lie on the opposite end of the business spectrum exists due to the different terminologies used in their respective fields regarding the same context. This gap has led to major software failures which prompted the OMG group has come up with a new standard - Semantic of Business Vocabulary and Business Rules (SBVR). Declarative models are provided by SBVR to represent Business Vocabulary and Business Rules which can be understood by everyone working throughout the business spectrum. Each business is governed by business rules which are constrained by the regulation policy set up by the policy guidelines of the organization and government regulations set up on the organization. Business rules are specified in documents like user guides, requirement documents, terms and conditions, do's and don'ts. Typically a Business Analyst interprets the document and manually extracts rules based on his understanding which leads to potential discrepancies, ambiguities and quality issues in the software system. To minimize such errors, in this paper we present an unsupervised approach to automatically extract SBVR vocabularies and rules from domain-specific business documents. We also present our initial results and comparative study with our earlier approach.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {12},
numpages = {11},
keywords = {Business Rules Extraction, Natural Language Processing, Rule Components, Rule Document, SBVR, Text Mining},
location = {<conf-loc>, <city>Jabalpur</city>, <country>India</country>, </conf-loc>},
series = {ISEC '20}
}

@inproceedings{10.1145/3437963.3441652,
author = {Shoham, Yoav},
title = {Some Thoughts on Computational Natural Language in the 21st Century},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441652},
doi = {10.1145/3437963.3441652},
abstract = {Modern neural language models (LMs) have grabbed much attention in recent years, due in part to their massive sizes and the resources (time, money, data) required to derive them, and in part to their unprecedented performance on language understanding and generation tasks. It is clear that massive LMs are a required component of any future natural language system, are critical to improving existing applications such as search, and enable new applications previously beyond the reach of technology.Modern LMs also make incredibly silly mistakes, that no five-year-old would ever make. One take on this is that all limitations will fade away as model sizes, training data and training time increase, as they surely will. An alternative take is that this is wishful thinking, and that the models require thoughtful guidance in order for them to approach human-level linguistic performance. This talk discusses this latter perspective.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {1},
numpages = {1},
keywords = {writing assistants, masked language models, lexical semantics, controlled natural language generation},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@inproceedings{10.1145/3368640.3368641,
author = {Danenas, Paulius and Skersys, Tomas and Butleris, Rimantas},
title = {Enhancing the extraction of SBVR business vocabularies and business rules from UML use case diagrams with natural language processing},
year = {2019},
isbn = {9781450372923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368640.3368641},
doi = {10.1145/3368640.3368641},
abstract = {Being among the best-selling and most advanced features of model-driven development, model-to-model transformation could help improving one of the most time- and resource-consuming efforts in the process of model-driven information systems engineering, namely, discovery and specification of business vocabularies and business rules within the problem domain. Nonetheless, despite the relatively high levels of automation throughout the whole systems' model-driven development process, business modeling stage remains among the most under re-searched areas throughout the whole process. In this paper, we introduce a novel natural language processing (NLP) technique to one of our latest developments for the automatic extraction of SBVR business vocabularies and business rules from UML use case diagrams. This development remains arguably the most comprehensive development of this kind currently available in public. The experiment provided proof that the developed NLP enhancement delivered even better extraction results compared to the already satisfactory performance of the previous development. This work contributes to the research in the areas of model transformations and NLP within the model-driven development of information systems, and beyond.},
booktitle = {Proceedings of the 23rd Pan-Hellenic Conference on Informatics},
pages = {1–8},
numpages = {8},
keywords = {use case diagram, natural language processing, model transformation, business vocabulary, business rules, UML, SBVR},
location = {Nicosia, Cyprus},
series = {PCI '19}
}

@inproceedings{10.1145/3299771.3299786,
author = {Chittimalli, Pavan Kumar and Bhattacharyya, Abhidip},
title = {SBVR-based Business Rule Creation for Legacy Programs using Variable Provenance},
year = {2019},
isbn = {9781450362153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299771.3299786},
doi = {10.1145/3299771.3299786},
abstract = {Functionality of a software system that implements business operations can be captured using business processes and rules. To understand the 'as-is' processes and rules, the source-code is arguably the best source of knowledge. We present a novel method that combines program analysis and domain knowledge to create the descriptions for "IT rules", as a critical step towards extracting business rules automatically. We introduce and use the concept of 'variable provenance' to propagate the domain descriptions into the source code to create Semantics of Business Vocabularies and Rules (SBVR) rules. In our experiments on sample, near-real-life systems, we could successfully annotate very large percentage (&gt; 90%) of IT rules and enable to create SBVR rules. We present and describe the ProgAnnotator tool which is based on variable provenance and generates descriptions for IT rules in the source code and subsequently create SBVR rules automatically.},
booktitle = {Proceedings of the 12th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {16},
numpages = {11},
keywords = {Business Rule Extraction, Rule Annotation, SBVR, Static Program Analysis, Variable Provenance},
location = {<conf-loc>, <city>Pune</city>, <country>India</country>, </conf-loc>},
series = {ISEC '19}
}

@inproceedings{10.1145/3290688.3290699,
author = {Schwitter, Rolf},
title = {Specifying Weak Constraints in Processable English},
year = {2019},
isbn = {9781450366038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290688.3290699},
doi = {10.1145/3290688.3290699},
abstract = {Processable English (PENG) is a controlled natural language designed to specify and conceptualize knowledge in a human-readable and machine-processable way. PENG specifications can be translated unambiguously into executable answer set programs (ASP). In this paper we suggest an extension of the language PENGASP so that weak constraints can be expressed in controlled natural language and processed as part of an ASP program. In contrast to strong constraints that have always to be satisfied and are already part of the controlled natural language, we introduce weak constraints that can be weighted and prioritised and should be satisfied whenever possible. The addition of weak constraints to the controlled natural language PENGASP makes it possible to specify optimisation problems in a natural way.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {56},
numpages = {4},
keywords = {specifications, answer set programming, Controlled languages},
location = {Sydney, NSW, Australia},
series = {ACSW '19}
}

@inproceedings{10.1145/3357150.3357400,
author = {Thompson, Jeff and Gusev, Peter and Burke, Jeff},
title = {NDN-CNL: A Hierarchical Namespace API for Named Data Networking},
year = {2019},
isbn = {9781450369701},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357150.3357400},
doi = {10.1145/3357150.3357400},
booktitle = {Proceedings of the 6th ACM Conference on Information-Centric Networking},
pages = {30–36},
numpages = {7},
location = {Macao, China},
series = {ICN '19}
}

@article{10.1145/3397619.3397623,
author = {Calvanese, Diego and Fodor, Paul and Montali, Marco},
title = {Report on the 3rd International Joint Conference on Rules and Reasoning (RuleML+RR 2019)},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
url = {https://doi.org/10.1145/3397619.3397623},
doi = {10.1145/3397619.3397623},
abstract = {The annual International Joint Conference on Rules and Reasoning (RuleML+RR) is an international conference on research, applications, languages and standards for rule technologies, rule-based programming and rule-based systems including production rules systems, logic programming rule engines, as well as business-rule engines and management systems; Semantic Web rule languages and rule standards, including RuleML (e.g., Datalog+ RuleML, Reaction RuleML and LegalRuleML), SWRL, RIF, Common Logic, PRR, decision rules and Decision Model and Notation (DMN), as well as Semantics of Business Vocabulary and Business Rules (SBVR); rule-based Event Processing Languages (EPLs) and technologies; and foundational research on inference rules, transformation rules, decision rules, production rules, and Event-Condition-Action (ECA) rules. In 2017, RuleML+RR joined the efforts of two wellestablished conference series: the International Web Rule Symposia (RuleML) and the Web Reasoning and Rule Systems (RR) conferences, and it is now the leading conference to build bridges between academia and industry in the field of Web rules and its applications, especially as part of the Semantic Technology stack. RuleML+RR is commonly listed together with and related to other major high-impact Artificial Intelligence conferences worldwide, starting with IJCAI in 2011 and 2016, ECAI in 2012, AAAI in 2013, ECAI in 2014, the AI Summit London in 2017, and GCAI in 2018 and 2019. The RuleML symposia and RR conferences have been held since 2002 and 2007, respectively. The RR conferences have been a forum for discussion and dissemination of new results on Web reasoning and rule systems, with an emphasis on rule-based approaches and languages. The RuleML symposia have been devoted to disseminating research, applications, languages, and standards for rule technologies, with attention to both theoretical and practical developments, to challenging new ideas, and to industrial applications. Building on the tradition of both, RuleML and RR, the joint conference series RuleML+RR aims at bridging academia and industry in the field of rules, and at fostering the cross-fertilization between the different communities focused on the research, development, and applications of rule-based systems. RuleML+RR aims at being the leading conference series for all subjects concerning theoretical advances, novel technologies, and innovative applications about knowledge representation and reasoning with rules.},
journal = {ACM SIGLOG News},
month = {apr},
pages = {16–18},
numpages = {3}
}

@article{10.1145/3625563,
author = {Baxter, James and Carvalho, Gustavo and Cavalcanti, Ana and J\'{u}nior, Francisco Rodrigues},
title = {RoboWorld: Verification of Robotic Systems with Environment in the Loop},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0934-5043},
url = {https://doi.org/10.1145/3625563},
doi = {10.1145/3625563},
abstract = {A robot affects and is affected by its environment, so that typically its behaviour depends on properties of that environment. For verification, we need to formalise those properties. Modelling the environment is very challenging, if not impossible, but we can capture assumptions. Here, we present RoboWorld, a domain-specific controlled natural language with a process algebraic semantics that can be used to define (a)&nbsp;operational requirements, and (b)&nbsp;environment interactions of a robot. RoboWorld is part of the RoboStar framework for verification of robotic systems. In this article, we define RoboWorld’s syntax and hybrid semantics, and illustrate its use for capturing operational requirements, for automatic test generation, and for proof. We also present a tool that supports the writing of RoboWorld documents. Since RoboWorld is a controlled natural language, it complements the other RoboStar notations in being accessible to roboticists, while at the same time benefitting from a formal semantics to support rigorous verification&nbsp;(via testing and proof).},
journal = {Form. Asp. Comput.},
month = {nov},
articleno = {26},
numpages = {46},
keywords = {Model-driven engineering, domain-specific languages, controlled natural languages, hybrid systems, process algebra}
}

@inproceedings{10.1145/3241403.3241457,
author = {Schr\"{o}der, Sandra and Riebisch, Matthias},
title = {An ontology-based approach for documenting and validating architecture rules},
year = {2018},
isbn = {9781450364836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241403.3241457},
doi = {10.1145/3241403.3241457},
abstract = {Architecture conformance checking is an important activity of architecture enforcement where the architect ensures that all architecture concepts are implemented correctly in the source code. In order to support the architect, a lot of tools for conformance checking are available that allow to formalize the architecture in order to perform an automated verification. Typically, the formalization uses a rigid, tool-specific architecture concept language that may strongly deviate from the project-specific architecture concept language. In addition, a high level of formal expertise is required in order to comprehend the created formalization. We present an approach that uses a controlled natural language for the formalization of architecture concepts. This language allows to flexibly express architecture rules directly with project-specific concepts. Consequently, the resulting formalization is easy to understand and might also be used as an architecture documentation at the same time. Nevertheless, the documentation can be automatically verified, since the approach is based on powerful means of the semantic web, i.e., ontologies and description logics. For the evaluation of the approach, we use the real-world software system TEAMMATES and show that architecture rules and concepts can be flexibly designed and checked for conformance in order to detect crucial architecture violations.},
booktitle = {Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings},
articleno = {52},
numpages = {7},
keywords = {architecture conformance checking, architecture documentation, architecture erosion, description logics, ontologies},
location = {Madrid, Spain},
series = {ECSA '18}
}

@inproceedings{10.1145/3341105.3373939,
author = {Silva, Alberto Rodrigues da and Sequeira, Carolina Lisboa},
title = {Towards a library of usability requirements},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373939},
doi = {10.1145/3341105.3373939},
abstract = {1Usability is an essential quality of software products if not an important concern in any business competition. This research discusses an approach to better define and specify reusable usability requirements. This is difficult because requirements specifications tend to be poorly structured, and tend to be introduced late in the development process, which results in major cost and rework. This research starts by identifying concerns related the classification and specification of usability requirements commonly found in literature. Then, it extends the ITLingo RSL language, which is a controlled natural language that allows to specify requirements and tests in a rigorous and structured way, and uses the respective tools to define a library of reusable usability requirements. This research was conducted in a software house operating in the healthcare domain and was applied and evaluated in its family of software products. The most important contribution of this research is the initial proposal of a library of reusable usability requirements, rigorously specified in a language like RSL, which may promote both quality and productivity.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1371–1378},
numpages = {8},
keywords = {RSL, requirements reuse, requirements specification, usability},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1109/ASE.2019.00134,
author = {Chittimalli, Pavan Kumar and Anand, Kritika and Pradhan, Shrishti and Mitra, Sayandeep and Prakash, Chandan and Shere, Rohit and Naik, Ravindra},
title = {BuRRiTo: a framework to extract, specify, verify and analyze business rules},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00134},
doi = {10.1109/ASE.2019.00134},
abstract = {An enterprise system operates business by providing various services that are guided by set of certain business rules (BR) and constraints. These BR are usually written using plain Natural Language in operating procedures, terms and conditions, and other documents or in source code of legacy enterprise systems. For implementing the BR in a software system, expressing them as UML use-case specifications, or preparing for Merger &amp; Acquisition (M&amp;A) activity, analysts manually interpret the documents or try to identify constraints from the source code, leading to potential discrepancies and ambiguities. These issues in the software system can be resolved only after testing, which is a very tedious and expensive activity. To minimize such errors and efforts, we propose BuRRiTo framework consisting of automatic extraction of BR by mining documents and source code, ability to clean them of various anomalies like inconsistency, redundancies, conflicts, etc. and able to analyze the functional gaps present and performing semantic querying and searching.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1190–1193},
numpages = {4},
keywords = {SBVR, business rules extraction, graphs, match and gap, natural language processing, rule components, rule document, search and query, source code, text mining},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3385032.3385051,
author = {Mitra, Sayandeep and Chittimalli, Pavan Kumar and Banerjee, Ansuman},
title = {Analyzing Business Systems comprised of Rules and Processes using Decision Diagrams},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385051},
doi = {10.1145/3385032.3385051},
abstract = {Modern Businesses are rapidly growing in complexity and functionalities. To ensure smooth functioning, businesses need to adhere to a set of guidelines and constraints which are efficiently represented by Business Rules(BRs). Due to the large number of inter-dependent BRs, anomalies such as inconsistencies, redundancies and circularities creep in to the rule base, which if not dealt with properly can cause the business to function improperly causing significant damage at multiple levels. Present state of the art methods identify such anomalies in BRs by converting the rules to knowledge representation (Ontology, SMT-LIBv2, etc.) and then running them on solvers. These approaches suffer from certain drawbacks, namely incomplete mappings and scalability of solvers. To overcome these shortcomings, in this paper we propose to represent the Business Rules(BRs) as Decision Diagrams (BDD, SDD, MDD, etc.) and use graph algorithms on top of their canonical representations to identify anomalies. Presently, business rules and processes are treated separately. We model rules as Decision Diagrams(DDs) to integrate with certain graphical representations of business processes (e.g., DCR Graphs, BPMN, etc.), enabling us to efficiently analyze a much more enriched set of business information. We show an initial set of mappings from business rules to Binary Decision Diagrams (BDD's), integrate with processes, identify various anomalies and outline our vision and prospective reach of this approach.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {17},
numpages = {5},
keywords = {BDD, Business Rules, Decision Diagrams, MDD, Ontology, Processes, SBVR, SDD, SMT, Test Case},
location = {<conf-loc>, <city>Jabalpur</city>, <country>India</country>, </conf-loc>},
series = {ISEC '20}
}

@inproceedings{10.1145/3172871.3172884,
author = {Bhattacharyya, Abhidip and Chittimalli, Pavan Kumar and Naik, Ravindra},
title = {Relation Identification in Business Rules for Domain-specific Documents},
year = {2018},
isbn = {9781450363983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172871.3172884},
doi = {10.1145/3172871.3172884},
abstract = {This paper focuses on an approach to mine business rules from documents and facilitates a methodology to represent them in a formal notation. Businesses are operated abiding by some rules and complying with respect to regulation and guidelines. The business rules are often written using English in operating procedures, terms and conditions, and various other supporting documents. The manual analysis of these rules for activities like impact analysis, maintenance, business transformation leads to potential discrepancies, ambiguities, and quality issues. In this paper, we discuss our approach of mining relations among the rule intents (atomic facts) defined for business rules. We also present our preliminary studies on a couple of openly available documents.},
booktitle = {Proceedings of the 11th Innovations in Software Engineering Conference},
articleno = {14},
numpages = {5},
keywords = {Business Rule Extraction, Document Mining, Maximum Entropy, Natural Language Processing},
location = {Hyderabad, India},
series = {ISEC '18}
}

@inproceedings{10.1145/3299771.3299782,
author = {Chaudhuri, Subhrojyoti Roy and Banerjee, Amar and Swaminathan, N. and Choppella, Venkatesh and Pal, Arpan and Balamurali, P.},
title = {A knowledge centric approach to conceptualizing robotic solutions},
year = {2019},
isbn = {9781450362153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299771.3299782},
doi = {10.1145/3299771.3299782},
abstract = {The focus of the ongoing Digital and Industry 4.0 revolution is on re-engineering business operations to take advantage of various technologies, including robotics. Conceptualizing, say, a robotics solution to automate aspects of warehouse operations involves multiple activities: understanding the problem space in sufficient detail to identify the right automation opportunities; working through the space of possible solution options and developing the solution design; and building a prototype solution with sufficient functional detail to enable customer experts to assess its suitability with respect to multiple concerns, and its impact on the business processes and environment. Only after that can a project be initiated to engineer and deploy the production solution, while making the necessary changes to the business system.With current practice, conceptualization and prototyping typically takes several months and considerable manual effort. In this paper, we present an environment for rapid prototyping of robotics solutions that facilitates a knowledge-centric approach based on capability composition. The environment enables systematic capture of functional domain knowledge, modular composition of solution space capabilities, and expression of the solution concept using a constrained natural language. Detailed functional simulators are generated automatically from the resulting design. This results in high customer confidence in the solution, substantial reductions in cycle time, and productivity gains due to modular reusability of solution knowledge and components.},
booktitle = {Proceedings of the 12th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {12},
numpages = {11},
keywords = {Component Capability, Domain Specific Engineering Environment, Goal Achievement Process, Model Driven Engineering},
location = {<conf-loc>, <city>Pune</city>, <country>India</country>, </conf-loc>},
series = {ISEC '19}
}

@inproceedings{10.1145/3325917.3325948,
author = {Rencis, Edgars},
title = {Natural Language-Based Knowledge Extraction in Healthcare Domain},
year = {2019},
isbn = {9781450366359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325917.3325948},
doi = {10.1145/3325917.3325948},
abstract = {There is a growing amount of data in the databases of hospitals. These data could be exploited to alleviate the decision-making process of hospital managers, physicians and researchers. However, these types of end-users often lack the expertise necessary for extracting those data from the database. Several approaches exist in the field of how to allow non-programmers writing queries in a convenient manner, but none of them has yet reached fully satisfactory results. This paper sketches a solution to this problem by introducing means for writing queries in a keywords-containing natural language thus alleviating the query writing process for the end-user. Introducing this approach in the knowledge management system of the organization would greatly benefit the domain experts by allowing them to carry out the decision-making process in a more rapid and less erroneous manner.},
booktitle = {Proceedings of the 2019 3rd International Conference on Information System and Data Mining},
pages = {138–142},
numpages = {5},
keywords = {query translation, query language, knowledge extraction, keywords-containing text, hospital management, Natural language processing},
location = {Houston, TX, USA},
series = {ICISDM '19}
}

@inproceedings{10.1145/3226116.3226133,
author = {Rencis, Edgars},
title = {Towards a natural language-based interface for querying hospital data},
year = {2018},
isbn = {9781450364270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3226116.3226133},
doi = {10.1145/3226116.3226133},
abstract = {There is a growing necessity in various domains for non-programmers to be able to retrieve information gathered about the operation of the organization and stored in its databases. This information could hugely benefit the decision making process of the managers of the institution, but it is not often exploited due to the complexity of extracting the information from the existing data. In this paper we sketch a way how that information could be managed by the domain experts themselves by the means of a natural language-based query language that works upon data stored in the ontology. Our experiments show that the proposed approach is indeed easy-to-use by our target end-users - managers and physicians of hospitals -, because lacking technical details the query language is very intuitive to use.},
booktitle = {Proceedings of the 1st International Conference on Big Data Technologies},
pages = {25–28},
numpages = {4},
keywords = {query translation, query language, natural language processing, hospital management},
location = {Hangzhou, China},
series = {ICBDT '18}
}

@inproceedings{10.1145/3594536.3595160,
author = {Meessen, P. N.},
title = {On Normative Arrows and Comparing Tax Automation Systems},
year = {2023},
isbn = {9798400701979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594536.3595160},
doi = {10.1145/3594536.3595160},
abstract = {Automation of legal norms can only exist because of compromises made in translating laws in natural languages to code (executables) in formal languages. Tax and benefits law is canonically considered one of the least compromising domains of law in terms of this nuance. In this paper, I compare two domain-specific languages used in the automation of tax and benefits law: Catala, which is proposed for use by the French Tax Authority, and Regelspraak, which the Dutch Tax Authority uses. The comparison is based on a top-down modeling approach, which does not try to go all the way down to the technical differences but aims to identify the points at which the two approaches diverge in the way the norm is shaped. It becomes evident that a lot of decisions that affect normativity are made inside the technical components of these systems. Safeguarding legal protection at the creation of automation of norms will likely require cross-domain efforts.},
booktitle = {Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law},
pages = {432–436},
numpages = {5},
keywords = {domain-specific languages, legal automation, norm engineering},
location = {<conf-loc>, <city>Braga</city>, <country>Portugal</country>, </conf-loc>},
series = {ICAIL '23}
}

@article{10.1145/3474829,
author = {Debruyne, Christophe and Munnelly, Gary and Kilgallon, Lynn and O’Sullivan, Declan and Crooks, Peter},
title = {Creating a Knowledge Graph for Ireland’s Lost History: Knowledge Engineering and Curation in the Beyond 2022 Project},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3474829},
doi = {10.1145/3474829},
abstract = {The Beyond 2022 project aims to create a virtual archive by digitally reconstructing and digitizing historical records lost in a catastrophic fire which consumed items in the Public Record Office of Ireland in 1922. The project is developing a knowledge graph (KG) to facilitate information retrieval and discovery over the reconstructed items. The project decided to adopt Semantic Web technologies to support its distributed KG and reasoning. In this article, we present our approach to KG generation and management. We elaborate on how we help historians contribute to the KG (via a suite of spreadsheets) and its ontology. We furthermore demonstrate how we use named graphs to store different versions of factoids and their provenance information and how these are serviced in two different endpoints. Modeling data in this manner allows us to acknowledge that history is, to some extent, subjective and different perspectives can exist in parallel. The construction of the KG is driven by competency questions elicited from subject matter experts within the consortium. We avail of CIDOC-CRM as our KG’s foundation, though we needed to extend this ontology with various qualifiers (types) and relations to support the competency questions. We illustrate how one can explore the KG to gain insights and answer questions. We conclude that CIDOC-CRM provides an adequate, albeit complex, foundation for the KG and that named graphs and Linked Data principles are a suitable mechanism to manage sets of factoids and their provenance.},
journal = {J. Comput. Cult. Herit.},
month = {apr},
articleno = {25},
numpages = {25},
keywords = {Knowledge graph creation, knowledge graph management, digital humanities}
}

@inproceedings{10.1145/3463274.3463807,
author = {Silva, Thiago Rocha and Fitzgerald, Brian},
title = {Empirical Findings on BDD Story Parsing to Support Consistency Assurance between Requirements and Artifacts},
year = {2021},
isbn = {9781450390538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463274.3463807},
doi = {10.1145/3463274.3463807},
abstract = {Behaviour-Driven Development (BDD) stories have gained considerable attention in recent years as an effective way to specify and test user requirements in agile software development projects. External testing frameworks also allow developers to automate the execution of BDD stories and check whether a fully functional software system behaves as expected. However, other software artifacts may quite often lose synchronization with the stories, and many inconsistencies can arise with respect to requirements representation. This paper reports on preliminary empirical findings regarding the performance of two existing approaches in the literature intended to support consistency assurance between BDD stories and software artifacts. The first approach involves the parsing of BDD stories in order to identify conceptual elements to automatically generate consistent class diagrams, while the second approach seeks to identify interaction elements to automatically assess the consistency of task models and GUI prototypes. We report on the precision of these approaches when applied to a study with BDD stories previously written by Product Owners (POs). Based on the results, we also identify a set of challenges and opportunities for BDD stories in the consistency assurance of such artifacts.},
booktitle = {Proceedings of the 25th International Conference on Evaluation and Assessment in Software Engineering},
pages = {266–271},
numpages = {6},
keywords = {User Stories, User Requirements, Software Artifacts., Consistency Assurance, Behaviour-Driven Development},
location = {Trondheim, Norway},
series = {EASE '21}
}

@article{10.1145/3596597,
author = {Hossain, Bayzid Ashik and Mukta, Md. Saddam Hossain and Islam, Md Adnanul and Zaman, Akib and Schwitter, Rolf},
title = {Natural Language–Based Conceptual Modelling Frameworks: State of the Art and Future Opportunities},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3596597},
doi = {10.1145/3596597},
abstract = {Identifying requirements for an information system is an important task and conceptual modelling is the first step in this process. Conceptual modelling plays a critical role in the information system design process and usually involves domain experts and knowledge engineers who brainstorm together to identify the required knowledge to build an information system. The conceptual modelling process starts with the collection of necessary information from the domain experts by the knowledge engineers. Afterwards, the knowledge engineers use traditional model driven engineering techniques to design the system based on the collected information. Natural language–based conceptual modelling frameworks or systems are used to help domain experts and knowledge engineers in eliciting requirements and building conceptual models from a natural language text. In this article, we discuss the state of the art of some recent conceptual modelling frameworks that are based on natural language. We take a closer look at how these frameworks are built, in particular at the underlying motivation, architecture, types of natural language used (e.g., restricted vs. unrestricted), types of the conceptual model generated, verification support of the requirements specifications as well as the conceptual models, and underlying knowledge representation formalism. We also discuss some future research opportunities that these frameworks offer.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {12},
numpages = {26},
keywords = {Natural language processing, information extraction, conceptual modelling, knowledge representation, semantic round-tripping}
}

@inproceedings{10.1145/3622758.3622890,
author = {Gordon, Colin S. and Matskevich, Sergey},
title = {Trustworthy Formal Natural Language Specifications},
year = {2023},
isbn = {9798400703881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622758.3622890},
doi = {10.1145/3622758.3622890},
abstract = {Interactive proof assistants are computer programs carefully constructed to check a human-designed proof of a mathematical claim with high confidence in the implementation. However, this only validates truth of a formal claim, which may have been mistranslated from a claim made in natural language. This is especially problematic when using proof assistants to formally verify the correctness of software with respect to a natural language specification. The translation from informal to formal remains a challenging, time-consuming process that is difficult to audit for correctness.  

This paper shows that it is possible to build support for specifications written in expressive subsets of natural language, within existing proof assistants, consistent with the principles used to establish trust and auditability in proof assistants themselves. We implement a means to provide specifications in a modularly extensible formal subset of English, and have them automatically translated into formal claims, entirely within the Lean proof assistant. Our approach is extensible (placing no permanent restrictions on grammatical structure), modular (allowing information about new words to be distributed alongside libraries), and produces proof certificates explaining how each word was interpreted and how the sentence's structure was used to compute the meaning.  

We apply our prototype to the translation of various English descriptions of formal specifications from a popular textbook into Lean formalizations; all can be translated correctly with a modest lexicon with only minor modifications related to lexicon size.},
booktitle = {Proceedings of the 2023 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {50–70},
numpages = {21},
keywords = {Proof Assistants, Formal Specification, Categorial Grammar},
location = {<conf-loc>, <city>Cascais</city>, <country>Portugal</country>, </conf-loc>},
series = {Onward! 2023}
}

@inproceedings{10.1145/3281278.3281282,
author = {Baudart, Guillaume and Hirzel, Martin and Mandel, Louis and Shinnar, Avraham and Sim\'{e}on, J\'{e}r\^{o}me},
title = {Reactive chatbot programming},
year = {2018},
isbn = {9781450360708},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281278.3281282},
doi = {10.1145/3281278.3281282},
abstract = {Chatbots are reactive applications with a conversational interface. They are usually implemented as compositions of client-side components and cloud-hosted services, including artificial-intelligence technology. Unfortunately, programming such reactive multi-tier applications with traditional programming languages is cumbersome. This paper introduces wcs-ocaml, a new multi-tier chatbot generator library designed for use with the reactive language ReactiveML. The paper explains our library with small didactic examples throughout, and closes with a larger case-study of a chatbot for authoring event-processing rules.},
booktitle = {Proceedings of the 5th ACM SIGPLAN International Workshop on Reactive and Event-Based Languages and Systems},
pages = {21–30},
numpages = {10},
keywords = {synchronous programming, reactive programming, Chatbot},
location = {Boston, MA, USA},
series = {REBLS 2018}
}

@inproceedings{10.1145/3551349.3563241,
author = {Pham, Khang and Nguyen, Vu and Nguyen, Tien},
title = {Application of Natural Language Processing Towards Autonomous Software Testing},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3563241},
doi = {10.1145/3551349.3563241},
abstract = {The process of creating test cases from requirements written in natural language (NL) requires intensive human efforts and can be tedious, repetitive, and error-prone. Thus, many studies have attempted to automate that process by utilizing Natural Language Processing (NLP) approaches. Furthermore, with the advent of massive language models and transfer learning techniques, people have introduced various advancements in NLP-assisted software testing with promising results. More notably, in recent years, not only have researchers been engrossed in solving the above task, but many companies have also embedded the feature to translate from human language to test cases their products. This paper presents an overview of NLP-assisted solutions being used in both the literature and the software testing industry.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {216},
numpages = {4},
location = {<conf-loc>, <city>Rochester</city>, <state>MI</state>, <country>USA</country>, </conf-loc>},
series = {ASE '22}
}

@article{10.1007/s00165-020-00512-5,
author = {Williams, David M. and Darwish, Salaheddin and Schneider, Steve and Michael, David R.},
title = {Legislation-driven development of a Gift Aid system using Event-B},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {2–3},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-020-00512-5},
doi = {10.1007/s00165-020-00512-5},
abstract = {This work presents our approach to formally model the Swiftaid system design, a digital platform that enables donors to automatically add Gift Aid to donations made via card payments. Following principles of Behaviour-Driven Development, we use Gherkin to capture requirements specified in legislation, specifically the UK Charity (Gift Aid Declarations) Regulations 2016. The Gherkin scenarios provide a basis for subsequent formal modelling and analysis using Event-B, Rodin and ProB. Interactive model simulations assist communication between domain experts, software architects and other stakeholders during requirements capture and system design, enabling the emergent system behaviour to be validated. Our approach was employed within the development of the real Swiftaid product, launched by Streeva in February 2019. Our analysis helped conclude that there was not a strong enough business case for one of the features, whichwas shown to provide nominal user convenience at the expense of increased complexity. This work provides a case study in allying formal and agile software development to enable rapid development of robust software.},
journal = {Form. Asp. Comput.},
month = {jul},
pages = {251–273},
numpages = {23},
keywords = {Behaviour-driven development, Formal modelling, Gherkin, Event-B, Gift Aid, Swiftaid}
}

@inproceedings{10.1145/3404663.3406876,
author = {Rencis, Edgars},
title = {Knowledge Extraction from Healthcare Data Using User-Adaptable Keywords-Based Query Language},
year = {2020},
isbn = {9781450377652},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404663.3406876},
doi = {10.1145/3404663.3406876},
abstract = {Nowadays, the volume of the information gathered by any organization increases more and more rapidly. It is essential to be able to use this information efficiently for it to benefit the operation of the organization. There is no point of gathering the information if it is not converted into knowledge. The knowledge extraction process becomes the backbone of any successful organization. Moreover, the extraction of the knowledge must be quick and efficient, so that the newly-obtained knowledge can be put in use at once. The problem addressed in this paper is how to allow the domain expert to extract the knowledge from their information systems themselves without involving the third party in the form of an IT specialist. This goal is of utmost importance for the domain experts, e.g. hospital managers and physicians, because they need to make decisions based on the available knowledge and to do it rapidly and efficiently. We propose a system in this paper that allows formulating queries in the natural language and that also adapts to the specifics of the user. Our experiments show that such kind of querying could provide an improvement in the decision-making process of healthcare professionals.},
booktitle = {Proceedings of the 2020 the 4th International Conference on Information System and Data Mining},
pages = {128–131},
numpages = {4},
keywords = {query translation, query language, knowledge extraction, keywords-containing text, hospital management, Natural language processing},
location = {Hawaii, HI, USA},
series = {ICISDM '20}
}

@inproceedings{10.1109/ECASE.2019.00010,
author = {Keim, Jan and Schneider, Yves and Koziolek, Anne},
title = {Towards consistency analysis between formal and informal software architecture artefacts},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ECASE.2019.00010},
doi = {10.1109/ECASE.2019.00010},
abstract = {Documenting the architecture of a software system is important, especially to capture reasoning and design decisions. A lot of tacit knowledge can easily get lost when the documentation is incomplete, resulting in threats for the software system's success and increased costs. However, software architecture documentation is often missing or outdated. One explanation for this phenomenon is the tedious and costly process of creating documentation in comparison to (perceived) low benefits. In this paper, we first present our long-term vision, where we plan to persist information from any sources, e.g. from whiteboard discussions, to avoid losing crucial information about a system. A core problem in this vision is the possible inconsistency of information from different sources. A major challenge of ensuring consistency is the consistency between formal artefacts, i.e. models, and informal documentation. We plan to address consistency analyses between models and textual natural language artefacts using natural language understanding and plan to include knowledge bases to improve these analyses. After extracting information out of the natural language documents, we plan to create traceability links and check whether statements within the textual documentation are consistent with the software architecture models. In this paper, we also outline our requirements for evaluating our approach with the help of a community-wide infrastructure and how our approach can be used to maintain community-wide case studies.},
booktitle = {Proceedings of the 2nd International Workshop on Establishing a Community-Wide Infrastructure for Architecture-Based Software Engineering},
pages = {6–12},
numpages = {7},
keywords = {software engineering, software architecture documentation, software architecture, natural language understanding, natural language processing, consistency},
location = {Montreal, Quebec, Canada},
series = {ECASE '19}
}

@inproceedings{10.1145/3371382.3378326,
author = {Ghosh, Ayan and Veres, Sandor M. and Paredes-Soto, Daniel and Clarke, James E. and Rossiter, John Anthony},
title = {Intuitive Programming with Remotely Instructed Robots inside Future Gloveboxes},
year = {2020},
isbn = {9781450370578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371382.3378326},
doi = {10.1145/3371382.3378326},
abstract = {Our research aims at facilitating the design of 'Remotely Instructed Robots' for future glove-boxes in the nuclear industry. The two main features of such systems are: (1) They can automatically model the working environment and relay that information to the operator in virtual reality (VR). (2) They can receive instructions from the operator that are executed by the robot. However, the deficiency of these kind of systems is that they heavily rely on knowledge of expert programmers when the robot's capabilities or hardware are to be reconfigured, altered or upgraded. This late breaking report proposes to introduce a third important advancement on remotely instructed robots: (3) Intuitive programming modifications by operators who are non-programmers but have basic knowledge of hardware, and most importantly, have experience of the weaknesses in particular handling tasks.},
booktitle = {Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {209–211},
numpages = {3},
keywords = {nuclear glove-boxes, language based interactions, intuitive interfaces, human-robot interaction},
location = {Cambridge, United Kingdom},
series = {HRI '20}
}

@inproceedings{10.1145/3478431.3499392,
author = {Becker, Brett A. and Gallagher, Daniel and Denny, Paul and Prather, James and Gostomski, Colleen and Norris, Kelli and Powell, Garrett},
title = {From the Horse's Mouth: The Words We Use to Teach Diverse Student Groups Across Three Continents},
year = {2022},
isbn = {9781450390705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478431.3499392},
doi = {10.1145/3478431.3499392},
abstract = {Humans adjust how they speak depending on context. Two key facets of this are utilizing different vocabulary and speaking rates depending on the audience. Exactly how we use language while teaching may depend on our students, their backgrounds and needs, and the subject matter. How we speak in the classroom likely affects student comprehension and may affect equity and accessibility.We analyzed audio transcripts of three introductory programming courses delivered by different instructors at different institutions on three continents as well as several sessions of a popular online introduction to CS course. All used the same programming language (C) and had varying percentages of non-native English-speaking students. We investigated the vocabulary used and the rate of speech of each.We found that many qualities of the instructional language used in these courses are remarkably similar. We did observe a striking difference in the rate of speech, a factor known to affect comprehension for non-native English speakers. These findings raise several questions about the speech we use in teaching. This is particularly relevant as the mode of delivery for many institutions is now entirely online, or involves recorded live lectures. These findings may also inform efforts to tailor delivery for non-native English speakers, students of different abilities, and pre-university students.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education - Volume 1},
pages = {71–77},
numpages = {7},
keywords = {terminology, talk, speech, recorded lectures, online education, non-native english-speakers, introductory programming, intelligibility, esl, english as a second language, cs1, basic english},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@inproceedings{10.1145/3459086.3459628,
author = {Cavalcanti, Ana},
title = {RoboStar modelling stack: tackling the reality gap},
year = {2021},
isbn = {9781450384445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459086.3459628},
doi = {10.1145/3459086.3459628},
abstract = {RoboStar technology for model-based Software Engineering for Robotics enables the construction of artefacts that capture and relate assumptions that can play a role in the reality. In this paper, we give a brief overview of the RoboStar approach.},
booktitle = {Proceedings of the 1st International Workshop on Verification of Autonomous &amp; Robotic Systems},
articleno = {2},
numpages = {3},
keywords = {software engineering, simulation, robotics, RoboSim, RoboChart},
location = {Nashville, Tennessee},
series = {VARS '21}
}

@article{10.1145/3640822,
author = {Liu, Yinling and Bruel, Jean-Michel},
title = {Modeling and Verification of Natural Language Requirements based on States and Modes},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0934-5043},
url = {https://doi.org/10.1145/3640822},
doi = {10.1145/3640822},
abstract = {The relationship between states (status of a system) and modes (capabilities of a system) used to describe system requirements is often poorly defined. The unclear relationship could make systems of interest out of control because of the out of boundaries of the systems caused by the newly added modes. Formally modeling and verifying requirements can clarify the relationship, making the system safer. To this end, an innovative approach to analyzing requirements is proposed. The MoSt language (a Domain Specific Language implemented on the Xtext framework) is firstly designed for requirements modeling and a model validator is realized to check requirements statically. A code generator is then provided to realize the automatic model transformation from the MoSt model to a NuSMV model, laying the foundation for the dynamic checks of requirements through symbolic model checking. Next, a NuSMV runner is designed to connect the NuSMV with the validator to automate the whole dynamic checks. The grammar, the model validator, the code generator, and the NuSMV runner are finally integrated into a publicly available Eclipse-based tool. Two case studies have been employed to illustrate the feasibility of our approach. For each case study, we injected 14 errors. The results show that the static and dynamic checks can successfully detect all the errors.},
note = {Just Accepted},
journal = {Form. Asp. Comput.},
month = {feb},
keywords = {states and modes, requirements modeling and verification, domain specific language, model checking}
}

@inproceedings{10.1145/3463677.3463721,
author = {Ahn, Michael and Huang, Chenyu and Huang, Pei-Chi and Zhong, Xin and Himmelreich, Johannes and Desouza, Kevin and Knepper, Richard},
title = {Cyber-physical innovations: Cyber-infrastructure for research, cyber-physical architecture for real-time applications, autonomous vehicle (AV) governance and AI artifacts for public value},
year = {2021},
isbn = {9781450384926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463677.3463721},
doi = {10.1145/3463677.3463721},
abstract = {This panel explores the development of innovative, integrative, and versatile strategies to facilitate more practical and effective use of intelligent cyber-physical technologies from a variety of perspectives, including engineering, regulation, management, and research. With the same goal of sustaining the development of emerging technologies to best benefit our communities, this panel shares their different approaches in terms of engineering solutions for real-time controlling in cyber-physical systems, regulatory strategies to overcome the conflict between efficiency and autonomy, artifacts for artificial intelligence project management, and meeting researcher needs through large-scale cyberinfrastructure. The selected cases discussed in this panel not only highlight the critical challenges in implementing cyber-physical technologies into real applications but also suggest promising strategies to overcome those issues from diverse facets.},
booktitle = {DG.O2021: The 22nd Annual International Conference on Digital Government Research},
pages = {590–592},
numpages = {3},
location = {Omaha, NE, USA},
series = {DG.O'21}
}

@inproceedings{10.1109/BotSE.2019.00010,
author = {Monperrus, Martin},
title = {Explainable software bot contributions: case study of automated bug fixes},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/BotSE.2019.00010},
doi = {10.1109/BotSE.2019.00010},
abstract = {In a software project, esp. in open-source, a contribution is a valuable piece of work made to the project: writing code, reporting bugs, translating, improving documentation, creating graphics, etc. We are now at the beginning of an exciting era where software bots will make contributions that are of similar nature than those by humans.Dry contributions, with no explanation, are often ignored or rejected, because the contribution is not understandable per se, because they are not put into a larger context, because they are not grounded on idioms shared by the core community of developers.We have been operating a program repair bot called Repairnator for 2 years and noticed the problem of "dry patches": a patch that does not say which bug it fixes, or that does not explain the effects of the patch on the system. We envision program repair systems that produce an "explainable bug fix": an integrated package of at least 1) a patch, 2) its explanation in natural or controlled language, and 3) a highlight of the behavioral difference with examples.In this paper, we generalize and suggest that software bot contributions must explainable, that they must be put into the context of the global software development conversation.},
booktitle = {Proceedings of the 1st International Workshop on Bots in Software Engineering},
pages = {12–15},
numpages = {4},
location = {Montreal, Quebec, Canada},
series = {BotSE '19}
}

@inproceedings{10.1109/ASE.2019.00145,
author = {Reich, Marina},
title = {Inference of properties from requirements and automation of their formal verification},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00145},
doi = {10.1109/ASE.2019.00145},
abstract = {Over the past decades, various techniques for the application of formal program analysis of software for embedded systems have been proposed. However, the application of formal methods for software verification is still limited in practise. It is acknowledged that the task of formally stating requirements by specifying the formal properties is a major hindrance. The verification step itself has its shortcoming in its scalability and its limitation to predefined proof tactics in case of automated theorem proving (ATP). These constraints are reduced today by the interaction of the user with the theorem prover (TP) during the execution of the proof. However, this is difficult for non-experts. The objectives of the presented PhD project are the automated inference of declarative property specifications from example data specified by the engineer for a function under development and their automated verification on abstract model level and on code level. We propose the meta-model for Scenario Modeling Language (SML) that allows to specify example data. For the automated property generation we are motivated by Inductive Logic Programming (ILP) techniques for first-order logic in pure mathematics. We propose modifications to its algorithm that allow to process the information that is incorporated in the meta-model of SML. However, this technique is expected to produce too many uninteresting properties. To turn this weakness into strength, our approach proposes to tailor the algorithm towards selection of the right properties that facilitate the automation of the proof. Automated property generation and less user interaction with the prover will leverage formal verification as it will relieve the engineer in the specification as well as in proofing tasks.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1222–1225},
numpages = {4},
keywords = {specification mining, formal verification, formal properties, embedded systems, declarative requirement specification},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3323503.3360639,
author = {Pinto, Thiago Delgado and Gon\c{c}alves, Willian Inacio and Costa, Pablo Veiga},
title = {User interface prototype generation from agile requirements specifications written in concordia},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3360639},
doi = {10.1145/3323503.3360639},
abstract = {User interface prototypes (UIP) are widely used to get feedback before building a software feature. They can prevent misunderstandings between the software development team and other stakeholders (e.g., users, investors) that lead to rework or a resulting software that does not meet their needs. UIP can also be a valuable resource in Agile software development, in which feedback is key. In this paper, we present an approach to generate UIP automatically from Agile requirements specifications written in Concordia and its corresponding prototype tool. The tool is able to generate UIP for web-based applications. We evaluated the approach and the tool with questionnaires, and the results revealed that: (i) the generated UIP are very similar to those drawn by respondents; (ii) the generated source code has good enough quality to be reused by developers; and (iii) they save design and development time.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {61–64},
numpages = {4},
keywords = {user story, user interface, generation, concordia, agile},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inproceedings{10.1109/MODELS-C.2019.00079,
author = {Aniculaesei, Adina and Vorwald, Andreas and Rausch, Andreas},
title = {Using the SCADE toolchain to generate requirements-based test cases for an adapative cruise control system},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00079},
doi = {10.1109/MODELS-C.2019.00079},
abstract = {In the last years, model-driven engineering has gained a lot of traction, especially in industrial domains, such as automotive or avionics. Various tools which support model-driven engineering, e.g. SCADE or MATLAB/Simulink, have developed over the years in fully fledged integrated development environments, with strong capabilities for the modeling of complex software systems. Model-driven engineering tools are mature enough so that the model created with them are amenable to formal analysis for the purpose of verification and validation. Acceptance testing is a validation method by which a system is tested extensively against legal and customer requirements, before it is allowed in series production. Due to the inherent complexity of automotive systems, large requirements catalogues have become usual in this domain. Checking that a complex automotive software system conforms to an extensive requirements catalogue is a task which cannot be managed manually anymore. In this paper, we design a workflow for test engineers to construct test cases from formalized requirements and examine the quality of tests via mutant testing within the SCADE toolchain. We construct an academic case study based on a prototypical adaptive cruise control system and evaluate our workflow on it. We report on results and lessons learned.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems},
pages = {503–513},
numpages = {11},
keywords = {SCADE toolchain, adaptive cruise control, automated test case generation, model checking, model-driven engineering, requirements-based testing},
location = {Munich, Germany},
series = {MODELS '19}
}

@inproceedings{10.1145/3437120.3437278,
author = {Kravari, Kalliopi and Antoniou, Christina and Bassiliades, Nick},
title = {Towards a Requirements Engineering Framework based on Semantics},
year = {2021},
isbn = {9781450388979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437120.3437278},
doi = {10.1145/3437120.3437278},
abstract = {Requirements engineering is one of the most important issues in systems development. Whether it is software or hardware systems or embedded systems, the need for well-defined requirements remains the same. The ultimate success or failure of developing a system stems largely from the initial definition and management of its requirements. However, despite the efforts that have been made, a coherent and easily understood process that leads from the requirements to correct implementations is still an open research issue, which seeks alternative promising approaches. To this end, in this paper, we propose a requirements engineering approach based on Semantics. It provides a novel mechanism that combines semantics, ontologies, and appropriate NLP techniques. The ultimate goal is to propose a framework that will include the minimum consistent set of formalities and languages to determine the requirements and perform the necessary verifications.},
booktitle = {Proceedings of the 24th Pan-Hellenic Conference on Informatics},
pages = {72–76},
numpages = {5},
keywords = {Boilerplates, Ontologies, Requirements Engineering, Semantics},
location = {Athens, Greece},
series = {PCI '20}
}

@inproceedings{10.1145/3324884.3416549,
author = {Frattini, Julian and Junker, Maximilian and Unterkalmsteiner, Michael and Mendez, Daniel},
title = {Automatic extraction of cause-effect-relations from requirements artifacts},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416549},
doi = {10.1145/3324884.3416549},
abstract = {Background: The detection and extraction of causality from natural language sentences have shown great potential in various fields of application. The field of requirements engineering is eligible for multiple reasons: (1) requirements artifacts are primarily written in natural language, (2) causal sentences convey essential context about the subject of requirements, and (3) extracted and formalized causality relations are usable for a (semi-)automatic translation into further artifacts, such as test cases.Objective: We aim at understanding the value of interactive causality extraction based on syntactic criteria for the context of requirements engineering.Method: We developed a prototype of a system for automatic causality extraction and evaluate it by applying it to a set of publicly available requirements artifacts, determining whether the automatic extraction reduces the manual effort of requirements formalization.Result: During the evaluation we analyzed 4457 natural language sentences from 18 requirements documents, 558 of which were causal (12.52%). The best evaluation of a requirements document provided an automatic extraction of 48.57% cause-effect graphs on average, which demonstrates the feasibility of the approach.Limitation: The feasibility of the approach has been proven in theory but lacks exploration of being scaled up for practical use. Evaluating the applicability of the automatic causality extraction for a requirements engineer is left for future research.Conclusion: A syntactic approach for causality extraction is viable for the context of requirements engineering and can aid a pipeline towards an automatic generation of further artifacts from requirements artifacts.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {561–572},
numpages = {12},
keywords = {causality extraction, natural language processing, pattern matching, requirements artifacts},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3437992.3439925,
author = {Becker, Heiko and Bos, Nathaniel and Gavran, Ivan and Darulova, Eva and Majumdar, Rupak},
title = {Lassie: HOL4 tactics by example},
year = {2021},
isbn = {9781450382991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437992.3439925},
doi = {10.1145/3437992.3439925},
abstract = {Proof engineering efforts using interactive theorem proving have yielded several impressive projects in software systems and mathematics. A key obstacle to such efforts is the requirement that the domain expert is also an expert in the low-level details in constructing the proof in a theorem prover. In particular, the user needs to select a sequence of tactics that lead to a successful proof, a task that in general requires knowledge of the exact names and use of a large set of tactics.  We present Lassie, a tactic framework for the HOL4 theorem prover that allows individual users to define their own tactic language by example and give frequently used tactics or tactic combinations easier-to-remember names. The core of Lassie is an extensible semantic parser, which allows the user to interactively extend the tactic language through a process of definitional generalization. Defining tactics in Lassie thus does not require any knowledge in implementing custom tactics, while proofs written in Lassie retain the correctness guarantees provided by the HOL4 system. We show through case studies how Lassie can be used in small and larger proofs by novice and more experienced interactive theorem prover users, and how we envision it to ease the learning curve in a HOL4 tutorial.},
booktitle = {Proceedings of the 10th ACM SIGPLAN International Conference on Certified Programs and Proofs},
pages = {212–223},
numpages = {12},
keywords = {Tactic Programming, Semantic Parsing, Interactive Theorem Proving, HOL4},
location = {Virtual, Denmark},
series = {CPP 2021}
}

@inproceedings{10.1145/3167132.3167268,
author = {Paz, Andr\'{e}s and Boussaidi, Ghizlane El},
title = {Building a software requirements specification and design for an avionics system: an experience report},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167268},
doi = {10.1145/3167132.3167268},
abstract = {As with many of the products and systems that pervade us, aircraft rely more and more on software for controlling the behaviour of their systems. In consequence, the field has seen increased work around more up-to-date, effective software engineering technologies for aiding avionics software providers in reducing software and development complexities and supporting them in their certification endeavours. However, there is a lack in the literature of reusable, comprehensive references about avionics software developments in conformance with DO-178C. Moreover, there is a need for a benchmark specification to support the evaluation of proposed engineering approaches in the field. This paper presents a software development case study of an avionics control software for a landing gear system. All the documentation for the software's requirements specification and design has been developed to conform with the DO-178C guideline and the applicable DO-331 and DO-332 supplements for model-based and object-oriented development, respectively. A requirements specification and design methodology is proposed and followed for the construction of the software in the case study. Furthermore, the paper discusses the observations, and challenges and issues experienced throughout the process.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1262–1271},
numpages = {10},
keywords = {software design, requirements specification, landing gear control software, experience report, case study, avionics software, DO-332, DO-331, DO-178C},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3550356.3561534,
author = {Cornelis, Milan and Vanommeslaeghe, Yon and Van Acker, Bert and De Meulenaere, Paul},
title = {An ontology DSL for the co-design of mechatronic systems},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561534},
doi = {10.1145/3550356.3561534},
abstract = {The complexity of mechatronic systems is vastly increasing. Therefore, the design of these systems requires different engineering domains, e.g., the mechanical, electrical, and control domains, to work together. The different domains often work in parallel to gain efficiency in this so-called co-design process. However, the design choices made by engineers in one domain can influence parameters in another domain. Too little or even no knowledge about these cross-domain influences may later lead to system integration problems or to degraded system performance. Solving these problems requires taking steps back in the development process, causing a higher design cost. In order to improve this cross-domain collaboration, we propose using ontologies to assist the co-design process by explicitly capturing the design dependencies, both within and across the engineering domains. However, designing ontologies can be complex and is labor-intensive, especially if one relies on generic ontology languages like the Web Ontology Language 2 (OWL 2). Therefore, we created a Domain Specific Language (DSL) focusing on the essential complexity, which enables engineers to design a cross-domain system ontology in a consistent and straightforward way. We elaborate on the metamodel for this DSL, discuss the realization of a prototype tool, and demonstrate how one can then reason on this ontology to derive new information about the various cross-domain design relationships.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {633–642},
numpages = {10},
keywords = {ontology, metamodel, mechatronics, domain-specific language, co-design},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3468791.3469119,
author = {Sima, Ana Claudia and Mendes de Farias, Tarcisio and Anisimova, Maria and Dessimoz, Christophe and Robinson-Rechavi, Marc and Zbinden, Erich and Stockinger, Kurt},
title = {Bio-SODA: Enabling Natural Language Question Answering over Knowledge Graphs without Training Data},
year = {2021},
isbn = {9781450384131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468791.3469119},
doi = {10.1145/3468791.3469119},
abstract = {The problem of natural language processing over structured data has become a growing research field, both within the relational database and the Semantic Web community, with significant efforts involved in question answering over knowledge graphs (KGQA). However, many of these approaches are either specifically targeted at open-domain question answering using DBpedia, or require large training datasets to translate a natural language question to SPARQL in order to query the knowledge graph. Hence, these approaches often cannot be applied directly to complex scientific datasets where no prior training data is available. In this paper, we focus on the challenges of natural language processing over knowledge graphs of scientific datasets. In particular, we introduce Bio-SODA, a natural language processing engine that does not require training data in the form of question-answer pairs for generating SPARQL queries. Bio-SODA uses a generic graph-based approach for translating user questions to a ranked list of SPARQL candidate queries. Furthermore, Bio-SODA uses a novel ranking algorithm that includes node centrality as a measure of relevance for selecting the best SPARQL candidate query. Our experiments with real-world datasets across several scientific domains, including the official bioinformatics Question Answering over Linked Data (QALD) challenge, as well as the CORDIS dataset of European projects, show that Bio-SODA outperforms publicly available KGQA systems by an F1-score of least 20% and by an even higher factor on more complex bioinformatics datasets.},
booktitle = {Proceedings of the 33rd International Conference on Scientific and Statistical Database Management},
pages = {61–72},
numpages = {12},
keywords = {Knowledge Graphs, Question Answering, Ranking},
location = {Tampa, FL, USA},
series = {SSDBM '21}
}

@inproceedings{10.1145/3555041.3589678,
author = {John, Rogers Jeffrey Leo and Bacon, Dylan and Chen, Junda and Ramesh, Ushmal and Li, Jiatong and Das, Deepan and Claus, Robert and Kendall, Amos and Patel, Jignesh M.},
title = {DataChat: An Intuitive and Collaborative Data Analytics Platform},
year = {2023},
isbn = {9781450395076},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555041.3589678},
doi = {10.1145/3555041.3589678},
abstract = {Enterprises invest in data platforms with the aim of extracting meaningful information through analytics. Typically, experts create analytics pipelines that feed into dashboards and provide answers to predetermined questions. This approach makes analytics a spectator sport for most people and introduces operational bottlenecks to leveraging those investments. To improve the value derived from data, many organizations are opting to open up their data assets and allow access to a wider range of users. However, using programming languages such as SQL and Python for analytics can be difficult for most enterprise users. DataChat provides a simplified data science approach that is intuitive, powerful, and accessible to all data users. The platform is built on a library of data functions that are cleanly abstracted to maximize efficiency and ease of use while maintaining a rich suite of tools necessary for data science. With these functions, users can create data analysis pipelines by using a simple point-and-click interface in a spreadsheet view or by using natural English interfaces. Modern sharing and collaboration features are central to all aspects of the platform, allowing teams to easily bridge expertise gaps. A deeper understanding of results is facilitated by providing automatically-generated English explanations of how they were derived. By enhancing these aspects of data science and human-to-human communication, the platform addresses the needs that many organizations are encountering as their analytics needs mature.},
booktitle = {Companion of the 2023 International Conference on Management of Data},
pages = {203–215},
numpages = {13},
keywords = {analytics, data science, generative AI, machine learning},
location = {Seattle, WA, USA},
series = {SIGMOD '23}
}

@inproceedings{10.1145/3550355.3552439,
author = {Martins, Jo\~{a}o and Fonseca, Jos\'{e} M. and Costa, Rafael and Campos, Jos\'{e} C. and Cunha, Alcino and Macedo, Nuno and Oliveira, Jos\'{e} N.},
title = {Verification of railway network models with EVEREST},
year = {2022},
isbn = {9781450394666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550355.3552439},
doi = {10.1145/3550355.3552439},
abstract = {Models - at different levels of abstraction and pertaining to different engineering views - are central in the design of railway networks, in particular signalling systems. The design of such systems must follow numerous strict rules, which may vary from project to project and require information from different views. This renders manual verification of railway networks costly and error-prone.This paper presents EVEREST, a tool for automating the verification of railway network models that preserves the loosely coupled nature of the design process. To achieve this goal, EVEREST first combines two different views of a railway network model - the topology provided in signalling diagrams containing the functional infrastructure, and the precise coordinates of the elements provided in technical drawings (CAD) - in a unified model stored in the railML standard format. This railML model is then verified against a set of user-defined infrastructure rules, written in a custom modal logic that simplifies the specification of spatial constraints in the network. The violated rules can be visualized both in the signalling diagrams and technical drawings, where the element(s) responsible for the violation are highlighted.EVEREST is integrated in a long-term effort of EFACEC to implement industry-strong tools to automate and formally verify the design of railway solutions.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems},
pages = {345–355},
numpages = {11},
keywords = {formal infrastructure rule specification, railML, railway engineering, railway network model verification},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3236024.3264838,
author = {Brun, Yuriy and Meliou, Alexandra},
title = {Software fairness},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3264838},
doi = {10.1145/3236024.3264838},
abstract = {A goal of software engineering research is advancing software quality and the success of the software engineering process. However, while recent studies have demonstrated a new kind of defect in software related to its ability to operate in fair and unbiased manner, software engineering has not yet wholeheartedly tackled these new kinds of defects, thus leaving software vulnerable. This paper outlines a vision for how software engineering research can help reduce fairness defects and represents a call to action by the software engineering research community to reify that vision. Modern software is riddled with examples of biased behavior, from automated translation injecting gender stereotypes, to vision systems failing to see faces of certain races, to the US criminal justice sytem relying on biased computational assessments of crime recidivism. While systems may learn bias from biased data, bias can also emerge from ambiguous or incomplete requirement specification, poor design, implementation bugs, and unintended component interactions. We argue that software fairness is analogous to software quality, and that numerous software engineering challenges in the areas of requirements, specification, design, testing, and verification need to be tackled to solve this problem.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {754–759},
numpages = {6},
keywords = {Software fairness, software bias, software process},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3531706.3536463,
author = {Rocha Silva, Thiago},
title = {Towards a Domain-Specific Language to Specify Interaction Scenarios for Web-Based Graphical User Interfaces},
year = {2022},
isbn = {9781450390316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531706.3536463},
doi = {10.1145/3531706.3536463},
abstract = {The communication gap between software developers and subject-matter experts is one of the foremost long-standing problems in software development. The level of formality of the user requirements specification has a strong impact on the ability of these two groups to communicate effectively. Domain-Specific Languages (DSLs) are seen as one of the potential solutions to address this issue by raising the abstraction level of the software specification while keeping the necessary formalism to allow for software analysis, design, and verification. This paper discusses the ongoing development of a high-level DSL and its rich editing environment to allow the specification of consistent and testable interaction scenarios as user requirements for web-based graphical user interfaces. The language grammar has been developed based on the Gherkin syntax that supports Behaviour-Driven Development (BDD). Results of a preliminary evaluation regarding the consistency of actions and states of interaction elements specified for web user interfaces showed that the grammar is able to support a consistent specification of BDD scenarios as user requirements at the interaction level.},
booktitle = {Companion of the 2022 ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {48–53},
numpages = {6},
keywords = {Behaviour-Driven Development (BDD), Domain-Specific Languages (DSLs), Scenario-Based Design, User Requirements.},
location = {Sophia Antipolis, France},
series = {EICS '22 Companion}
}

@inproceedings{10.1145/3239372.3239382,
author = {Shin, Seung Yeob and Chaouch, Karim and Nejati, Shiva and Sabetzadeh, Mehrdad and Briand, Lionel C. and Zimmer, Frank},
title = {HITECS: A UML Profile and Analysis Framework for Hardware-in-the-Loop Testing of Cyber Physical Systems},
year = {2018},
isbn = {9781450349499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239372.3239382},
doi = {10.1145/3239372.3239382},
abstract = {Hardware-in-the-loop (HiL) testing is an important step in the development of cyber physical systems (CPS). CPS HiL test cases manipulate hardware components, are time-consuming and their behaviors are impacted by the uncertainties in the CPS environment. To mitigate the risks associated with HiL testing, engineers have to ensure that (1) HiL test cases are well-behaved, i.e., they implement valid test scenarios and do not accidentally damage hardware, and (2) HiL test cases can execute within the time budget allotted to HiL testing. This paper proposes an approach to help engineers systematically specify and analyze CPS HiL test cases. Leveraging the UML profile mechanism, we develop an executable domain-specific language, HITECS, for HiL test case specification. HITECS builds on the UML Testing Profile (UTP) and the UML action language (Alf). Using HITECS, we provide analysis methods to check whether HiL test cases are well-behaved, and to estimate the execution times of these test cases before the actual HiL testing stage. We apply HITECS to an industrial case study from the satellite domain. Our results show that: (1) HITECS is feasible to use in practice; (2) HITECS helps engineers define more complete and effective well-behavedness assertions for HiL test cases, compared to when these assertions are defined without systematic guidance; (3) HITECS verifies in practical time that HiL test cases are well-behaved; and (4) HITECS accurately estimates HiL test case execution times.},
booktitle = {Proceedings of the 21th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {357–367},
numpages = {11},
keywords = {Cyber Physical Systems, JavaPathFinder, Model Checking and Simulation, Test Case Specification and Analysis, UML Profile},
location = {Copenhagen, Denmark},
series = {MODELS '18}
}

@inproceedings{10.1145/3634814.3634826,
author = {Zahrin, Mohd Firdaus and Osman, Mohd Hafeez and Hassan, Sa'adah and Haron, Azlena},
title = {Effectiveness of the Improvement Recommendations Model for Addressing the Syntactic Ambiguity in Malay Requirements Specifications},
year = {2024},
isbn = {9798400708534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634814.3634826},
doi = {10.1145/3634814.3634826},
abstract = {Malaysian government agencies have developed various citizen-service platforms and applications. However, unresolved Malay requirements specification (RS) ambiguities might disrupt the software development project's completion. Most prior research has focused on English RS but not Malay. Therefore, devising a model to recommend improvements for addressing the ambiguous Malay requirements specifications is challenging. This paper investigates the state-of-the-art approaches for establishing and validating the effectiveness of an improvement recommendations model for addressing syntactical ambiguity in Malay RS. We devised an improvement recommendations model using natural language processing (NLP), rule-based, part-of-speech (POS), subject-verb-object (SVO) patterns, and Malay boilerplate. Thirteen experts from various Malaysian public sector agencies validated the model's effectiveness based on improved Malay RS enhanced by disambiguating syntactic ambiguity terms. We surveyed the experts’ opinions on the model's effectiveness and work experience through semi-structured interviews. This study revealed that 76.9% of the experts agreed and 23.1% strongly agreed that the improvement recommendations model effectively improved Malay RS by handling ambiguity. Experts with at least six years of experience in requirements engineering can comprehensively validate the improved Malay RS. The experts recognised the necessity for a model/ tool to aid requirements engineers in validating and enhancing the Malay RS by addressing ambiguity and recommending improved Malay RS structure based on Malay boilerplate syntax.},
booktitle = {Proceedings of the 2023 4th Asia Service Sciences and Software Engineering Conference},
pages = {80–88},
numpages = {9},
keywords = {Effectiveness, Improvement Recommendations Model, Malay Requirements Specification, Natural Language Processing, Syntactic Ambiguity},
location = {<conf-loc>, <city>Aizu-Wakamatsu City</city>, <country>Japan</country>, </conf-loc>},
series = {ASSE '23}
}

@inproceedings{10.1109/MODELS-C.2019.00108,
author = {Saini, Rijul and Mussbacher, Gunter and Guo, Jin L. C. and Kienzle, J\"{o}rg},
title = {Teaching modelling literacy: an artificial intelligence approach},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00108},
doi = {10.1109/MODELS-C.2019.00108},
abstract = {In Model-Driven Engineering (MDE), models are used to build and analyze complex systems. In the last decades, different modelling formalisms have been proposed for supporting software development. However, their adoption and practice strongly rely on mastering essential modelling skills to develop a complete and coherent model-based system. Moreover, it is often difficult for novice modellers to get direct and timely feedback and recommendations on their modelling strategies and decisions, particularly in large classroom settings which hinders their learning. Certainly, there is an opportunity to apply Artificial Intelligence (AI) techniques to an MDE learning environment to empower the provisioning of automated and intelligent modelling advocacy. In this paper, we propose a framework called ModBud (a modelling buddy) to educate novice modellers about the art of abstraction. ModBud uses natural language processing (NLP) and machine learning (ML) to create modelling bots with the aim of improving the modelling skills of novice modellers and assisting other practitioners, too. These bots could be used to support teaching with automatic creation or grading of models and enhance learning beyond the traditional classroom-based MDE education with timely feedback and personalized tutoring. Research challenges for the proposed framework are discussed and a research roadmap is presented.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems},
pages = {714–719},
numpages = {6},
keywords = {AI, MDE, ML, ModBud, NLP, bots, models},
location = {Munich, Germany},
series = {MODELS '19}
}

@inproceedings{10.1145/3167132.3167344,
author = {Halilaj, Lavdim and Grangel-Gonz\'{a}lez, Irl\'{a}n and Lohmann, Steffen and Vidal, Maria-Esther and Auer, S\"{o}ren},
title = {EffTE: a dependency-aware approach for test-driven ontology development},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167344},
doi = {10.1145/3167132.3167344},
abstract = {The development of domain-specific ontologies requires joint efforts among different groups of stakeholders, such as ontology engineers and domain experts. By following a test-driven development technique, a set of test cases ensures that ontology changes do not violate predefined requirements. However, since the number of test cases can be large and their evaluation time may be high, the ontology development process can be negatively impacted. We propose EffTE, an approach for efficient test-driven ontology development relying on a graph-based model of dependencies between test cases. It enables prioritization and selection of test cases to be evaluated. Traversing the dependency graph is realized using breadth-first search along with a mechanism that tracks tabu test cases, i.e., test cases to be ignored for further evaluation due to faulty parent test cases. As a result, the number of evaluated test cases is minimized, thus reducing the time required for validating the ontology after each modification. We conducted an empirical evaluation to determine the efficiency of our approach. The evaluation results suggest that our approach is more efficient than an exhaustive evaluation of the test cases; in particular with an increasing ontology size and number of test cases.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1976–1983},
numpages = {8},
keywords = {dependency graph, ontology engineering, test cases, test-driven ontology development},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3358501.3361240,
author = {Pschorn, Patrick and Oliveira Antonino, Pablo and Morgenstern, Andreas and Kuhn, Thomas},
title = {A constraint modeling framework for domain-specific languages},
year = {2019},
isbn = {9781450369848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358501.3361240},
doi = {10.1145/3358501.3361240},
abstract = {The growing usage of Domain Specific Modeling Languages (DSML) for architecture view frameworks induces a need for automatic verification of non-functional model properties like completeness and consistency. However, we argue that the high demand for tailored architecture view frameworks is not complemented by appropriate constraint specification facilities. OCL is a common language for defining modeling constraints, but industry user reports indicate that despite its accuracy, it is too complex to be adopted in industrial scale. Approaches that were proposed to simplify the use of OCL either operate on technical formalisms or lack tool support to express new, or more complex types of constraints that can be validated automatically on the model. To address this challenge, we present a constraint modeling framework for the specification and validation of constraints on DSMLs. A Constraint Modeling Language (CML) created based on this framework provides a high level constraint specification en- vironment by using extensible template implementations to enable the automatic validation in computer aided software engineering (CASE) tools. We evaluate the approach in different industry projects and observe that using the proposed framework enhances understandability and effectiveness of constraint specification.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Workshop on Domain-Specific Modeling},
pages = {20–29},
numpages = {10},
keywords = {Architecture View Frameworks, Constraint Modeling, Constraints, Domain-Specific Modeling},
location = {Athens, Greece},
series = {DSM 2019}
}

@article{10.1145/3587691,
author = {Hirzel, Martin},
title = {Low-Code Programming Models},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/3587691},
doi = {10.1145/3587691},
abstract = {Low-code has the potential to empower more people to automate tasks by creating computer programs.},
journal = {Commun. ACM},
month = {sep},
pages = {76–85},
numpages = {10}
}

@inproceedings{10.1145/3229345.3229372,
author = {de Araujo, Denis Andrei and Hentges, Alencar Rodrigo and Rigo, Sandro},
title = {A Linguistic Approach to Short Query and Answer Systems},
year = {2018},
isbn = {9781450365598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229345.3229372},
doi = {10.1145/3229345.3229372},
abstract = {Conversational agents for natural language interaction, in general, face some challenges in providing content to support the responses requested by users. Usually of the content of the responses is manually defined, which is not an adequate solution. This work presents an approach to enabling these systems to use automatically the concepts and relationships stored in Knowledge Bases. The main contribution of our approach is the generalization of the system, which can answer questions related to multiple domains. This is accomplished based on linguistic information as support to the activities of natural language understanding. The results obtained in the classification of question types and identification of nominal complements are presented and indicate promising results.},
booktitle = {Proceedings of the XIV Brazilian Symposium on Information Systems},
articleno = {24},
numpages = {8},
keywords = {Chatbots, Conversational Agents, Natural Language Processing, Ontology, Question and Answer System},
location = {Caxias do Sul, Brazil},
series = {SBSI '18}
}

@article{10.1145/3299887.3299892,
author = {Hirzel, Martin and Baudart, Guillaume and Bonifati, Angela and Della Valle, Emanuele and Sakr, Sherif and Akrivi Vlachou, Akrivi},
title = {Stream Processing Languages in the Big Data Era},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/3299887.3299892},
doi = {10.1145/3299887.3299892},
abstract = {This paper is a survey of recent stream processing languages, which are programming languages for writing applications that analyze data streams. Data streams, or continuous data flows, have been around for decades. But with the advent of the big-data era, the size of data streams has increased dramatically. Analyzing big data streams yields immense advantages across all sectors of our society. To analyze streams, one needs to write a stream processing application. This paper showcases several languages designed for this purpose, articulates underlying principles, and outlines open challenges.},
journal = {SIGMOD Rec.},
month = {dec},
pages = {29–40},
numpages = {12}
}

@article{10.1162/coli_a_00378,
author = {Ranta, Aarne and Angelov, Krasimir and Gruzitis, Normunds and Kolachina, Prasanth},
title = {Abstract Syntax as Interlingua: Scaling Up the Grammatical Framework
                    from Controlled Languages to Robust Pipelines},
year = {2020},
issue_date = {June 2020},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {46},
number = {2},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00378},
doi = {10.1162/coli_a_00378},
abstract = {Abstract syntax is an interlingual representation used in compilers. Grammatical
                    Framework (GF) applies the abstract syntax idea to natural languages. The
                    development of GF started in 1998, first as a tool for controlled language
                    implementations, where it has gained an established position in both academic
                    and commercial projects. GF provides grammar resources for over 40 languages,
                    enabling accurate generation and translation, as well as grammar engineering
                    tools and components for mobile and Web applications. On the research side, the
                    focus in the last ten years has been on scaling up GF to wide-coverage language
                    processing. The concept of abstract syntax offers a unified view on many other
                    approaches: Universal Dependencies, WordNets, FrameNets, Construction Grammars,
                    and Abstract Meaning Representations. This makes it possible for GF to utilize
                    data from the other approaches and to build robust pipelines. In return, GF can
                    contribute to data-driven approaches by methods to transfer resources from one
                    language to others, to augment data by rule-based generation, to check the
                    consistency of hand-annotated corpora, and to pipe analyses into high-precision
                    semantic back ends. This article gives an overview of the use of abstract syntax
                    as interlingua through both established and emerging NLP applications involving
                    GF.},
journal = {Comput. Linguist.},
month = {jun},
pages = {425–486},
numpages = {62}
}

@proceedings{10.1145/3622758,
title = {Onward! 2023: Proceedings of the 2023 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
year = {2023},
isbn = {9798400703881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 2023 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward! 2023), the premier multidisciplinary conference focused on everything to do with programming and software, including processes, methods, languages, communities and applications. Onward! is more radical, more visionary, and more open than other conferences to ideas that are well-argued but not yet fully proven. We welcome different ways of thinking about, approaching, and reporting on programming language and software engineering research.  

Onward! 2023 is co-located with SPLASH 2023, running from Sunday 22nd of October till Friday 27th of October, in Cascais, Portugal. We are delighted to have Felienne Hermans giving the Onward! keynote, on Wednesday 25th of October, on "Creating a learnable and inclusive programming language".  

All papers and essays that lie here before you received at least three reviews, leading to a decision of accept, reject, or conditional accept. Authors of conditionally accepted papers were provided with explicit requirements for acceptance, and were carefully re-reviewed in the second phase. The essays track received six submissions, out of which four were accepted. The papers track accepted nine out of nineteen submissions.  

We hope that the papers and essays in these proceedings will stimulate and challenge your thinking about programming and software engineering, and we are looking forward to many discussions at the conference.},
location = {<conf-loc>, <city>Cascais</city>, <country>Portugal</country>, </conf-loc>}
}

@inproceedings{10.1145/3459637.3482387,
author = {Wisniewski, Dawid and Potoniec, Jedrzej and Lawrynowicz, Agnieszka},
title = {SeeQuery: An Automatic Method for Recommending Translations of Ontology Competency Questions into SPARQL-OWL},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482387},
doi = {10.1145/3459637.3482387},
abstract = {Ontology authoring is a complicated and error-prone process since the knowledge being modeled is expressed using logic-based formalisms, in which logical consequences of the knowledge have to be foreseen. To make that process easier, competency questions (CQs), being questions expressed in natural language are often stated to trace both the correctness and completeness of the ontology at a given time. However, CQs have to be translated into a formal language, like ontology query language (SPARQL-OWL), to query the ontology. Since the translation step is time-consuming and requires familiarity with the query language used, in this paper, we propose an automatic method named SeeQuery, which recommends SPARQL-OWL queries being translations of CQs stated against a given ontology. It consists of a pipeline of transformations based on template matching and filling, being motivated by the biggest to date publicly available CQ to SPARQL-OWL datasets. We provide a detailed description of SeeQuery and evaluate the method on a separate set of 2 ontologies with their CQs. It is, to date, the only automatic method available for recommending SPARQL-OWL queries out of CQs. The source code of SeeQuery is available at: https://github.com/dwisniewski/SeeQuery.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {2119–2128},
numpages = {10},
keywords = {automatic translation, competency questions, ontology authoring, semantic similarity, sparql-owl},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3430984.3431002,
author = {Sen, Jaydeep and Saha, Diptikalyan and Mittal, Ashish and Sankaranarayanan, Karthik},
title = {Optimizing Interpretation Generation in Natural Language Query Answering for Real Time End Users},
year = {2021},
isbn = {9781450388177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430984.3431002},
doi = {10.1145/3430984.3431002},
abstract = {Natural Language Querying over Database is gaining popularity across different use cases. Most common of them is to democratize the process of data analysis and querying of backend data to naive end users especially business users, obviating the need of knowing back end query language. Natural Language Query answering systems have thus seen widespread usage in industry too where business users want to search their own data to make business decisions. However, a common challenge faced by any natural language query answering system is generation of precise interpretations. The research community although tries to handle the problem via asking clarification questions back to the user, in industry setup this remains an ineffective solution due to various practical usage limitations. For example, it is not fair to assume any end user will be aware of the correct option to answer these clarification questions. Moreover, involving clarification questions and user feedbacks makes the system unusable by one shot API calls, which is the most intuitive usage among common use cases in industry like automated report generation. In this paper, we investigate practical ways to address the problem of precise interpretation generation. We propose novel algorithms to make use of existing technologies like Functional Partitioning of Ontology and Lazy Inclusion to solve this problem. We take our previous state-of-the-art paper ATHENA and further extend it to include our proposed methods. We test with 3 benchmark ontologies to empirically demonstrate the huge improvement over state-of-the-art results by factors of at least 400% in number of interpretation generation and also in the computation time.},
booktitle = {Proceedings of the 3rd ACM India Joint International Conference on Data Science &amp; Management of Data (8th ACM IKDD CODS &amp; 26th COMAD)},
pages = {341–349},
numpages = {9},
location = {Bangalore, India},
series = {CODS-COMAD '21}
}

@inproceedings{10.1145/3372885.3373827,
author = {Wang, Qingxiang and Brown, Chad and Kaliszyk, Cezary and Urban, Josef},
title = {Exploration of neural machine translation in autoformalization of mathematics in Mizar},
year = {2020},
isbn = {9781450370974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372885.3373827},
doi = {10.1145/3372885.3373827},
abstract = {In this paper we share several experiments trying to automatically translate informal mathematics into formal mathematics. In our context informal mathematics refers to human-written mathematical sentences in the LaTeX format; and formal mathematics refers to statements in the Mizar language. We conducted our experiments against three established neural network-based machine translation models that are known to deliver competitive results on translating between natural languages. To train these models we also prepared four informal-to-formal datasets. We compare and analyze our results according to whether the model is supervised or unsupervised. In order to augment the data available for auto-formalization and improve the results, we develop a custom type-elaboration mechanism and integrate it in the supervised translation.},
booktitle = {Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs},
pages = {85–98},
numpages = {14},
keywords = {Automating Formalization, Machine Learning, Mizar, Neural Machine Translation, Proof Assistants},
location = {New Orleans, LA, USA},
series = {CPP 2020}
}

@inproceedings{10.1145/3510003.3510171,
author = {He, Jie and Bartocci, Ezio and Ni\v{c}kovi\'{c}, Dejan and Isakovic, Haris and Grosu, Radu},
title = {DeepSTL: from english requirements to signal temporal logic},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510171},
doi = {10.1145/3510003.3510171},
abstract = {Formal methods provide very powerful tools and techniques for the design and analysis of complex systems. Their practical application remains however limited, due to the widely accepted belief that formal methods require extensive expertise and a steep learning curve. Writing correct formal specifications in form of logical formulas is still considered to be a difficult and error prone task.In this paper we propose DeepSTL, a tool and technique for the translation of informal requirements, given as free English sentences, into Signal Temporal Logic (STL), a formal specification language for cyber-physical systems, used both by academia and advanced research labs in industry. A major challenge to devise such a translator is the lack of publicly available informal requirements and formal specifications. We propose a two-step workflow to address this challenge. We first design a grammar-based generation technique of synthetic data, where each output is a random STL formula and its associated set of possible English translations. In the second step, we use a state-of-the-art transformer-based neural translation technique, to train an accurate attentional translator of English to STL. The experimental results show high translation quality for patterns of English requirements that have been well trained, making this workflow promising to be extended for processing more complex translation tasks.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {610–622},
numpages = {13},
keywords = {formal specification, machine translation, requirements engineering, signal temporal logic (STL)},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3611643.3616323,
author = {Wang, Shangwen and Geng, Mingyang and Lin, Bo and Sun, Zhensu and Wen, Ming and Liu, Yepang and Li, Li and Bissyand\'{e}, Tegawend\'{e} F. and Mao, Xiaoguang},
title = {Natural Language to Code: How Far Are We?},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616323},
doi = {10.1145/3611643.3616323},
abstract = {A longstanding dream in software engineering research is to devise effective approaches for automating development tasks based on developers' informally-specified intentions. Such intentions are generally in the form of natural language descriptions. In recent literature, a number of approaches have been proposed to automate tasks such as code search and even code generation based on natural language inputs. While these approaches vary in terms of technical designs, their objective is the same: transforming a developer's intention into source code. The literature, however, lacks a comprehensive understanding towards the effectiveness of existing techniques as well as their complementarity to each other. We propose to fill this gap through a large-scale empirical study where we systematically evaluate natural language to code techniques. Specifically, we consider six state-of-the-art techniques targeting code search, and four targeting code generation. Through extensive evaluations on a dataset of 22K+ natural language queries, our study reveals the following major findings: (1) code search techniques based on model pre-training are so far the most effective while code generation techniques can also provide promising results; (2) complementarity widely exists among the existing techniques; and (3) combining the ten techniques together can enhance the performance for 35% compared with the most effective standalone technique. Finally, we propose a post-processing strategy to automatically integrate different techniques based on their generated code. Experimental results show that our devised strategy is both effective and extensible.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {375–387},
numpages = {13},
keywords = {Code Generation, Code Search, Pre-Training Technique},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.1145/3133324,
author = {Padget, Julian A. and Vasconcelos, Wamberto W.},
title = {Fine-Grained Access Control via Policy-Carrying Data},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3133324},
doi = {10.1145/3133324},
abstract = {We address the problem of associating access policies with datasets and how to monitor compliance via policy-carrying data. Our contributions are a formal model in first-order logic inspired by normative multi-agent systems to regulate data access, and a computational model for the validation of specific use cases and the verification of policies against criteria. Existing work on access policy identifies roles as a key enabler, with which we concur, but much of the rest focusses on authentication and authorization technology. Our proposal aims to address the normative principles put forward in Berners-Lee’s bill of rights for the internet, through human-readable but machine-processable access control policies.},
journal = {ACM Trans. Internet Technol.},
month = {feb},
articleno = {31},
numpages = {24},
keywords = {Deontic logic, action language, answer set programming, data sharing, privacy policy}
}

@article{10.1145/3464383,
author = {Crovari, Pietro and Pid\`{o}, Sara and Pinoli, Pietro and Bernasconi, Anna and Canakoglu, Arif and Garzotto, Franca and Ceri, Stefano},
title = {GeCoAgent: A Conversational Agent for Empowering Genomic Data Extraction and Analysis},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3464383},
doi = {10.1145/3464383},
abstract = {With the availability of reliable and low-cost DNA sequencing, human genomics is relevant to a growing number of end-users, including biologists and clinicians. Typical interactions require applying comparative data analysis to huge repositories of genomic information for building new knowledge, taking advantage of the latest findings in applied genomics for healthcare. Powerful technology for data extraction and analysis is available, but broad use of the technology is hampered by the complexity of accessing such methods and tools.This work presents GeCoAgent, a big-data service for clinicians and biologists. GeCoAgent uses a dialogic interface, animated by a chatbot, for supporting the end-users’ interaction with computational tools accompanied by multi-modal support. While the dialogue progresses, the user is accompanied in extracting the relevant data from repositories and then performing data analysis, which often requires the use of statistical methods or machine learning. Results are returned using simple representations (spreadsheets and graphics), while at the end of a session the dialogue is summarized in textual format. The innovation presented in this article is concerned with not only the delivery of a new tool but also our novel approach to conversational technologies, potentially extensible to other healthcare domains or to general data science.},
journal = {ACM Trans. Comput. Healthcare},
month = {oct},
articleno = {3},
numpages = {29},
keywords = {Conversational agents, natural language understanding, genomic computing}
}

@article{10.1145/3453475,
author = {Dwivedi, Vimal and Pattanaik, Vishwajeet and Deval, Vipin and Dixit, Abhishek and Norta, Alex and Draheim, Dirk},
title = {Legally Enforceable Smart-Contract Languages: A Systematic Literature Review},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3453475},
doi = {10.1145/3453475},
abstract = {Smart contracts are a key component of today’s blockchains. They are critical in controlling decentralized autonomous organizations (DAO). However, smart contracts are not yet legally binding nor enforceable; this makes it difficult for businesses to adopt the DAO paradigm. Therefore, this study reviews existing Smart Contract Languages (SCL) and identifies properties that are critical to any future SCL for drafting legally binding contracts. This is achieved by conducting a Systematic Literature Review (SLR) of white- and grey literature published between 2015 and 2019. Using the SLR methodology, 45 Selected and 28 Supporting Studies detailing 45 state-of-the-art SCLs are selected. Finally, 10 SCL properties that enable legally compliant DAOs are discovered, and specifications for developing SCLs are explored.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {110},
numpages = {34},
keywords = {Blockchain, decentralized autonomous organization, expressiveness, smart contract language, suitability, systematic literature review}
}

@article{10.1145/3423166,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Quintero, Antonia M. Reina},
title = {Smart Contract Languages: A Multivocal Mapping Study},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3423166},
doi = {10.1145/3423166},
abstract = {Blockchain is a disruptive technology that has attracted the attention of the scientific community and companies, as proven by the exponential growth of publications on this topic in recent years. This growing interest is mainly due to the promise that the use of blockchain enables it to be verified, without including any trusted intermediaries, that the information received from the network is authentic and up-to-date. In this respect, blockchain is a distributed database that can be seen as a ledger that records all transactions that have ever been executed. In this context, smart contracts are pieces of software used to facilitate, verify, and enforce the negotiation of a transaction on a blockchain platform. These pieces of software are implemented by using programming languages, which are sometimes provided by the blockchain platforms themselves. This study aims to (1) identify and categorise the state-of-the-art related to smart contract languages, in terms of the existing languages and their main features, and (2) identify new research opportunities. The review has been conducted as a multivocal mapping study that follows the guidelines proposed by Garousi et&nbsp;al. for conducting multivocal literature reviews, as well as the guidelines proposed by Kitchenham and Charters for conducting mapping studies. As a result of the implementation of the review protocol, 4,119 papers were gathered, and 109 of them were selected for extraction. The contributions of this article are twofold: (1) 101 different smart contract languages have been identified and classified according to a variety of criteria; (2) a discussion on the findings and their implications for future research have been outlined. As a conclusion, it could be stated that a rigorous and replicable overview of the state-of-the-art of smart contract languages has been provided that can benefit not only researchers but also practitioners in the field, thanks to its multivocal nature.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {3},
numpages = {38},
keywords = {Smart contract language, blockchain, multivocal literature mapping study, systematic literature review}
}

@article{10.1145/3589339,
author = {Sharma, Yogesh and Bhamare, Deval and Sastry, Nishanth and Javadi, Bahman and Buyya, Rajkumar},
title = {SLA Management in Intent-Driven Service Management Systems: A Taxonomy and Future Directions},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3589339},
doi = {10.1145/3589339},
abstract = {Traditional, slow and error-prone human-driven methods to configure and manage Internet service requests are proving unsatisfactory. This is due to an increase in Internet applications with stringent quality of service (QoS) requirements. Which demands faster and fault-free service deployment with minimal or without human intervention. With this aim, intent-driven service management (IDSM) has emerged, where users express their service level agreement (SLA) requirements in a declarative manner as intents. With the help of closed control-loop operations, IDSM performs service configurations and deployments, autonomously to fulfill the intents. This results in a faster deployment of services and reduction in configuration errors caused by manual operations, which in turn reduces the SLA violations. This article is an attempt to provide a systematic review of How the IDSM systems manage and fulfill the SLA requirements specified as intents. As an outcome, the review identifies four intent management activities, which are performed in a closed-loop manner. For each activity, a taxonomy is proposed and used to compare the existing techniques for SLA management in IDSM systems. A critical analysis of all the considered research articles in the review and future research directions are presented in the conclusion.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {292},
numpages = {38},
keywords = {Intent-driven service management, intent processing, service level agreements, cloud computing, networks, zero-touch service management}
}

@article{10.1145/3616401,
author = {Bringhenti, Daniele and Marchetto, Guido and Sisto, Riccardo and Valenza, Fulvio},
title = {Automation for Network Security Configuration: State of the Art and Research Trends},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3616401},
doi = {10.1145/3616401},
abstract = {The size and complexity of modern computer networks are progressively increasing, as a consequence of novel architectural paradigms such as the Internet of Things and network virtualization. Consequently, a manual orchestration and configuration of network security functions is no more feasible in an environment where cyber attacks can dramatically exploit breaches related to any minimum configuration error. A new frontier is then the introduction of automation in network security configuration, i.e., automatically designing the architecture of security services and the configurations of network security functions, such as firewalls, Virtual Private Networks gateways, and so on. This opportunity has been enabled by modern computer networks technologies, such as virtualization. In view of these considerations, the motivations for the introduction of automation in network security configuration are first introduced, along with the key automation enablers. Then, the current state of the art in this context is surveyed, focusing on both the achieved improvements and the current limitations. Finally, possible future trends in the field are illustrated.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {57},
numpages = {37},
keywords = {Network security, network virtualization, policy-based management}
}

@inproceedings{10.1145/3460620.3460768,
author = {M. Maatuk, Abdelsalam and A. Abdelnabi, Esra},
title = {Generating UML Use Case and Activity Diagrams Using NLP Techniques and Heuristics Rules},
year = {2021},
isbn = {9781450388382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460620.3460768},
doi = {10.1145/3460620.3460768},
abstract = {The process of generating Unified Modeling Language (UML) Diagrams from Natural Language (NL) requirements is considered a complex and challenging task. Software requirements specification is often written in NL format, which causes potential problems. Requirement's analysts analyze and process natural language requirements manually to extract the UML elements. The manual analysis takes a lot of time and effort, which justifies the need for automated support. This paper proposes an approach to facilitate the NL requirements analysis process and UML diagrams extraction from NL textual requirements using natural language processing (NLP) techniques and heuristics rules. This approach focuses on generating use-case and activity diagrams. The approach has been applied to a case study and evaluated through an experimental. The evaluation of the approach will be conducted through a comparative study. The experimental results prove that the proposed approach is considerably improved as compared to the other approaches.},
booktitle = {International Conference on Data Science, E-Learning and Information Systems 2021},
pages = {271–277},
numpages = {7},
keywords = {NLP, Requirement Analysis, Software Requirement Specification, UML diagrams},
location = {Ma'an, Jordan},
series = {DATA'21}
}

@inproceedings{10.1145/3357526.3357535,
author = {Shang, Xiaojing and Ling, Ming and Shen, Shan and Shao, Tianxiang and Yang, Jun},
title = {RRS cache: a low voltage cache based on timing speculation SRAM with a reuse-aware cacheline remapping mechanism},
year = {2019},
isbn = {9781450372060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357526.3357535},
doi = {10.1145/3357526.3357535},
abstract = {More and more applications are expanding their power supply to the near-threshold region to mitigate the ever-worsening "Power Wall" problem. However, the probability of timing failure in SRAM accessing becomes unacceptably high at low voltage, which makes the SRAM bottleneck of the system performance. The timing speculation SRAM (SSRAM) can recognize the timing failures and correct them by extending the bitline discharging time. But for the cache with an operation granularity of a cacheline, even one-bit cell with timing failure makes the whole cacheline to be accessed with the conservative timing margin, which nullifies the advantage of timing speculations. Such weak cachelines, which contain at least one-bit cell with timing failure, bring significant performance degradation in a low-voltage cache.In this paper, we propose RRS cache, a set-associative L1 cache based on the SSRAM, to reduce the weak cacheline access penalty under low supply voltages. RRS cache improves the proportion of the error-free cachelines (or strong cachelines) by clustering as more as possible bit cells with timing failures into weak cachelines. In addition, it swaps the frequently reused data to the strong cachelines to further reduce the average access latency and the energy consumption by an optimized filling/replacement policy. According to the simulation results, RRS cache improves the performance by 29.79% and reduces the energy consumption by 17.64% for a four-core processor with only 4.12% extra area overhead compared to the cache design with the raw timing speculation SRAM.},
booktitle = {Proceedings of the International Symposium on Memory Systems},
pages = {451–458},
numpages = {8},
keywords = {cacheline remapping, low-voltage cache, near-threshold voltage, replacement policy, timing speculation SRAM},
location = {Washington, District of Columbia, USA},
series = {MEMSYS '19}
}

@article{10.1145/3572837,
author = {Baxter, James and Cavalcanti, Ana and Gazda, Maciej and Hierons, Robert M.},
title = {Testing using CSP Models: Time, Inputs, and Outputs},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {1529-3785},
url = {https://doi.org/10.1145/3572837},
doi = {10.1145/3572837},
abstract = {The existing testing theories for CSP cater for verification of interaction patterns&nbsp;(traces) and deadlocks, but not time. We address here refinement and testing based on a dialect of CSP, called tock-CSP, which can capture discrete time properties. This version of CSP has been of widespread interest for decades; recently, it has been given a denotational semantics, and model checking has become possible using a well established tool. Here, we first equip tock-CSP with a novel semantics for testing, which distinguishes input and output events:&nbsp;the standard models of (tock-)CSP do not differentiate them, but for testing this is essential. We then present a new testing theory for timewise refinement, based on novel definitions of test and test execution. Finally, we reconcile refinement and testing by relating timed ioco testing and refinement in tock-CSP with inputs and outputs. With these results, this paper provides, for the first time, a systematic theory that allows both timed testing and timed refinement to be expressed. An important practical consequence is that this ensures that the notion of correctness used by developers guarantees that tests pass when applied to a correct system and, in addition, faults identified during testing correspond to development mistakes.},
journal = {ACM Trans. Comput. Logic},
month = {jan},
articleno = {17},
numpages = {40},
keywords = {Model-based testing, exhaustive test set, process algebra, refinement}
}

@inproceedings{10.1145/3220228.3220255,
author = {Apaza, Ren\'{a}n Dar\'{\i}o Gonzales and Barrios, Jhon Edilberto Monroy and Becerra, Diego Alonso Iquira and Quispe, Jos\'{e} Alfredo Herrera},
title = {ERS-TOOL: hybrid model for software requirements elicitation in Spanish language},
year = {2018},
isbn = {9781450364454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220228.3220255},
doi = {10.1145/3220228.3220255},
abstract = {The nature of the software requirements is subjective and varied. For this reason the level of complexity increases according to the volume, especially when the requirements are made in a natural language. Therefore obtain quality software requirements that are understandable and unambiguous in the Spanish language becomes a necessity. First, a controlled syntax was proposed to express software requirements taking into account the static and dynamic behavior among the different actors of the system, where the expressions are elaborated based on the Backus-Naur form (BNF). Then a set of writing rules were adapted to the Spanish language, creating four additional rules. Finally, the results of the case study had high accuracy in understandability; also the ambiguity of requirements elicitation was reduced. In addition to improving the development of software engineering activities, since there are no tools available for the elicitation of software requirements with language Spanish.},
booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
pages = {27–30},
numpages = {4},
keywords = {ambiguity of requirements, controlled syntax, requirements engineering},
location = {Prague, Czech Republic},
series = {ICGDA '18}
}

@inproceedings{10.1145/3510003.3510621,
author = {Tufano, Rosalia and Masiero, Simone and Mastropaolo, Antonio and Pascarella, Luca and Poshyvanyk, Denys and Bavota, Gabriele},
title = {Using pre-trained models to boost code review automation},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510621},
doi = {10.1145/3510003.3510621},
abstract = {Code review is a practice widely adopted in open source and industrial projects. Given the non-negligible cost of such a process, researchers started investigating the possibility of automating specific code review tasks. We recently proposed Deep Learning (DL) models targeting the automation of two tasks: the first model takes as input a code submitted for review and implements in it changes likely to be recommended by a reviewer; the second takes as input the submitted code and a reviewer comment posted in natural language and automatically implements the change required by the reviewer. While the preliminary results we achieved are encouraging, both models had been tested in rather simple code review scenarios, substantially simplifying the targeted problem. This was also due to the choices we made when designing both the technique and the experiments. In this paper, we build on top of that work by demonstrating that a pre-trained Text-To-Text Transfer Transformer (T5) model can outperform previous DL models for automating code review tasks. Also, we conducted our experiments on a larger and more realistic (and challenging) dataset of code review activities.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {2291–2302},
numpages = {12},
keywords = {code review, empirical study, machine learning on code},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3474085.3475349,
author = {Ma, Ding and Wu, Xiangqian},
title = {Capsule-based Object Tracking with Natural Language Specification},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475349},
doi = {10.1145/3474085.3475349},
abstract = {Tracking with Natural-Language Specification (TNL) is a joint topic of understanding the vision and natural language with a wide range of applications. In previous works, the communication between two heterogeneous features of vision and language is mainly through a simple dynamic convolution. However, the performance of prior works is capped by the difficulty of linguistic variation of natural language in modeling the dynamically changing target and its surroundings. In the meanwhile, natural language and vision are firstly fused and then utilized for tracking, which is hard to model the query-focused context. Query-focused should pay more attention to context modeling to promote the correlation between these two features. To address these issues, we propose a capsule-based network, referred to as CapsuleTNL, which performs regression tracking with natural language query. In the beginning, the visual and textual input is encoded with capsules, which can not only establish the relationship between entities but also the relationship between the parts of the entity itself. Then, we devise two interaction routing modules, which consist of visual-textual routing module to reduce the linguistic variation of input query and textual-visual routing module to precisely incorporate query-based visual cues simultaneously. To validate the potential of the proposed network for visual object tracking, we evaluate our method on two large tracking benchmarks. The experimental evaluation demonstrates the effectiveness of our capsule-based network.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1948–1956},
numpages = {9},
keywords = {capsule network, natural language specification, visual tracking},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3603166.3632550,
author = {Kruber, Marvin and Pfandzelter, Tobias and Bermbach, David},
title = {A Hybrid Communication Approach for Metadata Exchange in Geo-Distributed Fog Environments},
year = {2024},
isbn = {9798400702341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603166.3632550},
doi = {10.1145/3603166.3632550},
abstract = {Metadata exchange is crucial for efficient geo-distributed fog computing. Existing solutions for metadata exchange overlook geo-awareness or lack adequate failure tolerance. We propose HFCS, a novel hybrid communication system that combines hierarchical and peer-to-peer elements, along with edge pools. HFCS utilizes a gossip protocol for dynamic metadata exchange.In simulation, we investigate the impact of node density and edge pool size on HFCS performance. We observe a performance improvement for clustered node distributions, aligning well with real-world scenarios. HFCS outperforms a hierarchical and a P2P approach in task fulfillment at a slight cost to failure detection.},
booktitle = {Proceedings of the IEEE/ACM 16th International Conference on Utility and Cloud Computing},
articleno = {66},
numpages = {7},
keywords = {fog computing, metadata exchange, hybrid communication},
location = {<conf-loc>, <city>Taormina (Messina)</city>, <country>Italy</country>, </conf-loc>},
series = {UCC '23}
}

@inproceedings{10.1145/3408066.3408073,
author = {Iwata, Yoritaka and Takei, Yasuhiro},
title = {Numerical Scheme based on the Spectral Method for Calculating Nonlinear Hyperbolic Evolution Equations},
year = {2020},
isbn = {9781450377034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408066.3408073},
doi = {10.1145/3408066.3408073},
abstract = {High-precision numerical scheme for nonlinear hyperbolic evolution equations is proposed based on the spectral method. The detail discretization processes are discussed in case of one-dimensional Klein-Gordon equations. In conclusion, a numerical scheme with the order of total calculation cost O(N log 2N) is proposed. As benchmark results, the relation between the numerical precision and the discretization unit size are demonstrated.},
booktitle = {Proceedings of the 12th International Conference on Computer Modeling and Simulation},
pages = {25–30},
numpages = {6},
keywords = {fourier spectral method, high-precision calculation},
location = {Brisbane, QLD, Australia},
series = {ICCMS '20}
}

@inproceedings{10.1145/3511430.3511456,
author = {Prakash, Chandan and Chittimalli, Pavan Kumar and Naik, Ravindra},
title = {Domain Specific Text Preprocessing for Open Information Extraction},
year = {2022},
isbn = {9781450396189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511430.3511456},
doi = {10.1145/3511430.3511456},
abstract = {Preprocessing is an integral part of Natural Language Processing (NLP) based applications. Standard preprocessing steps consist of removal of irrelevant, unwanted characters or parts of the text based on several observed patterns, while preserving the original intent of the text. We introduce domain-specific preprocessing to filter domain-irrelevant parts of the text while preserving the intended, semantically relevant meaning and syntactic correctness of the text. For this, we define multiple patterns using the dependency tree that represents the Natural Language text based on its dependency grammar. We applied this technique and the patterns to the United States retirement domain documents for open information extraction task as a pre-cursor for mining business product information and rules, and were able to reduce the document data aka information for analysis and mining by at least 13%, which enhanced the F1-score of relation extraction by a minimum of 16%.},
booktitle = {Proceedings of the 15th Innovations in Software Engineering Conference},
articleno = {28},
numpages = {5},
keywords = {Dependency Parser, Information Extraction, Sentence Cleaning, Text Preprocessing},
location = {<conf-loc>, <city>Gandhinagar</city>, <country>India</country>, </conf-loc>},
series = {ISEC '22}
}

@inproceedings{10.5555/3237383.3237861,
author = {Arieli, Ofer and Borg, AnneMarie and Stra\ss{}er, Christian},
title = {Prioritized Sequent-Based Argumentation},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this paper we integrate priorities in sequent-based argumentation. The former is a useful and extensively investigated tool in the context of non-monotonic reasoning, and the latter is a modular and general way of handling logical argumentation. Their combination offers a platform for representing and reasoning with maximally consistent subsets of prioritized knowledge bases. Moreover, many frameworks of the resulting formalisms satisfy common rationality postulates and other desirable properties, like conflict preservation.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1105–1113},
numpages = {9},
keywords = {dung-semantics, maximally consistent subsets, nonmonotonic logic, priorities, sequent calculi, structured argumentation},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@article{10.5555/3648699.3648711,
author = {Dolera, Emanuele and Favaro, Stefano and Peluchetti, Stefano},
title = {Learning-augmented count-min sketches via Bayesian nonparametrics},
year = {2024},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {The count-min sketch (CMS) is a time and memory efficient randomized data structure that provides estimates of tokens' frequencies in a data stream of tokens, i.e. point queries, based on random hashed data. A learning-augmented version of the CMS, referred to as CMS-DP, has been proposed by Cai, Mitzenmacher and Adams (NeurIPS 2018), and it relies on Bayesian nonparametric (BNP) modeling of the data stream of tokens via a Dirichlet process (DP) prior, with estimates of a point query being that are obtained as mean functionals of the posterior distribution of the point query, given the hashed data. While the CMS-DP has proved to improve on some aspects of CMS, it has the major drawback of arising from a "constructive" proof that builds upon arguments that are tailored to the DP prior, namely arguments that are not usable for other nonparametric priors. In this paper, we present a "Bayesian" proof of the CMS-DP that has the main advantage of building upon arguments that are usable under the popular Pitman-Yor process (PYP) prior, which generalizes the DP prior by allowing for a more exible tail behaviour, ranging from geometric tails to heavy power-law tails. This result leads to develop a novel learning-augmented CMS under power-law data streams, referred to as CMS-PYP, which relies on BNP modeling of the data stream of tokens via a PYP prior. Under this more general framework, we apply the arguments of the "Bayesian" proof of the CMS-DP, suitably adapted to the PYP prior, in order to compute the posterior distribution of a point query, given the hashed data. Applications to synthetic data and real textual data show that the CMS-PYP outperforms the CMS and the CMS-DP in estimating low-frequency tokens, which are known to be of critical interest in textual data, and it is competitive with respect to a variation of the CMS designed to deal with the estimation of low-frequency tokens. An extension of our BNP approach to more general queries, such as range queries, is also discussed.},
journal = {J. Mach. Learn. Res.},
month = {mar},
articleno = {12},
numpages = {60},
keywords = {Bayesian nonparametrics, count-min sketch, Dirichlet process prior, likelihood-free estimation, Pitman-Yor process prior, point query, power-law data stream, random hashing}
}

@inproceedings{10.1145/3229345.3229373,
author = {de Almeida Bordignon, Ana Cl\'{a}udia and Thom, Lucin\'{e}ia Heloisa and Silva, Thanner Soares and Dani, Vinicius Stein and Fantinato, Marcelo and Ferreira, Renato Cesar Borges},
title = {Natural Language Processing in Business Process Identification and Modeling: A Systematic Literature Review},
year = {2018},
isbn = {9781450365598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229345.3229373},
doi = {10.1145/3229345.3229373},
abstract = {Business Process Management (BPM) has been receiving increasing attention in recent years. Many organizations have been adapting their business to a process-centered view since they started noticing its potential to reduce costs, improve productivity and achieve higher levels of quality. However, implementing BPM in organizations requires time, making the automation of process identification and discovery highly desirable. To achieve this expectation, the application of Natural Language Processing (NLP) techniques and tools has emerged to generate process models from unstructured text. In this paper, we provide the results of a systematic literature review conducted in preparation and processing of natural language text aiming the extraction of business processes and process quality assurance. The study presents techniques applied to the BPM life-cycle phases of process identification, process discovery and process analysis as well as tools to support process discovery. This review covered papers from 2009 up to 2016 and identifies 518 articles of which 33 were selected as relevant to our work. The results of the present study may be valuable to support research in extraction of business process models from natural language text.},
booktitle = {Proceedings of the XIV Brazilian Symposium on Information Systems},
articleno = {25},
numpages = {8},
keywords = {Business Process Management, Natural Language Processing, Process Analysis, Process Discovery, Systematic Literature Review},
location = {Caxias do Sul, Brazil},
series = {SBSI '18}
}

@inproceedings{10.1145/3299771.3301648,
author = {Choppella, Venkatesh and Rastogi, Aseem},
title = {Tutorials and Technical Briefings at ISEC 2019},
year = {2019},
isbn = {9781450362153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299771.3301648},
doi = {10.1145/3299771.3301648},
abstract = {This is a short report on the Tutorials and Tech Briefings session of the 12th Innovations in Software Engineering (ISEC 2019) conference held on 14th February 2019 in Pune, India.The tutorials and tech briefings at ISEC have been popular with the participants because they offer a gentle and friendly introduction to cutting edge topics and research at the frontiers of the discipline of software engineering.This year's track attracted a total of 11 submissions (four tutorial and seven tech briefings). The five submissions that were selected reflect the current interests and directions of the field of software engineering.},
booktitle = {Proceedings of the 12th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {33},
numpages = {2},
keywords = {ACM proceedings, Software engineering},
location = {<conf-loc>, <city>Pune</city>, <country>India</country>, </conf-loc>},
series = {ISEC '19}
}

@inproceedings{10.1145/3269206.3269253,
author = {Fang, Anjie and Ounis, Iadh and MacDonald, Craig and Habel, Philip and Xiong, Xiaoyu and Yu, Hai-Tao},
title = {An Effective Approach for Modelling Time Features for Classifying Bursty Topics on Twitter},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3269253},
doi = {10.1145/3269206.3269253},
abstract = {Several previous approaches attempted to predict bursty topics on Twitter. Such approaches have usually reported that the time information (e.g. the topic popularity over time) of hashtag topics contribute the most to the prediction of bursty topics. In this paper, we propose a novel approach to use time features to predict bursty topics on Twitter. We model the popularity of topics as density curves described by the density function of a beta distribution with different parameters. We then propose various approaches to predict/classify the bursty topics by estimating the parameters of topics, using estimators such as Gradient Decent or Likelihood Maximization. In our experiments, we show that the estimated parameters of topics have a positive effect on classifying bursty topics. In particular, our estimators when combined together improve the bursty topic classification by 6.9 in terms of micro F1 compared to a baseline classifier using hashtag content features.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1547–1550},
numpages = {4},
keywords = {bursty topics, classification, social media, twitter},
location = {Torino, Italy},
series = {CIKM '18}
}

@article{10.1109/TASLP.2023.3261753,
author = {Zhang, Jianwei and Liss, Julie and Jayasuriya, Suren and Berisha, Visar},
title = {Robust Vocal Quality Feature Embeddings for Dysphonic Voice Detection},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3261753},
doi = {10.1109/TASLP.2023.3261753},
abstract = {Approximately 1.2% of the world's population has impaired voice production. As a result, automatic dysphonic voice detection has attracted considerable academic and clinical interest. However, existing methods for automated voice assessment often fail to generalize outside the training conditions or to other related applications. In this paper, we propose a deep learning framework for generating acoustic feature embeddings sensitive to vocal quality and robust across different corpora. A contrastive loss is combined with a classification loss to train our deep learning model jointly. Data warping methods are used on input voice samples to improve the robustness of our method. Empirical results demonstrate that our method not only achieves high in-corpus and cross-corpus classification accuracy but also generates good embeddings sensitive to voice quality and robust across different corpora. We also compare our results against three baseline methods on clean and three variations of deteriorated in-corpus and cross-corpus datasets and demonstrate that the proposed model consistently outperforms the baseline methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {1348–1359},
numpages = {12}
}

@article{10.1145/3632857,
author = {Popescu, Andrei},
title = {Nominal Recursors as Epi-Recursors},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {POPL},
url = {https://doi.org/10.1145/3632857},
doi = {10.1145/3632857},
abstract = {We study nominal recursors from the literature on syntax with bindings and compare them with respect to expressiveness. The term "nominal" refers to the fact that these recursors operate on a syntax representation where the names of bound variables appear explicitly, as in nominal logic. We argue that nominal recursors can be viewed as epi-recursors, a concept that captures abstractly the distinction between the constructors on which one actually recurses, and other operators and properties that further underpin recursion. We develop an abstract framework for comparing epi-recursors and instantiate it to the existing nominal recursors, and also to several recursors obtained from them by cross-pollination. The resulted expressiveness hierarchies depend on how strictly we perform this comparison, and bring insight into the relative merits of different axiomatizations of syntax. We also apply our methodology to produce an expressiveness hierarchy of nominal corecursors, which are principles for defining functions targeting infinitary non-well-founded terms (which underlie lambda-calculus semantics concepts such as B\"{o}hm trees). Our results are validated with the Isabelle/HOL theorem prover.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {15},
numpages = {32},
keywords = {epi-(co)recuror, formal reasoning, nominal logic, nominal recursion and corecursion, syntax with bindings, theorem proving}
}

@inproceedings{10.5555/3320516.3320536,
author = {Hachem, Wissam E L and De Giovanni, Pietro},
title = {Transition to alternative fuel vehicles: a distributive justice perspective},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {In this paper, we build a system dynamics model to investigate the ongoing endeavor to transition from conventional non-renewable transportation systems to renewable ones. The model focuses on light to mid duty vehicles in the private transportation sector. The literature focuses on the environmental and economic aspects of such a transition. We adopt distributive justice as a new angle, define it as access to transportation, justify its relevance by considering it a vital need for people to actualize their capabilities in society and propose a measure to quantitatively measure it in our context. There are several layers of tradeoffs in policy appraisal, yet we are able to catalyze the transition to AFV's while improving its sustainability. A policy that ensures such a harmonious behavior is one that focuses on the GHG emissions with little to no emphasis on the AFV quotas, while providing support for consumers to switch to AFV's.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {111–122},
numpages = {12},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.1145/3448139.3448184,
author = {Zhang, Yupei and An, Rui and Cui, Jiaqi and Shang, Xuequn},
title = {Undergraduate Grade Prediction in Chinese Higher Education Using Convolutional Neural Networks},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448184},
doi = {10.1145/3448139.3448184},
abstract = {Prediction of undergraduate grades before their course enrollments is beneficial to the student’s learning plan on selective courses and failure warnings to compulsory courses in Chinese higher education. This study proposed to use a deep learning-based model composed of sparse attention layers, convolutional neural layers, and a fully connected layer, called Sparse Attention Convolutional Neural Networks (SACNN), to predict undergraduate grades. Concretely, sparse attention layers response to the fact that courses have different contributions to the grade prediction of the target course; convolutional neural layers aim to capture the one-dimensional temporal feature on these courses organized in terms; the fully connected layer is to complete the final classification based on achieved features. We collected a dataset including grade records, student’s demographics and course descriptions from our institution in the past five years. The dataset contained about 54k grade records from 1307 students and 137 courses, where all mentioned methods were evaluated by the hold-out evaluation. The result shows SACNN achieves 81% prediction precision and 85% accuracy on the failure prediction, which is more effective than those compared methods. Besides, SACNN delivers a potential explanation to the reason of the predicted result, thanks to the sparse attention layer. This study provides a useful technique for personalized learning and course relationship discovery in undergraduate education.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {462–468},
numpages = {7},
keywords = {convolutional neural networks, grade prediction, personalized learning, sparse attention},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3337821.3337899,
author = {Yasugi, Masahiro and Muraoka, Daisuke and Hiraishi, Tasuku and Umatani, Seiji and Emoto, Kento},
title = {HOPE: A Parallel Execution Model Based on Hierarchical Omission},
year = {2019},
isbn = {9781450362955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3337821.3337899},
doi = {10.1145/3337821.3337899},
abstract = {This paper presents a new approach to fault-tolerant language systems without a single point of failure for irregular parallel applications. Work-stealing frameworks provide good load balancing for many parallel applications, including irregular ones written in a divide-and-conquer style. However, work-stealing frameworks with fault-tolerant features such as checkpointing do not always work well. This paper proposes a completely opposite "work omission" paradigm and its more detailed concept as a "hierarchical omission"-based parallel execution model called HOPE. HOPE programmers' task is to specify which regions in imperative code can be executed in sequential but arbitrary order and how their partial results can be accessed. HOPE workers spawn no tasks/threads at all; rather, every worker has the entire work of the program with its own planned execution order, and then the workers and the underlying message mediation systems automatically exchange partial results to omit hierarchical subcomputations. Even with fault tolerance, the HOPE framework provides parallel speedups for many parallel applications, including irregular ones.},
booktitle = {Proceedings of the 48th International Conference on Parallel Processing},
articleno = {77},
numpages = {11},
keywords = {fault tolerance, language systems, parallel execution model, work omission},
location = {Kyoto, Japan},
series = {ICPP '19}
}

@inproceedings{10.1145/3503161.3548077,
author = {Shi, Haichao and Zhang, Xiao-Yu and Li, Changsheng and Gong, Lixing and Li, Yong and Bao, Yongjun},
title = {Dynamic Graph Modeling for Weakly-Supervised Temporal Action Localization},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548077},
doi = {10.1145/3503161.3548077},
abstract = {Weakly supervised action localization is a challenging task that aims to localize action instances in untrimmed videos given only video-level supervision. Existing methods mostly distinguish action from background via attentive feature fusion with RGB and optical flow modalities. Unfortunately, this strategy fails to retain the distinct characteristics of each modality, leading to inaccurate localization under hard-to-discriminate cases such as action-context interference and in-action stationary period. As an action is typically comprised of multiple stages, an intuitive solution is to model the relation between the finer-grained action segments to obtain a more detailed analysis. In this paper, we propose a dynamic graph-based method, namely DGCNN, to explore the two-stream relation between action segments. To be specific, segments within a video which are likely to be actions are dynamically selected to construct an action graph. For each graph, a triplet adjacency matrix is devised to explore the temporal and contextual correlations between the pseudo action segments, which consists of three components, i.e., mutual importance, feature similarity, and high-level contextual similarity. The two-stream dynamic pseudo graphs, along with the pseudo background segments, are used to derive more detailed video representation. For action localization, a non-local based temporal refinement module is proposed to fully leverage the temporal consistency between consecutive segments. Experimental results on three datasets, i.e., THUMOS14, ActivityNet v1.2 and v1.3, demonstrate that our method is superior to the state-of-the-arts.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {3820–3828},
numpages = {9},
keywords = {dynamic graph modeling, pseudo action generation, temporal action localization, weakly supervised learning},
location = {<conf-loc>, <city>Lisboa</city>, <country>Portugal</country>, </conf-loc>},
series = {MM '22}
}

@inproceedings{10.5555/3237383.3237860,
author = {Borg, AnneMarie and Arieli, Ofer},
title = {Hypersequential Argumentation Frameworks: An Instantiation in the Modal Logic S5},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this paper we introduce hypersequent-based frameworks for the modeling of defeasible reasoning by means of logic-based argumentation. These frameworks are an extension of sequent-based argumentation frameworks, in which arguments are represented not only by sequents, but by more general expressions, called hypersequents . This generalization allows to incorporate, as the deductive-base of our formalism, some well-studied logics like the modal logic S5, the relavent logic RM, and G\"{o}del-Dummett logic LC, to which no cut-free sequent calculi are known. In this paper we take S5 as the core logic and show that the hypersequent-based argumentation frameworks that are obtained in this case yield a robust defeasible variant of S5 with several desirable properties.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1097–1104},
numpages = {8},
keywords = {dung-style semantics, hypersequent calculi, modal logic, nonmonotonic logic, sequent calculi, structured argumentation},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1145/3299874.3318001,
author = {Alam, Mahabubul and Ghosh, Swaroop and Hosur, Sujay S.},
title = {TOIC: Timing Obfuscated Integrated Circuits},
year = {2019},
isbn = {9781450362528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299874.3318001},
doi = {10.1145/3299874.3318001},
abstract = {To counter the threats of reverse engineering (RE) and Trojan in-sertion, researchers have considered gate-level obfuscation in inte-grated circuits (IC) as a viable solution. However, several techniques are present in the literature to crack the obfuscation with varying degree of success raising the concern about their secrecy. In this article, we have presented TOIC (Timing Obfuscated Integrated Circuits), a novel technique where sequential elements are obfuscated to hide the true timing paths in the design. TOIC can act as a standalone countermeasure against IC reverse engineering or can be incorporated with existing gate camouflaging techniques to maximize adversarial RE effort. Previous research has shown that limiting access to internal nodes can improve the adversarial RE effort at the cost of poor testability. TOIC can impose prohibitively large decamouflaging time complexity by limiting the controllability and observability over the internal nodes in an IC while preserving complete testability.},
booktitle = {Proceedings of the 2019 on Great Lakes Symposium on VLSI},
pages = {105–110},
numpages = {6},
keywords = {camouflaged flip-flop, reverse engineering, sat, threshold-defined switch, timing obfuscation},
location = {Tysons Corner, VA, USA},
series = {GLSVLSI '19}
}

@inproceedings{10.1145/3368691.3368709,
author = {Alsaadi, Mohmood and Lisitsa, Alexei and Qasaimeh, Malik},
title = {Minimizing the ambiguities in medical devices regulations based on software requirement engineering techniques},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3368709},
doi = {10.1145/3368691.3368709},
abstract = {Trusted medical software devices, must comply with one of the healthcare regulations such as Food and Drug Administration (FDA), EU Medical Device Regulation (MDR), and Health Insurance Portability and Accountability Act (HIPAA). Since these regulations are enacted by law legislators and written in a legal text, these regulations are typically written with ambiguities. However, people have a different interpretation for the legal text for example, software developers faced challenges in identifying and understanding the regulatory requirements that are related to the software development process. Moreover, ambiguous in regulatory requirements play a big role in software compliance with regulatory particularly, when the requirements are legal text.In this paper, we intend to minimize the ambiguities in EU MDR requirements based on requirement engineering techniques in order to make MDR requirements clear and precise for software developers. In our previous work, we extracted the requirements of MDR that would affect the software development life cycle (SDLC) directly or indirectly. Herein, we will identify the ambiguity types in the extracted requirements of MDR. Moreover, this paper will present a method to minimize the ambiguities in MDR requirements based on requirement engineering techniques (user story and use case diagram). Finally, this work will be evaluated by the critical-to-quality tree measurement technique.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {18},
numpages = {5},
keywords = {CTQ tree, MDR software requirements, ambiguity of MD requirements, medical devices regulations, requirement engineering techniques},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00024,
author = {Rajbhoj, Asha and Nistala, Padmalata and Kulkarni, Vinay and Soni, Shivani and Pathan, Ajim},
title = {DocToModel: Automated Authoring of Models from Diverse Requirements Specification Documents},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00024},
doi = {10.1109/ICSE-SEIP58684.2023.00024},
abstract = {Early stages of Software Development Life Cycle (SDLC) namely requirement elicitation and requirements analysis have remained document-centric in the industry for market-driven, complex, large-scale business applications and products. The documentation typically runs into hundreds of Natural Language (NL) text documents which requirements engineers need to sift looking for the relevant information and also maintain these documents in-sync over time - a time-consuming and error-prone activity. Much of this difficulty can be overcome if the information is available in a structured form that is amenable to automated processing. Purposive models offer a way out. However, for easy adoption by industry practitioners, these models must be populated from NL text documents in a largely automated manner. This task is characterized by high variability with several documents containing different information conforming to different structures and styles. As a result, purposive information extractors need to be developed for each project/ product. Moreover, being an open-ended space there is no upper bound on the information extractors that need to be developed. To overcome this difficulty, we propose a document structure agnostic and meta-model agnostic tool, DocToModel, for the automated authoring of models from NL text documents. It provides a pattern mapping language to specify a mapping of structured and unstructured document information to meta-model elements, and a pattern interpreter to automate model authoring. The configurable and extensible architecture of DocToModel makes it generic and amenable to easy repurposing for other NL documents. This paper, describes the approach and illustrates its utility and efficacy on multiple real-world case studies.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {199–210},
numpages = {12},
keywords = {meta-model, automated model authoring, model extraction, document parser, NLP, meta-model pattern, pattern interpreter},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@proceedings{10.1145/3640115,
title = {ICITEE '23: Proceedings of the 6th International Conference on Information Technologies and Electrical Engineering},
year = {2023},
isbn = {9798400708299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {<conf-loc>, <city>Changde, Hunan</city>, <country>China</country>, </conf-loc>}
}

@inproceedings{10.1145/3344948.3344956,
author = {Schr\"{o}der, Sandra and Buchgeher, Georg},
title = {Discovering architectural rules in practice},
year = {2019},
isbn = {9781450371421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3344948.3344956},
doi = {10.1145/3344948.3344956},
abstract = {Architecture conformance checking is an important technique to verify whether a software system's implementation adheres to predefined architectural rules. The adherence to architectural rules is crucial, since architectural rules are intended to ensure that fundamental properties are correctly implemented in the source code. A lot of tools have been proposed and developed for automatic conformance checking. They are also successfully applied in industry. However, those tools mainly focus on the validation of static dependencies between modules or on ensuring the adherence to the layered architecture. In this poster, we present an industrial case study which discovers architectural rules from three industrial projects. This study shows that software architects also define other types of architectural rules that are not yet covered by state-of-the-art conformance checking tools. The architectural rules found in the projects are categorized into architectural rule categories. We use four representative tools as examples and show that the tools are able to formalize only a specific extent of the discovered rules. The results of the study motivate for improving existing tools and to collect further examples of architectural rules that can potentially be reused in projects with similar characteristics.},
booktitle = {Proceedings of the 13th European Conference on Software Architecture - Volume 2},
pages = {10–13},
numpages = {4},
keywords = {architectural rules, conformance checking, empirical study},
location = {Paris, France},
series = {ECSA '19}
}

@inproceedings{10.1145/3354166.3354186,
author = {van Bakel, Steffen},
title = {Exception Handling and Classical Logic},
year = {2019},
isbn = {9781450372497},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3354166.3354186},
doi = {10.1145/3354166.3354186},
abstract = {We present λtry, an extension of the λ-calculus with named exception handling, via try, throw and catch, and present a basic notion of type assignment expressing recoverable exception handling and show that it is sound. We define an interpretation for λtry to Parigot's λμ-calculus, and show that reduction (both lazy and call by value) is preserved by the interpretation. We will show that also types assignable in the basic system are preserved by the interpretation.We will then add a notion of total failure through halt that escapes applicative contexts without being caught by a handler, and show that we can interpret this in λμ when adding top as destination. We will argue that introducing handlers for halt will break the relation with λμ.We will conclude the paper by showing that it is possible to add handlers for program failure by introducing panic and dedicated handlers to λtry. We will need to extend the language with a conditional construct that is typed in a non-traditional way, that cannot be expressed in λμ or logic. This will allow both recoverable exceptions and total failure, dealt with by handlers; we will show a non-standard soundness result for this system.},
booktitle = {Proceedings of the 21st International Symposium on Principles and Practice of Declarative Programming},
articleno = {21},
numpages = {14},
keywords = {abort, classical logic, exception handling, lambda calculus},
location = {Porto, Portugal},
series = {PPDP '19}
}

@inproceedings{10.5555/3400397.3400466,
author = {Deo, Anand and Juneja, Sandeep},
title = {Limiting distributional fixed points in systemic risk graph models},
year = {2020},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {We analyse the equilibrium behaviour of a large network of banks in presence of incomplete information, where inter-bank borrowing and lending is allowed, and banks suffer shocks to assets. In a two time period graphical model, we show that the equilibrium wealth distribution is the unique fixed point of a complex, high dimensional distribution-valued map. Fortunately, there is a dimension collapse in the limit as the network size increases, where the equilibriated system converges to the unique fixed point involving a simple, one dimensional distribution-valued operator, which, we show, is amenable to simulation. Specifically, we develop a Monte-Carlo algorithm that computes the fixed point of a general distribution-valued map and derive sample complexity guarantees for it. We numerically show that this limiting one-dimensional regime can be used to obtain useful structural insights and approximations for networks with as low as a few hundred banks.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {878–889},
numpages = {12},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.5555/3643142.3643337,
author = {Lee, Kwanwoo and Jeon, Woosung and Park, Sangchul},
title = {Multi-Resolution Modeling Method for Automated Material Handling Systems in Semiconductor Fabs},
year = {2024},
isbn = {9798350369663},
publisher = {IEEE Press},
abstract = {This paper presents a novel modeling framework for semiconductor fabrication facilities (FABs) that integrates production and material handling systems. Because the productivity of semiconductor FABs is significantly influenced by their material-handling systems, existing research has focused on optimizing operational logic considering both aspects. However, the scale and complexity of modern FABs make implementation of fully integrated models challenging, resulting in slow simulation speeds for long periods. To address this issue, we propose a multi-resolution modeling framework that creates material-handling system models at two distinct resolution levels, enabling fast, fully integrated FAB models while accounting for material-handling effects. Experimental results demonstrated accelerated simulation completion compared to single-resolution models while maintaining consistent results. The proposed method provides a practical approach for semiconductor FABs to investigate long-term phenomena and urgent decision-making problems while considering both production and material-handling systems.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2345–2356},
numpages = {12},
location = {<conf-loc>, <city>San Antonio</city>, <state>Texas</state>, <country>USA</country>, </conf-loc>},
series = {WSC '23}
}

@article{10.1145/3183628.3183630,
author = {Filipovikj, Predrag and Rodriguez-Navas, Guillermo and Nyberg, Mattias and Seceleanu, Cristina},
title = {Automated SMT-based consistency checking of industrial critical requirements},
year = {2018},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1559-6915},
url = {https://doi.org/10.1145/3183628.3183630},
doi = {10.1145/3183628.3183630},
abstract = {With the ever-increasing size, complexity and intricacy of system requirements specifications, it becomes difficult to ensure their correctness with respect to certain criteria such as consistency. Automated formal techniques for consistency checking of requirements, mostly by means of model checking, have been proposed in academia. Sometimes such techniques incur a high modeling cost or analysis time, or are not applicable. To address such problems, in this paper we propose an automated consistency analysis technique of requirements that are formalized based on patterns, and checked using state-of-the-art Satisfiability Modulo Theories solvers. Our method assumes several transformation steps, from textual requirements to formal logic, and next into the format suited for the SMT tool. To automate such steps, we propose a tool, called PROPAS, that does not require any user intervention during the transformation and analysis phases, thus making the consistency analysis usable by non-expert practitioners. For validation, we apply our method on a set of timed computation tree logic requirements of an industrial automotive system called the Fuel Level Display.},
journal = {SIGAPP Appl. Comput. Rev.},
month = {jan},
pages = {15–28},
numpages = {14},
keywords = {SMT, Z3, formal methods, requirements consistency analysis}
}

@inproceedings{10.1145/3550356.3561542,
author = {Cabot, Jordi and Delgado, David and Burgue\~{n}o, Lola},
title = {Combining OCL and natural language: a call for a community effort},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561542},
doi = {10.1145/3550356.3561542},
abstract = {The growing popularity and availability of pretrained natural language models opens the door to many interesting applications combining natural language (NL) with software artefacts. A couple of examples are the generation of code excerpts from NL instructions or the verbalization of programs in NL to facilitate their comprehension.Many of these language models have been trained with open source software datasets and therefore "understand" a variety of programming languages, but not OCL.We argue that OCL needs to jump into the machine learning bandwagon or it will risk losing its appeal as a constraint specification language. For that, the key first task is to create together an OCL corpus dataset amenable for natural language processing.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {908–912},
numpages = {5},
keywords = {OCL, community, corpus, dataset, natural language},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3569902.3569953,
author = {Pereira, Jos\'{e} D'Abruzzo and Ribeiro, Jo\~{a}o David and Pires, Jo\~{a}o and Moita, Pedro and Laranjeiro, Nuno and Vieira, Marco},
title = {On the use of the TMA Framework to promote self-adaptation capabilities in TalkConnect},
year = {2023},
isbn = {9781450397377},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569902.3569953},
doi = {10.1145/3569902.3569953},
abstract = {Cloud applications are usually deployed in a third-party infrastructure, which follows a pay-per-use model. This is especially important to companies (such as Talkdesk) that have flexibility to adjust their infrastructure without needing to buy new hardware. However, the resources acquired should be adjusted to the demand at any given moment, on one hand to assure the right quality of service (i.e., resources should not be less than what is needed) and in the other hand to reduce costs (i.e., resources should not be more than what is required). TMA (Trustworthiness Monitoring &amp; Assessment Framework) is a solution developed at the University of Coimbra that promotes self-adaptation capabilities to cloud applications. This paper presents a case on the use of TMA in TalkConnect, showcasing a collaboration between academia and industry. TalkConnect is a solution developed by Talkdesk to promote global interconnection to multiple telecommunication infrastructures.},
booktitle = {Proceedings of the 11th Latin-American Symposium on Dependable Computing},
pages = {89–90},
numpages = {2},
keywords = {Cloud Applications, Microservices, Self-adaptive Systems},
location = {<conf-loc>, <city>Fortaleza/CE</city>, <country>Brazil</country>, </conf-loc>},
series = {LADC '22}
}

@inproceedings{10.1145/3297663.3309674,
author = {Schulz, Henning and Okanovi\'{c}, Du\v{s}an and van Hoorn, Andr\'{e} and Ferme, Vincenzo and Pautasso, Cesare},
title = {Behavior-driven Load Testing Using Contextual Knowledge - Approach and Experiences},
year = {2019},
isbn = {9781450362399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297663.3309674},
doi = {10.1145/3297663.3309674},
abstract = {Load testing is widely considered a meaningful technique for performance quality assurance. However, empirical studies reveal that in practice, load testing is not applied systematically, due to the sound expert knowledge required to specify, implement, and execute load tests.Our Behavior-driven Load Testing (BDLT) approach eases load test specification and execution for users with no or little expert knowledge. It allows a user to describe a load test in a template-based natural language and to rely on an automated framework to execute the test. Utilizing the system's contextual knowledge such as workload-influencing events, the framework automatically determines the workload and test configuration. We investigated the applicability of our approach in an industrial case study, where we were able to express four load test concerns using BDLT and received positive feedback from our industrial partner. They understood the BDLT definitions well and proposed further applications, such as the usage for software quality acceptance criteria.},
booktitle = {Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {265–272},
numpages = {8},
keywords = {behavior-driven testing, declarative performance engineering, load testing},
location = {Mumbai, India},
series = {ICPE '19}
}

@inproceedings{10.1145/3489525.3511688,
author = {Luo, Lixiang and Chung, I-Hsin and Seelam, Seetharami and Chen, Ming-hung and Soh, Yun Joon},
title = {NVMe Virtualization for Cloud Virtual Machines},
year = {2022},
isbn = {9781450391436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489525.3511688},
doi = {10.1145/3489525.3511688},
abstract = {Public clouds are rapidly moving to support Non-Volatile Memory Express (NVMe) based storage to meet the ever-increasing I/O throughput and latency demands of modern workloads. They provide NVMe storage through virtual machines (VMs) where multiple VMs running on a host may share a physical NVMe device. The virtualization method used to share the NVMe capability has important performance, usability and security implications. In this paper, we propose three NVMe storage virtualization methods: PCI device passthrough, virtual block device method, and Storage Performance Development Kit (SPDK) virtual host target method. We evaluate these virtualization methods in terms of performance, scalability, CPU overhead, technology maturity, security, and availability to use one or more of these methods in IBM public cloud.},
booktitle = {Proceedings of the 2022 ACM/SPEC on International Conference on Performance Engineering},
pages = {37–46},
numpages = {10},
keywords = {data privacy, kvm, nvme namespaces, qemu, virtio},
location = {Beijing, China},
series = {ICPE '22}
}

@inproceedings{10.1145/3288155.3288199,
author = {Wu, Yongzhong and Chen, Xiangying and Ma, Jingwen},
title = {Modeling Passengers' Choice in Ride-Hailing Service with Dedicated-Ride Option and Ride-Sharing Option},
year = {2018},
isbn = {9781450365574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3288155.3288199},
doi = {10.1145/3288155.3288199},
abstract = {With the popularity of mobile application and mobile payment, online ride-hailing business developed most rapidly in recent years. Most ride-hailing companies provide both dedicated-ride option (with one passenger in the car) and ride-sharing option (with more than one passenger for different destinations) for passengers. The passengers' choice among different options is of great interests to the ride-hailing operators for improving their business. In this paper, logit-based choice models are established to estimate the passengers' choice among taxi, dedicated ride, and ride-sharing. Survey data in China is collected for model calibration. The models reveal how the key factors, e.g., traveling distance, waiting time, and pricing affect passengers' choice. It is also interesting to find that the pricing of ride-sharing option is vital for profit maximization.},
booktitle = {Proceedings of the 4th International Conference on Industrial and Business Engineering},
pages = {94–98},
numpages = {5},
keywords = {Choice Model, Logit Models, Ride-hailing, Ride-sharing},
location = {<conf-loc>, <city>Macau</city>, <country>Macao</country>, </conf-loc>},
series = {ICIBE '18}
}

@inproceedings{10.1145/3268866.3268880,
author = {Du, Bowen and Guo, Xiaoxia and Chen, Yangyang},
title = {Research on Face Recognition System based on Embedded Processor and Deep Neural Network},
year = {2018},
isbn = {9781450365246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3268866.3268880},
doi = {10.1145/3268866.3268880},
abstract = {In view of the face recognition system implemented on the traditional computer, the face recognition technology is combined with the embedded system, which is not easy to carry and work inefficiency. The current mainstream embedded systems have the advantages of high chip integration, minimization of hardware and software, high automation, concurrent processing, real-time response, and stability and reliability. This system can not only play the advantages of biometric identification, but also make full use of the characteristics of the embedded system with small body volume, low cost and stable reliability. It is the development trend of face recognition system. In view of the above two points, the face recognition system based on embedded processor is deeply researched, and a more accurate recognition result is obtained.},
booktitle = {Proceedings of the 2018 International Conference on Artificial Intelligence and Pattern Recognition},
pages = {11–14},
numpages = {4},
keywords = {Component, Convolutional neural network, Face recognition embedded system},
location = {Beijing, China},
series = {AIPR '18}
}

@inproceedings{10.1145/3323679.3326509,
author = {Kamran, Khashayar and Yeh, Edmund and Ma, Qian},
title = {DECO: Joint Computation, Caching and Forwarding in Data-Centric Computing Networks},
year = {2019},
isbn = {9781450367646},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323679.3326509},
doi = {10.1145/3323679.3326509},
abstract = {The emergence of IoT devices and the predicted increase in the number of data-driven and delay-sensitive applications highlight the importance of dispersed computing platforms (e.g. edge computing and fog computing) that can intelligently manage in-network computation and data placement. In this paper, we propose the DECO (Data-cEntric COmputation) framework for joint computation, caching, and request forwarding in data-centric computing networks. DECO utilizes a virtual control plane which operates on the demand rates for computation and data, and an actual plane which handles computation requests, data requests, data objects and computation results in the physical network. We present a throughput optimal policy within the virtual plane, and use it as a basis for adaptive and distributed computation, caching, and request forwarding in the actual plane. We demonstrate the superior performance of the DECO policy in terms of request satisfaction delay as compared with several baseline policies, through extensive numerical simulations over multiple network topologies.},
booktitle = {Proceedings of the Twentieth ACM International Symposium on Mobile Ad Hoc Networking and Computing},
pages = {111–120},
numpages = {10},
keywords = {Distributed computing networks, caching, data-centric computing, data-intensive computing, fog computing, mobile edge computing},
location = {Catania, Italy},
series = {Mobihoc '19}
}

@inproceedings{10.1145/3552327.3552330,
author = {Launay, Marina and Ruellan, Marie and Barcellini, Flore},
title = {Designing new energy management technologies and systems: understanding Energy Management Activities (EMAs) collective and temporal dimensions in French social housing},
year = {2022},
isbn = {9781450398084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3552327.3552330},
doi = {10.1145/3552327.3552330},
abstract = {Ecological and energy issues are pushing for action in the housing sector which is a major greenhouse gases emitter. Most solutions considered by public stakeholders have a significant technical dimension. However the existing energy management technologies have limited effectiveness and are difficult for many dwellers to use [11]. We propose new reflections concerning the design of resources systems integrating energy management technologies in social housing. These reflections are based on the understanding of Energy Management Activities (EMAs) complexity. More precisely we focus on EMAs collective and temporal dimensions and their implications on tasks sharing between different stakeholders (dwellers, lessor, third parties) and between these and technologies. Our work is grounded in Activity-Centered Ergonomics in collaboration with electrical engineering and intends to be built with actors concerned by energetical issues in a participative approach. We present our first research results: a mapping of strategic actors in social housing and energy to be involved in our research and first understanding on EMAs collective and temporal dimensions.},
booktitle = {Proceedings of the 33rd European Conference on Cognitive Ergonomics},
articleno = {11},
numpages = {4},
keywords = {Activity-Centered Ergonomics, Energy Management Activities (EMAs), Energy management technologies, French social housing},
location = {<conf-loc>, <city>Kaiserslautern</city>, <country>Germany</country>, </conf-loc>},
series = {ECCE '22}
}

@article{10.5555/3586589.3586910,
author = {Hamdi, Nima and Bayati, Mohsen},
title = {On low-rank trace regression under general sampling distribution},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {In this paper, we study the trace regression when a matrix of parameters B★ is estimated via the convex relaxation of a rank-regularized regression or via regularized nonconvex optimization. It is known that these estimators satisfy near-optimal error bounds under assumptions on the rank, coherence, and spikiness of B★. We start by introducing a general notion of spikiness for B★ that provides a generic recipe to prove the restricted strong convexity of the sampling operator of the trace regression and obtain near-optimal and non-asymptotic error bounds for the estimation error. Similar to the existing literature, these results require the regularization parameter to be above a certain theory-inspired threshold that depends on observation noise that may be unknown in practice. Next, we extend the error bounds to cases where the regularization parameter is chosen via cross-validation. This result is significant in that existing theoretical results on cross-validated estimators (Kale et al., 2011; Kumar et al., 2013; Abou-Moustafa and Szepesvari, 2017) do not apply to our setting since the estimators we study are not known to satisfy their required notion of stability. Finally, using simulations on synthetic and real data, we show that the cross-validated estimator selects a near-optimal penalty parameter and outperforms the theory-inspired approach of selecting the parameter.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {321},
numpages = {49},
keywords = {matrix completion, multi-task learning, compressed sensing, low-rank matrices, cross-validation}
}

@inproceedings{10.5555/3433701.3433783,
author = {Slaughter, Elliott and Wu, Wei and Fu, Yuankun and Brandenburg, Legend and Garcia, Nicolai and Kautz, Wilhem and Marx, Emily and Morris, Kaleb S. and Cao, Qinglei and Bosilca, George and Mirchandaney, Seema and Lee, Wonchan and Treichler, Sean and McCormick, Patrick and Aiken, Alex},
title = {Task bench: a parameterized benchmark for evaluating parallel runtime performance},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {We present Task Bench, a parameterized benchmark designed to explore the performance of distributed programming systems under a variety of application scenarios. Task Bench dramatically lowers the barrier to benchmarking and comparing multiple programming systems by making the implementation for a given system orthogonal to the benchmarks themselves: every benchmark constructed with Task Bench runs on every Task Bench implementation. Furthermore, Task Bench's parameterization enables a wide variety of benchmark scenarios that distill the key characteristics of larger applications.To assess the effectiveness and overheads of the tested systems, we introduce a novel metric, minimum effective task granularity (METG). We conduct a comprehensive study with 15 programming systems on up to 256 Haswell nodes of the Cori supercomputer. Running at scale, 100μs-long tasks are the finest granularity that any system runs efficiently with current technologies. We also study each system's scalability, ability to hide communication and mitigate load imbalance.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {62},
numpages = {15},
location = {Atlanta, Georgia},
series = {SC '20}
}

@article{10.1145/3552490.3552494,
author = {Dave, Dev and Celestino, Angelica and Varde, Aparna S. and Anu, Vaibhav},
title = {Management of Implicit Requirements Data in Large SRS Documents: Taxonomy and Techniques},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/3552490.3552494},
doi = {10.1145/3552490.3552494},
abstract = {Implicit Requirements (IMR) identification is part of the Requirements Engineering (RE) phase in Software Engineering during which data is gathered to create SRS (Software Requirements Specifications) documents. As opposed to explicit requirements clearly stated, IMRs constitute subtle data and need to be inferred. Research has shown that IMRs are crucial to the success of software development. Many software systems can encounter failures due to lack of IMR data management. SRS documents are large, often hundreds of pages, due to which manually identifying IMRs by human software engineers is not feasible. Moreover, such data is evergrowing due to the expansion of software systems. It is thus important to address the crucial issue of IMR data management. This article presents a survey on IMRs in SRS documents with the definition and overview of IMR data, detailed taxonomy of IMRs with explanation and examples, practices in managing IMR data, and tools for IMR identification. In addition to reviewing classical and state-of-the-art approaches, we highlight trends and challenges and point out open issues for future research. This survey article is interesting based on data quality, hidden information retrieval, veracity and salience, and knowledge discovery from large textual documents with complex heterogeneous data.},
journal = {SIGMOD Rec.},
month = {jul},
pages = {18–29},
numpages = {12}
}

@article{10.1109/TNET.2021.3136157,
author = {Kamran, Khashayar and Yeh, Edmund and Ma, Qian},
title = {DECO: Joint Computation Scheduling, Caching, and Communication in Data-Intensive Computing Networks},
year = {2022},
issue_date = {June 2022},
publisher = {IEEE Press},
volume = {30},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3136157},
doi = {10.1109/TNET.2021.3136157},
abstract = {Driven by technologies such as IoT-enabled health care, machine learning applications at the edge, and industrial automation, mobile edge and fog computing paradigms have reinforced a general trend toward decentralized computing, where any network node can route traffic, compute tasks, and store data, possibly at the same time. In many such computing environments, there is a need to cache significant amounts of data, which may include large data sets, machine learning models, or executable code. In this work, we propose a framework for joint computation scheduling, caching, and request forwarding within such decentralized computing environments. We first characterize the stability region of a “genie-aided” computing network where data required by computation are instantly accessible, and develop a throughput optimal control policy for this model. Based on this, we develop a practically implementable distributed and adaptive algorithm, and show that it exhibits superior performance in terms of average task completion time, when compared to several baseline policies.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {1058–1072},
numpages = {15}
}

@inproceedings{10.1145/3229147.3229166,
author = {Giunchi, Daniele and James, Stuart and Steed, Anthony},
title = {3D sketching for interactive model retrieval in virtual reality},
year = {2018},
isbn = {9781450358927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229147.3229166},
doi = {10.1145/3229147.3229166},
abstract = {We describe a novel method for searching 3D model collections using free-form sketches within a virtual environment as queries. As opposed to traditional sketch retrieval, our queries are drawn directly onto an example model. Using immersive virtual reality the user can express their query through a sketch that demonstrates the desired structure, color and texture. Unlike previous sketch-based retrieval methods, users remain immersed within the environment without relying on textual queries or 2D projections which can disconnect the user from the environment. We perform a test using queries over several descriptors, evaluating the precision in order to select the most accurate one. We show how a convolutional neural network (CNN) can create multi-view representations of colored 3D sketches. Using such a descriptor representation, our system is able to rapidly retrieve models and in this way, we provide the user with an interactive method of navigating large object datasets. Through a user study we demonstrate that by using our VR 3D model retrieval system, users can perform search more quickly and intuitively than with a naive linear browsing method. Using our system users can rapidly populate a virtual environment with specific models from a very large database, and thus the technique has the potential to be broadly applicable in immersive editing systems.},
booktitle = {Proceedings of the Joint Symposium on Computational Aesthetics and Sketch-Based Interfaces and Modeling and Non-Photorealistic Animation and Rendering},
articleno = {1},
numpages = {12},
keywords = {CNN, HCI, sketch, virtual reality},
location = {Victoria, British Columbia, Canada},
series = {Expressive '18}
}

@inproceedings{10.5555/3437539.3437554,
author = {Chen, Pei-Wei and Huang, Yu-Ching and Lee, Cheng-Lin and Jiang, Jie-Hong Roland},
title = {Circuit learning for logic regression on high dimensional boolean space},
year = {2020},
isbn = {9781450367257},
publisher = {IEEE Press},
abstract = {Logic regression aims to find a Boolean model involving binary covariates that predicts the response of an unknown system. It has many important applications, e.g., in data analysis and system design. In the 2019 ICCAD CAD Contest, the challenge of learning a compact circuit representing a black-box input-output pattern generator in a high dimensional Boolean space is formulated as the logic regression problem. This paper presents our winning approach to the problem based on a decision-tree reasoning procedure assisted with a template based preprocessing. Our methods outperformed other contestants in the competition in both prediction accuracy and circuit size.},
booktitle = {Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference},
articleno = {15},
numpages = {6},
location = {Virtual Event, USA},
series = {DAC '20}
}

@article{10.1109/TCBB.2021.3066086,
author = {Mondol, Raktim Kumar and Truong, Nhan Duy and Reza, Mohammad and Ippolito, Samuel and Ebrahimie, Esmaeil and Kavehei, Omid},
title = {AFExNet: An Adversarial Autoencoder for Differentiating Breast Cancer Sub-Types and Extracting Biologically Relevant Genes},
year = {2021},
issue_date = {July-Aug. 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3066086},
doi = {10.1109/TCBB.2021.3066086},
abstract = {Technological advancements in high-throughput genomics enable the generation of complex and large data sets that can be used for classification, clustering, and bio-marker identification. Modern deep learning algorithms provide us with the opportunity of finding most significant features in such huge dataset to characterize diseases (e.g., cancer) and their sub-types. Thus, developing such deep learning method, which can successfully extract meaningful features from various breast cancer sub-types, is of current research interest. In this paper, we develop dual stage (unsupervised pre-training and supervised fine-tuning) neural network architecture termed AFExNet based on adversarial auto-encoder (AAE) to extract features from high dimensional genetic data. We evaluated the performance of our model through twelve different supervised classifiers to verify the usefulness of the new features using public RNA-Seq dataset of breast cancer. AFExNet provides consistent results in all performance metrics across twelve different classifiers which makes our model classifier independent. We also develop a method named ‘TopGene’ to find highly weighted genes from the latent space which could be useful for finding cancer bio-markers. Put together, AFExNet has great potential for biological data to accurately and effectively extract features. Our work is fully reproducible and source code can be downloaded from Github: &lt;uri&gt;https://github.com/NeuroSyd/breast-cancer-sub-types&lt;/uri&gt;.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {mar},
pages = {2060–2070},
numpages = {11}
}

@article{10.14778/3503585.3503599,
author = {Duong, Chi Thang and Hoang, Trung Dung and Yin, Hongzhi and Weidlich, Matthias and Nguyen, Quoc Viet Hung and Aberer, Karl},
title = {Scalable robust graph embedding with Spark},
year = {2021},
issue_date = {December 2021},
publisher = {VLDB Endowment},
volume = {15},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3503585.3503599},
doi = {10.14778/3503585.3503599},
abstract = {Graph embedding aims at learning a vector-based representation of vertices that incorporates the structure of the graph. This representation then enables inference of graph properties. Existing graph embedding techniques, however, do not scale well to large graphs. While several techniques to scale graph embedding using compute clusters have been proposed, they require continuous communication between the compute nodes and cannot handle node failure. We therefore propose a framework for scalable and robust graph embedding based on the MapReduce model, which can distribute any existing embedding technique. Our method splits a graph into subgraphs to learn their embeddings in isolation and subsequently reconciles the embedding spaces derived for the subgraphs. We realize this idea through a novel distributed graph decomposition algorithm. In addition, we show how to implement our framework in Spark to enable efficient learning of effective embeddings. Experimental results illustrate that our approach scales well, while largely maintaining the embedding quality.},
journal = {Proc. VLDB Endow.},
month = {dec},
pages = {914–922},
numpages = {9}
}

@inproceedings{10.1145/3366030.3366079,
author = {Sampaio, Vanderson S. de O. L. and Fileto, Renato and de Macedo, Douglas D. J.},
title = {A Method to Estimate Entity Performance from Mentions to Related Entities in Texts on the Web},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366079},
doi = {10.1145/3366030.3366079},
abstract = {Publications on the Web can influence the public opinion about certain entities (e.g., politicians, institutions). At the same time, a variety of indicators can be extracted from these publications and used to estimate entity performance (e.g., popularity, votes share). This work proposes an automatic method that employs state-of-the-art natural language processing tools to extract indicators about entities mentioned in texts, for estimating the performance of these entities or semantically related ones. Our method calculates performance metrics from performance indicators consolidated for semantically related entities, assess correlations of these consolidated metrics with ground true performance, and uses these metrics to predict certain fluctuations in entity performance. Experimental results in a case study on politics show that consolidated metrics for several interrelated entities are better correlated to observed real performance measures of some target entities and lead to better predictions, than metrics for just one entity.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {267–276},
numpages = {10},
keywords = {Entity performance correlation, entity performance prediction, semantic relatedness},
location = {Munich, Germany},
series = {iiWAS2019}
}

@article{10.1145/3386090,
author = {Liu, Hui and Wang, Haiou and Wu, Yan and Xing, Lei},
title = {Superpixel Region Merging Based on Deep Network for Medical Image Segmentation},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3386090},
doi = {10.1145/3386090},
abstract = {Automatic and accurate semantic segmentation of pathological structures in medical images is challenging because of noisy disturbance, deformable shapes of pathology, and low contrast between soft tissues. Classical superpixel-based classification algorithms suffer from edge leakage due to complexity and heterogeneity inherent in medical images. Therefore, we propose a deep U-Net with superpixel region merging processing incorporated for edge enhancement to facilitate and optimize segmentation. Our approach combines three innovations: (1) different from deep learning--based image segmentation, the segmentation evolved from superpixel region merging via U-Net training getting rich semantic information, in addition to gray similarity; (2) a bilateral filtering module was adopted at the beginning of the network to eliminate external noise and enhance soft tissue contrast at edges of pathogy; and (3) a normalization layer was inserted after the convolutional layer at each feature scale, to prevent overfitting and increase the sensitivity to model parameters. This model was validated on lung CT, brain MR, and coronary CT datasets, respectively. Different superpixel methods and cross validation show the effectiveness of this architecture. The hyperparameter settings were empirically explored to achieve a good trade-off between the performance and efficiency, where a four-layer network achieves the best result in precision, recall, F-measure, and running speed. It was demonstrated that our method outperformed state-of-the-art networks, including FCN-16s, SegNet, PSPNet, DeepLabv3, and traditional U-Net, both quantitatively and qualitatively. Source code for the complete method is available at https://github.com/Leahnawho/Superpixel-network.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {may},
articleno = {39},
numpages = {22},
keywords = {Medical image segmentation, bilateral filtering, deep U-Net, normalization layer, superpixel-based classification algorithm}
}

@article{10.1145/3519311,
author = {Zeng, Bixiao and Yang, Xiaodong and Chen, Yiqiang and Yu, Hanchao and Zhang, Yingwei},
title = {CLC: A Consensus-based Label Correction Approach in Federated Learning},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3519311},
doi = {10.1145/3519311},
abstract = {Federated learning (FL) is a novel distributed learning framework where multiple participants collaboratively train a global model without sharing any raw data to preserve privacy. However, data quality may vary among the participants, the most typical of which is label noise. The incorrect label would significantly damage the performance of the global model. In FL, the inaccessibility of raw data makes this issue more challenging. Previously published studies are limited to using a task-specific benchmark-trained model to evaluate the relevance between the benchmark dataset in the server and the local one on the participants’ side. However, such approaches have failed to exploit the cooperative nature of FL itself and are not practical. This paper proposes a Consensus-based Label Correction approach (CLC) in FL, which tries to correct the noisy labels using the developed consensus method among the FL participants. The consensus-defined class-wise information is used to identify the noisy labels and correct them with pseudo-labels. Extensive experiments are conducted on several public datasets in various settings. The experimental results prove the advantage over the state-of-art methods. The link to the source code is .},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jun},
articleno = {75},
numpages = {23},
keywords = {Federated learning, data evaluation, consensus mechanism}
}

@inproceedings{10.5555/3433701.3433711,
author = {Maulik, Romit and Egele, Romain and Lusch, Bethany and Balaprakash, Prasanna},
title = {Recurrent neural network architecture search for geophysical emulation},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {Developing surrogate geophysical models from data is a key research topic in atmospheric and oceanic modeling because of the large computational costs associated with numerical simulation methods. Researchers have started applying a wide range of machine learning models, in particular neural networks, to geophysical data for forecasting without these constraints. Constructing neural networks for forecasting such data is non-trivial, however, and often requires trial and error. To address these limitations, we focus on developing proper-orthogonal-decomposition-based long short-term memory networks (POD-LSTMs). We develop a scalable neural architecture search for generating stacked LSTMs to forecast temperature in the NOAA Optimum Interpolation Sea-Surface Temperature data set. Our approach identifies POD-LSTMs that are superior to manually designed variants and baseline time-series prediction methods. We also assess the scalability of different architecture search strategies on up to 512 Intel Knights Landing nodes of the Theta supercomputer at the Argonne Leadership Computing Facility.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {8},
numpages = {14},
keywords = {emulation, geophysics, recurrent neural networks},
location = {Atlanta, Georgia},
series = {SC '20}
}

@article{10.1145/3567594,
author = {Mahlaza, Zola and Keet, C. Maria},
title = {Surface Realization Architecture for Low-resourced African Languages},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3567594},
doi = {10.1145/3567594},
abstract = {There has been growing interest in building surface realization systems to support the automatic generation of text in African languages. Such tools focus on converting abstract representations of meaning to a text. Since African languages are low-resourced, economical use of resources and general maintainability are key considerations. However, there is no existing surface realizer architecture that possesses most of the maintainability characteristics (e.g., modularity, reusability, and analyzability) that will lead to maintainable software that can be used for the languages. Moreover, there is no consensus surface realization architecture created for other languages that can be adapted for the languages in question. In this work, we solve this by creating a novel surface realizer architecture suitable for low-resourced African languages that abides by the features of maintainable software. Its design comes after a granular analysis, classification, and comparison of the architectures used by 77 existing NLG systems. We compare our architecture to existing architectures and show that it supports the most features of a maintainable software product.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
articleno = {84},
numpages = {26},
keywords = {Natural language generation, software architecture, low-resourced languages, surface realisation}
}

@inproceedings{10.1145/3289602.3293905,
author = {Hardieck, Martin and Kumm, Martin and M\"{o}ller, Konrad and Zipf, Peter},
title = {Reconfigurable Convolutional Kernels for Neural Networks on FPGAs},
year = {2019},
isbn = {9781450361378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289602.3293905},
doi = {10.1145/3289602.3293905},
abstract = {Convolutional neural networks (CNNs) gained great success in machine learning applications and much attention was paid to their acceleration on field programmable gate arrays (FPGAs). The most demanding computational complexity of CNNs is found in the convolutional layers, which account for 90% of the total operations. The fact that parameters in convolutional layers do not change over a long time interval in weight stationary CNNs allows the use of reconfiguration to reduce the resource requirements. This work proposes several alternative reconfiguration schemes that significantly reduce the complexity of sum-of-products operations. The proposed direct configuration schemes provide the least resource requirements and fast reconfiguration times of 32 clock cycles but require additional memory for the pre-computed configurations. The proposed online reconfiguration scheme uses an online computation of the LUT contents to avoid this memory overhead. Finally, a scheme that duplicates the reconfigurable LUTs is proposed for which the reconfiguration time can be completely hidden in the computation time. Combined with a few online reconfiguration circuits, this provides the same configuration memory and configuration time as a conventional parallel kernel but offers large resource reductions of up to 80% of the LUTs.},
booktitle = {Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {43–52},
numpages = {10},
keywords = {sop, sum of product},
location = {Seaside, CA, USA},
series = {FPGA '19}
}

@article{10.1109/TCBB.2019.2910061,
author = {McDermott, Matthew B.A. and Wang, Jennifer and Zhao, Wen-Ning and Sheridan, Steven D. and Szolovits, Peter and Kohane, Isaac and Haggarty, Stephen J. and Perlis, Roy H.},
title = {Deep Learning Benchmarks on L1000 Gene Expression Data},
year = {2020},
issue_date = {Nov.-Dec. 2020},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {17},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2910061},
doi = {10.1109/TCBB.2019.2910061},
abstract = {Gene expression data can offer deep, physiological insights beyond the static coding of the genome alone. We believe that realizing this potential requires specialized, high-capacity machine learning methods capable of using underlying biological structure, but the development of such models is hampered by the lack of published benchmark tasks and well characterized baselines. In this work, we establish such benchmarks and baselines by profiling many classifiers against biologically motivated tasks on two curated views of a large, public gene expression dataset (the LINCS corpus) and one privately produced dataset. We provide these two curated views of the public LINCS dataset and our benchmark tasks to enable direct comparisons to future methodological work and help spur deep learning method development on this modality. In addition to profiling a battery of traditional classifiers, including linear models, random forests, decision trees, K nearest neighbor (KNN) classifiers, and feed-forward artificial neural networks (FF-ANNs), we also test a method novel to this data modality: graph convolugtional neural networks (GCNNs), which allow us to incorporate prior biological domain knowledge. We find that GCNNs can be highly performant, with large datasets, whereas FF-ANNs consistently perform well. Non-neural classifiers are dominated by linear models and KNN classifiers.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {dec},
pages = {1846–1857},
numpages = {12}
}

@inproceedings{10.1145/3580305.3599358,
author = {Geng, Haoyu and Wang, Runzhong and Wu, Fei and Yan, Junchi},
title = {GAL-VNE: Solving the VNE Problem with Global Reinforcement Learning and Local One-Shot Neural Prediction},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599358},
doi = {10.1145/3580305.3599358},
abstract = {The NP-hard combinatorial Virtual Network Embedding (VNE) Problem refers to finding the node and edge mapping between a virtual net (request) and the physical net (resource). Learning-based methods are recently devised beyond traditional heuristic solvers. However, the efficiency and scalability hinder its applicability as reinforcement learning (RL) is often adopted in an auto-regressive node-by-node mapping manner to handle complex mapping constraints, for each coming request for mapping. Moreover, existing learning-based works often independently consider each online request, limiting the long-term online service performance. In this paper, we present a synergistic Global-And-Local learning approach for the VNE problem (GAL-VNE). At the global level across requests, RL is employed to capture the cross-request relation for better global resource accommodation to improve overall performance. At the local level within each request, we aim to replace the sequential decision-making procedure which relies much on the network size, with a more efficient one-shot solution generation scheme. The main challenge for such a one-shot model is how to encode the constraints under an end-to-end learning and inference paradigm. Accordingly, within the "rank-then-search" paradigm, we propose to first pretrain a graph neural network (GNN)-based node ranker with imitation supervision from an off-the-shelf solver (moderately expensive yet high quality), which is meanwhile regularized by a neighboring smooth prior. Then RL is used to finetune the GNN ranker whose supervision directly refers to the final (undifferentiable) business objectives concerning revenue and cost, etc. Experiments on benchmarks show that our method outperforms classic and learning-based methods in both efficacy and efficiency.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {531–543},
numpages = {13},
keywords = {combinatorial optimization, reinforcement learning, virtual network embedding},
location = {<conf-loc>, <city>Long Beach</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {KDD '23}
}

@article{10.5555/3546258.3546480,
author = {Muthukumar, Vidya and Narang, Adhyyan and Subramanian, Vignesh and Belkin, Mikhail and Hsu, Daniel and Sahai, Anant},
title = {Classification vs regression in overparameterized regimes: does the loss function matter?},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {We compare classification and regression tasks in an overparameterized linear model with Gaussian features. On the one hand, we show that with sufficient overparameterization all training points are support vectors: solutions obtained by least-squares minimum-norm interpolation, typically used for regression, are identical to those produced by the hard-margin support vector machine (SVM) that minimizes the hinge loss, typically used for training classifiers. On the other hand, we show that there exist regimes where these interpolating solutions generalize well when evaluated by the 0-1 test loss function, but do not generalize if evaluated by the square loss function, i.e. they approach the null risk. Our results demonstrate the very different roles and properties of loss functions used at the training phase (optimization) and the testing phase (generalization).},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {222},
numpages = {69},
keywords = {classification, regression, overparameterized, support vector machines, survival, contamination}
}

@article{10.1109/TNET.2021.3093452,
author = {Yan, Zun and Cheng, Peng and Chen, Zhuo and Vucetic, Branka and Li, Yonghui},
title = {Two-Dimensional Task Offloading for Mobile Networks: An Imitation Learning Framework},
year = {2021},
issue_date = {Dec. 2021},
publisher = {IEEE Press},
volume = {29},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3093452},
doi = {10.1109/TNET.2021.3093452},
abstract = {Mobile computing network is envisioned as a powerful framework to support the growing computation-intensive applications in the era of the Internet of Things (IoT). In this paper, we exploit the potential of a multi-layer network via a two-dimensional (2-D) task offloading scheme, which enables horizontal cooperations among the edge nodes. To minimize the average task offloading delay for all the mobile users, we formulate a mixed non-linear programming (MINLP) by jointly optimizing the 2-D offloading decisions and communication/computational resource allocation. To address this very challenging problem, we exploit the unique algorithmic structure of the optimal branch-and-bound (B&amp;B) algorithm, and propose a novel Gaussian process imitation learning (GPIL) method to learn how to discover the shortcut for node searching in the B&amp;B enumeration tree and significantly accelerate the B&amp;B algorithm. When the network key parameters change, we further propose a novel recursive GPIL (RGPIL) method to agilely adapt to the new scenario with a fast policy update, where the new posterior distribution can be recursively updated based on a few new training data. Our simulation results show that the proposed method can achieve a near optimal solution with a significantly reduced complexity (e.g., a reduction of 98.7% in the number of searched nodes for a typical case). On this basis, the advantage of 2-D offloading scheme over the conventional schemes is also verified.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {2494–2507},
numpages = {14}
}

@article{10.1145/3589295,
author = {Wang, Yisu Remy and Willsey, Max and Suciu, Dan},
title = {Free Join: Unifying Worst-Case Optimal and Traditional Joins},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589295},
doi = {10.1145/3589295},
abstract = {Over the last decade, worst-case optimal join (WCOJ) algorithms have emerged as a new paradigm for one of the most fundamental challenges in query processing: computing joins efficiently. Such an algorithm can be asymptotically faster than traditional binary joins, all the while remaining simple to understand and implement. However, they have been found to be less efficient than the old paradigm, traditional binary join plans, on the typical acyclic queries found in practice. Some database systems that support WCOJ use a hybrid approach: use WCOJ to process the cyclic subparts of the query (if any), and rely on traditional binary joins otherwise. In this paper we propose a new framework, called Free Join, that unifies the two paradigms. We describe a new type of plan, a new data structure (which unifies the hash tables and tries used by the two paradigms), and a suite of optimization techniques. Our system, implemented in Rust, matches or outperforms both traditional binary joins and WCOJ on standard query benchmarks.},
journal = {Proc. ACM Manag. Data},
month = {jun},
articleno = {150},
numpages = {23}
}

@inproceedings{10.1145/3295500.3356183,
author = {Patel, Tirthak and Byna, Suren and Lockwood, Glenn K. and Tiwari, Devesh},
title = {Revisiting I/O behavior in large-scale storage systems: the expected and the unexpected},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356183},
doi = {10.1145/3295500.3356183},
abstract = {Large-scale applications typically spend a large fraction of their execution time performing I/O to a parallel storage system. However, with rapid progress in compute and storage system stack of large-scale systems, it is critical to investigate and update our understanding of the I/O behavior of large-scale applications. Toward that end, in this work, we monitor, collect and analyze a year worth of storage system data from a large-scale production parallel storage system. We perform temporal, spatial and correlative analysis of the system and uncover surprising patterns which defy existing assumptions and have important implications for future systems.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {65},
numpages = {13},
location = {Denver, Colorado},
series = {SC '19}
}

@article{10.1145/3462775,
author = {Gade, Sri Harsha and Deb, Sujay},
title = {A Novel Hybrid Cache Coherence with Global Snooping for Many-core Architectures},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/3462775},
doi = {10.1145/3462775},
abstract = {Cache coherence ensures correctness of cached data in multi-core processors. Traditional implementations of existing protocols make them unscalable for many core architectures. While snoopy coherence requires unscalable ordered networks, directory coherence is weighed down by high area and energy overheads. In this work, we propose Wireless-enabled Share-aware Hybrid (WiSH) to provide scalable coherence in many core processors. WiSH implements a novel Snoopy over Directory protocol using on-chip wireless links and hierarchical, clustered Network-on-Chip to achieve low-overhead and highly efficient coherence. A local directory protocol maintains coherence within a cluster of cores, while coherence among such clusters is achieved through global snoopy protocol. The ordered network for global snooping is provided through low-latency and low-energy broadcast wireless links. The overheads are further reduced through share-aware cache segmentation to eliminate coherence for private blocks. Evaluations show that WiSH reduces traffic by  and runtime by , while requiring  smaller storage and  lower energy as compared to existing hierarchical and hybrid coherence protocols. Owing to its modularity, WiSH provides highly efficient and scalable coherence for many core processors.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {sep},
articleno = {2},
numpages = {31},
keywords = {Cache coherence, hybrid protocol, many core processors, mm-wave wireless links}
}

@article{10.1109/TNET.2020.2965161,
author = {Liu, Tingwei and Lui, John C. S.},
title = {FAVE: A Fast and Efficient Network Flow AVailability Estimation Method With Bounded Relative Error},
year = {2020},
issue_date = {April 2020},
publisher = {IEEE Press},
volume = {28},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2965161},
doi = {10.1109/TNET.2020.2965161},
abstract = {Capacity planning and sales projection are essential tasks for network operators. This work aims to help network providers to carry out network capacity planning and sales projection by answering: Given topology and capacity, whether the network can serve current flow demands with high probabilities? We name such probability as the “&lt;italic&gt;flow availability&lt;/italic&gt;”, and present the flow availability estimation (FAVE) problem with generalizing the classical network connectivity based and maximum flow based reliability estimations. To quickly estimate flow availabilities, we utilize correlations among link and flow failures to figure out the importance of roles played by different links in flow failures (i.e., flow demands could not be satisfied). And we design three sequential importance sampling (SIS) estimation methods, which are: (1) &lt;italic&gt;Accurate and efficient&lt;/italic&gt;: They achieve a bounded or even vanishing relative error with linear computational complexities. Hence they can provide more accurate estimations in less simulation time. (2) &lt;italic&gt;Robust and scalable:&lt;/italic&gt; They maintain such estimation efficiencies even if only a partial SEED set information is available, or when the FAVE problem is extended to the multiple flows case. When applying to a realistic backbone network, our method can reduce the flow availability estimation cost by 900 and 130 times compared with MC and baseline IS methods; and also facilitate capacity planning and sales projection by providing better flow availability guarantees, compared with traditional methods.},
journal = {IEEE/ACM Trans. Netw.},
month = {apr},
pages = {505–518},
numpages = {14}
}

@article{10.1145/3314407,
author = {Nair, Suraj and Javkar, Kiran and Wu, Jiahui and Frias-Martinez, Vanessa},
title = {Understanding Cycling Trip Purpose and Route Choice Using GPS Traces and Open Data},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3314407},
doi = {10.1145/3314407},
abstract = {Many mobile applications such as Strava or Mapmyride allow cyclists to collect detailed GPS traces of their trips for health or route sharing purposes. However, cycling GPS traces also have a lot of potential from an urban planning perspective. In this paper, we focus on two important issues to characterize urban cyclist behavior: trip purpose and route choice. Cycling trip purpose has been typically analyzed using survey data. Here, we present a method to automatically infer the purpose of a cycling trip using cyclists' personal data, GPS traces and a variety of built-in and social environment features extracted from open datasets characterizing the streets cycled. We evaluate the proposed method using GPS traces from over 7, 000 cycling routes in the city of Philadelphia and report F1 scores of up to 86% when four trip purposes are considered. On the other hand, we also present a novel statistical method to identify the role that certain variables characterizing the built-in and social environment play in the selection of a specific cycling route. Our results show that cyclists in Philadelphia tend to favor routes with green areas, safety and centrality.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {20},
numpages = {26},
keywords = {spatio-temporal cycling traces, statistical analysis of route choice, trip purpose classification}
}

@article{10.1145/3448610,
author = {Mistry, Sajib and Qu, Lie and Bouguettaya, Athman},
title = {Layer-based Composite Reputation Bootstrapping},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3448610},
doi = {10.1145/3448610},
abstract = {We propose a novel generic reputation bootstrapping framework for composite services. Multiple reputation-related indicators are considered in a layer-based framework to implicitly reflect the reputation of the component services. The importance of an indicator on the future performance of a component service is learned using a modified Random Forest algorithm. We propose a topology-aware Forest Deep Neural Network (fDNN) to find the correlations between the reputation of a composite service and reputation indicators of component services. The trained fDNN model predicts the reputation of a new composite service with the confidence value. Experimental results with real-world dataset prove the efficiency of the proposed approach.},
journal = {ACM Trans. Internet Technol.},
month = {sep},
articleno = {13},
numpages = {28},
keywords = {Reputation bootstrapping, composite services, reputation indicators, composition topology, Random Forest, Deep Neural Network, bootstrapping confidence}
}

@proceedings{10.1145/3594536,
title = {ICAIL '23: Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law},
year = {2023},
isbn = {9798400701979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is my great pleasure to present to you the proceedings of the Nineteenth International Conference on Artificial Intelligence and Law (ICAIL 2023). The conference will be held June 19-23 at the Universidade do Minho in Braga, Portugal. It has been organized by the International Association for Artificial Intelligence and Law (IAAIL) and is held in cooperation with AAAI and ACM SIGAI. IAAIL's mission is to facilitate research, collaboration, and interdisciplinary communication at the intersection of law and the technical disciplines belonging to the field of artificial intelligence. The first ICAIL conference was held in 1987 and its 2023 iteration is the first to be held in person again after the Covid-19 pandemic.},
location = {<conf-loc>, <city>Braga</city>, <country>Portugal</country>, </conf-loc>}
}

@proceedings{10.1145/3603166,
title = {UCC '23: Proceedings of the IEEE/ACM 16th International Conference on Utility and Cloud Computing},
year = {2023},
isbn = {9798400702341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The IEEE/ACM International Conference on Utility and Cloud Computing (UCC) is a premier annual conference series aiming to provide a platform for researchers from both academia and industry to present new discoveries in the broad area of Cloud and Edge utility computing and applications.},
location = {<conf-loc>, <city>Taormina (Messina)</city>, <country>Italy</country>, </conf-loc>}
}

@proceedings{10.1145/3634814,
title = {ASSE '23: Proceedings of the 2023 4th Asia Service Sciences and Software Engineering Conference},
year = {2023},
isbn = {9798400708534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {<conf-loc>, <city>Aizu-Wakamatsu City</city>, <country>Japan</country>, </conf-loc>}
}

@article{10.1145/3448977,
author = {Laranjeiro, Nuno and Agnelo, Jo\~{a}o and Bernardino, Jorge},
title = {A Systematic Review on Software Robustness Assessment},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3448977},
doi = {10.1145/3448977},
abstract = {Robustness is the degree to which a certain system or component can operate correctly in the presence of invalid inputs or stressful environmental conditions. With the increasing complexity and widespread use of computer systems, obtaining assurances regarding their robustness has become of vital importance. This survey discusses the state of the art on software robustness assessment, with emphasis on key aspects like types of systems being evaluated, assessment techniques used, the target of the techniques, the types of faults used, and how system behavior is classified. The survey concludes with the identification of gaps and open challenges related with robustness assessment.},
journal = {ACM Comput. Surv.},
month = {may},
articleno = {89},
numpages = {65},
keywords = {Software robustness, robustness evaluation, robustness testing}
}

@proceedings{10.1145/3229147,
title = {Expressive '18: Proceedings of the Joint Symposium on Computational Aesthetics and Sketch-Based Interfaces and Modeling and Non-Photorealistic Animation and Rendering},
year = {2018},
isbn = {9781450358927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The Expressive conference series was born out of three workshops; CAe - Computational Aesthetics, SBIM - Sketch Based Interfaces Modelling and animation and NPAR - Non-Photo Realistic Animation and Rendering. The amalgamation has brought together artists, scientists, researchers and practitioners to showcase cutting-edge research and artistic innovation in these disciplines. We are jointly sponsored by Eurographics and ACM SIGGRAPH and are grateful for a generous donation from Disney Research. The conference is now in the fourteenth year of running under the Expressive banner.},
location = {Victoria, British Columbia, Canada}
}

@proceedings{10.1145/3569902,
title = {LADC '22: Proceedings of the 11th Latin-American Symposium on Dependable Computing},
year = {2022},
isbn = {9781450397377},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {<conf-loc>, <city>Fortaleza/CE</city>, <country>Brazil</country>, </conf-loc>}
}

@proceedings{10.1145/3552327,
title = {ECCE '22: Proceedings of the 33rd European Conference on Cognitive Ergonomics},
year = {2022},
isbn = {9781450398084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {<conf-loc>, <city>Kaiserslautern</city>, <country>Germany</country>, </conf-loc>}
}

@proceedings{10.5555/3623293,
title = {ICSE-SEIP '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
location = {Melbourne, Australia}
}

@proceedings{10.1145/3551349,
title = {ASE '22: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
year = {2022},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {<conf-loc>, <city>Rochester</city>, <state>MI</state>, <country>USA</country>, </conf-loc>}
}

@proceedings{10.1145/3611643,
title = {ESEC/FSE 2023: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to ESEC/FSE 2023, the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. ESEC/FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>}
}

@proceedings{10.5555/3643142,
title = {WSC '23: Proceedings of the Winter Simulation Conference},
year = {2023},
isbn = {9798350369663},
publisher = {IEEE Press},
location = {<conf-loc>, <city>San Antonio</city>, <state>Texas</state>, <country>USA</country>, </conf-loc>}
}

