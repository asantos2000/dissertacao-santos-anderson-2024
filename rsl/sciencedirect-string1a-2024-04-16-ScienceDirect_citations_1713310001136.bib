@article{WISNIEWSKI2019100534,
title = {Analysis of Ontology Competency Questions and their formalizations in SPARQL-OWL},
journal = {Journal of Web Semantics},
volume = {59},
pages = {100534},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2019.100534},
url = {https://www.sciencedirect.com/science/article/pii/S1570826819300617},
author = {Dawid Wiśniewski and Jedrzej Potoniec and Agnieszka Ławrynowicz and C. Maria Keet},
keywords = {Ontology Authoring, Competency Questions, SPARQL-OWL},
abstract = {Competency Questions (CQs) are natural language questions outlining and constraining the scope of knowledge represented in an ontology. Despite that CQs are a part of several ontology engineering methodologies, the actual publication of CQs for the available ontologies is very limited and even scarcer is the publication of their respective formalizations in terms of, e.g., SPARQL queries. This paper aims to contribute to addressing the myriad of engineering hurdles to using CQs in ontology development. A prerequisite to this is to understand the relation between CQs and the queries over the ontology. We use a new dataset of 234 competency questions and their SPARQL-OWL queries for several ontologies in different domains developed by different groups, and analysed the CQs in two principal ways. The first stage focused on a linguistic analysis of the natural language text itself, i.e., a lexico-syntactic analysis without any presuppositions of ontology elements, and a subsequent step of semantic analysis in order to find patterns. This increased diversity of CQ sources resulted in a 4-5-fold increase of hitherto published patterns, to 106 distinct CQ patterns, which have a limited subset of few patterns shared across the CQ sets from the different ontologies. Next, we analysed the relation between the found CQ patterns and their respective SPARQL-OWL patterns, which revealed that one CQ pattern may be realized by more than one SPARQL-OWL query pattern, and vice versa. These insights may contribute to establishing common practices, templates, automation, and user tools that will support CQ formulation, formalization, execution, and general management.}
}
@article{HU2020588,
title = {Scalable aggregate keyword query over knowledge graph},
journal = {Future Generation Computer Systems},
volume = {107},
pages = {588-600},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19300251},
author = {Xin Hu and Jiangli Duan and Depeng Dang},
keywords = {Knowledge graph, Question answering, Keyword search, Aggregation},
abstract = {Existing keyword query systems over knowledge graphs are easy to use and can produce interesting results. However, they cannot address even simple aggregate queries (i.e., a query that needs statistics such as COUNT, SUM, AVG, MAX, MIN, >, < and =), and the sizes of existing schema graphs grow exponentially with the growth of the number of types or predicates in the knowledge graph, so that they have low scalability for building SPARQL statements. Therefore, we propose a framework called SAKQ (scalable aggregate keyword query over knowledge graph) that enables users to pose aggregate queries using simple keywords. First, we propose a scalable schema graph (i.e., type-predicate graph) that consists of the relationships between types and predicates, which has a small data size and contains all information needed for building SPARQL statements. Second, based on the type-predicate graph, we propose two algorithms to build query graphs with aggregation and transform the query graphs into SPARQL statements with aggregation. Finally, the experimental results over the benchmark datasets demonstrate that SAKQ can answer various general aggregate keyword queries.}
}
@article{ALELAIMAT2023103805,
title = {XPlaM: A toolkit for automating the acquisition of BDI agent-based Digital Twins of organizations},
journal = {Computers in Industry},
volume = {145},
pages = {103805},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103805},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522002019},
author = {Ahmad Alelaimat and Aditya Ghose and Hoa Khanh Dam},
keywords = {Digital Twin, BDI agents, Abductive reasoning},
abstract = {A Digital Twin ideally manifests the same behaviour (in silico) of its physical counterpart. While considerable attention has been paid to the development of Digital Twins for physical devices/systems, the question of developing Twins for organizations has received relatively little attention. The setting in which we address this problem is very general and, consequently, very challenging. We look at the automatic acquisition of Digital Twins of organizations. To that end, this paper builds on the following two premises: (1) that Digital Twins of organizations can provide value, and (2) that Belief-Desire-Intention (BDI) agents are a particularly effective means for representing Digital Twins. The overall approach is to leverage the externally observable behaviour of the target system and then generate candidate BDI agent programs that best explain (in the sense of formal abduction) the observed behaviour. The candidate agent programs are generated by searching through potentially large hypotheses spaces for possible plans, selection functions and beliefs. The resulting approach suggests that using abduction to generate Digital Twins of organizations in the form of BDI agents can be effective.}
}
@article{ZHONG2022102871,
title = {Using long vector extensions for MPI reductions},
journal = {Parallel Computing},
volume = {109},
pages = {102871},
year = {2022},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2021.102871},
url = {https://www.sciencedirect.com/science/article/pii/S0167819121001137},
author = {Dong Zhong and Qinglei Cao and George Bosilca and Jack Dongarra},
keywords = {Long vector extension, Vector operation, Intel AVX2/AVX-512, Instruction level parallelism, Single instruction multiple data, MPI reduction operation, Scalable Vector Extension (SVE)},
abstract = {The modern CPU’s design, including the deep memory hierarchies and SIMD/vectorization capability have a more significant impact on algorithms’ efficiency than the modest frequency increase observed recently. The current introduction of wide vector instruction set extensions (AVX and SVE) motivated vectorization to become a critical software component to increase efficiency and close the gap to peak performance. In this paper, we investigate the impact of the vectorization of MPI reduction operations. We propose an implementation of predefined MPI reduction operations using vector intrinsics (AVX and SVE) to improve the time-to-solution of the predefined MPI reduction operations. The evaluation of the resulting software stack under different scenarios demonstrates that the approach is not only efficient but also generalizable to many vector architectures. Experiments conducted on varied architectures (Intel Xeon Gold, AMD Zen 2, and Arm A64FX), show that the proposed vector extension optimized reduction operations significantly reduce completion time for collective communication reductions. With these optimizations, we achieve higher memory bandwidth and an increased efficiency for local computations, which directly benefit the overall cost of collective reductions and applications based on them.}
}
@article{DANENAS2020101822,
title = {Natural language processing-enhanced extraction of SBVR business vocabularies and business rules from UML use case diagrams},
journal = {Data & Knowledge Engineering},
volume = {128},
pages = {101822},
year = {2020},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2020.101822},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X1930299X},
author = {Paulius Danenas and Tomas Skersys and Rimantas Butleris},
keywords = {SBVR business vocabulary and rules, UML use case diagram, Model-to-model transformation, Controlled natural language, Natural language processing, Information extraction},
abstract = {Discovery, specification and proper representation of various aspects of business knowledge plays crucial part in model-driven information systems engineering, especially when it comes to the early stages of systems development. Being among the most applicable and advanced features of model-driven development, model transformation could help improving one of the most time- and resource-consuming efforts in this process, namely, discovery and specification of business vocabularies and business rules within the problem domain. One of our latest developments in this area was the solution for the automatic extraction of SBVR business vocabularies and business rules from UML use case diagrams, which was arguably one of the most comprehensive developments of this kind currently available in public. In this paper, we present an enhancement to our previous development by introducing a novel natural language processing component to it. This enhancement provides more advanced extraction capabilities (such as recognition of entities, entire noun and verb phrases, multinary associations) and better quality of the extraction results compared to our previous solution. The main contributions presented in this paper are pre- and post-processing algorithms, and two extraction algorithms using custom-trained POS tagger. Based on the related work findings, it is safe to state that the presented solution is novel and original in its approach of combining together M2M transformation of UML and SBVR models with natural language processing techniques in the field of model-driven information systems engineering.}
}
@incollection{2022267,
title = {Index},
editor = {Sanju Tiwari and Fernando {Ortiz Rodriguez} and M.A. Jabbar},
booktitle = {Semantic Models in IoT and eHealth Applications},
publisher = {Academic Press},
pages = {267-273},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-91773-5},
doi = {https://doi.org/10.1016/B978-0-32-391773-5.00019-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323917735000194}
}
@article{PANZARELLA2023100059,
title = {Using ontologies for life science text-based resource organization},
journal = {Artificial Intelligence in the Life Sciences},
volume = {3},
pages = {100059},
year = {2023},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2023.100059},
url = {https://www.sciencedirect.com/science/article/pii/S266731852300003X},
author = {Giulia Panzarella and Pierangelo Veltri and Stefano Alcaro},
keywords = {Information overload, Ontology, Semantic web, Life science terms},
abstract = {Ontologies are used to support access to a multitude of databases that cover domains relevant information. Heterogeneity and different semantics can be accessed by using structured texts and descriptions in a hierarchical concept definition. We are interested in Life Sciences (LS) related ontologies including components taken from molecular biology, bioinformatics, physics, chemistry, medicine and other related areas. An Ontology comprises: (i) term connections, (ii) the identification of core concepts, (iii) data management, (iv) knowledge classification and integration to collect key information. An ontology may be very useful in navigating through LS terms. This paper explores some available biomedical ontologies and frameworks. It describes the most common ontology development environments (ODE): Protégé, Topbraid Composer, Ontostudio, Fluent Editor, VocBench, Swoop and Obo-edit, to create ontologies from textual scientific resources for LS plans. It also compares ontology methodologies in terms of Usability, Scalability, Stability, Integration, Documentation and Originality.}
}
@incollection{MCGILVRAY202129,
title = {Chapter 3 - Key Concepts},
editor = {Danette McGilvray},
booktitle = {Executing Data Quality Projects (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {29-72},
year = {2021},
isbn = {978-0-12-818015-0},
doi = {https://doi.org/10.1016/B978-0-12-818015-0.00009-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128180150000098},
author = {Danette McGilvray},
keywords = {Framework for Information Quality (FIQ), information life cycle, data life cycle, lineage, data quality dimensions, business impact techniques, data categories, master data, data specifications, metadata, data standards, reference data, data models, business rules, data governance, data stewardship, Ten Steps Process, data quality improvement cycle},
abstract = {This chapter introduces fundamental ideas, the understanding of which, will aid data quality work and use of the Ten Steps Process. Information, like financial and human resources, must be properly managed throughout its life cycle to get the full use and benefit from it, so the information life cycle is discussed with the acronym POSMAD as an easy way to remember the six phases of the information life cycle: Plan, Obtain, Store and Share, Maintain, Apply, and Dispose. POSMAD plus several additional concepts are summarized in the Framework for Information Quality (FIQ), which provides an at-a-glance view of the components necessary to have high quality information. Other concepts central to understanding and managing data are discussed, such as data quality dimensions, business impact techniques, data categories, and data specifications (with a focus on metadata, data standards, reference data, data models, and business rules), data governance and stewardship. An overview of each of the steps in the Ten Steps Process is given, along with their relationship to the concepts.}
}
@article{FAYOUMI2021100221,
title = {An integrated socio-technical enterprise modelling: A scenario of healthcare system analysis and design},
journal = {Journal of Industrial Information Integration},
volume = {23},
pages = {100221},
year = {2021},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100221},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000212},
author = {Amjad Fayoumi and Richard Williams},
keywords = {Enterprise Modelling, Socio-technical Systems, Enterprise Integrated Model, Conceptual Modelling, Healthcare System},
abstract = {One of the crucial issues facing enterprise modelling (EM) practices is that EM is considered technical, and rarely or never has a social focus. Social aspects referred to here are the soft aspects of the organisation that lead to organic organisation development (communication, collaboration, culture, skills and personal goals). There are many EM approaches and enterprise architecture frameworks were proposed recently. These cover different enterprise aspects, perspectives, artefacts and models with different qualities and levels of details. Yet, the imperative determination has overlaid the declarative exploration in EM as a necessity of the design effort. Rethinking the assumptions underlying EM should bring a new and different understanding on how EM can be tackled within the enterprise, in particular the joint development and optimisation of socio-technical systems. This paper discusses EM from a socio-technical systems (STS) perspective, and towards forming a new model of EM that is driven from STS theory and combined with STS practices. Then proposes a conceptual integrated model that incorporates the new concepts of STS toward building an EM framework for balanced socio-technical joint development and optimisation. The approach is illustrated in a scenario from healthcare industry. A combination between modelling and STS practices proved powerful for holistic IT modernisation, future work discussed toward the end of the paper.}
}
@article{SAHAY201989,
title = {The application of Software Defined Networking on securing computer networks: A survey},
journal = {Journal of Network and Computer Applications},
volume = {131},
pages = {89-108},
year = {2019},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2019.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S108480451930027X},
author = {Rishikesh Sahay and Weizhi Meng and Christian D. Jensen},
keywords = {Software Defined Networking, Attack detection and mitigation, Network security, Middlebox management, Traffic management, Policy management, Traffic engineering, Smart grid security},
abstract = {Software Defined Networking (SDN) has emerged as a new networking paradigm for managing different kinds of networks ranging from enterprise to home network through software enabled control. The logically centralized control plane and programmability offers a great opportunity to improve network security, like implementing new mechanisms to detect and mitigate various threats, as well as enables deploying security as a service on the SDN controller. Due to the increasing and fast development of SDN, this paper provides an extensive survey on the application of SDN on enhancing the security of computer networks. In particular, we survey recent research studies that focus on applying SDN for network security including attack detection and mitigation, traffic monitoring and engineering, configuration and policy management, service chaining, and middlebox deployment, in addition to smart grid security. We further identify some challenges and promising future directions on SDN security, compatibility and scalability issues that should be addressed in this field.}
}
@article{GUNTHER2024103413,
title = {The formal verification of the ctm approach to forcing},
journal = {Annals of Pure and Applied Logic},
volume = {175},
number = {5},
pages = {103413},
year = {2024},
issn = {0168-0072},
doi = {https://doi.org/10.1016/j.apal.2024.103413},
url = {https://www.sciencedirect.com/science/article/pii/S0168007224000101},
author = {Emmanuel Gunther and Miguel Pagano and Pedro {Sánchez Terraf} and Matías Steinberg},
keywords = {Isabelle/ZF, Countable transitive models, Continuum hypothesis, Proof assistants, Interactive theorem provers, Generic extension},
abstract = {We discuss some highlights of our computer-verified proof of the construction, given a countable transitive set-model M of ZFC, of generic extensions satisfying ZFC+¬CH and ZFC+CH. Moreover, let R be the set of instances of the Axiom of Replacement. We isolated a 21-element subset Ω⊆R and defined F:R→R such that for every Φ⊆R and M-generic G, M⊨ZC∪F“Φ∪Ω implies M[G]⊨ZC∪Φ∪{¬CH}, where ZC is Zermelo set theory with Choice. To achieve this, we worked in the proof assistant Isabelle, basing our development on the Isabelle/ZF library by L. Paulson and others.}
}
@article{SONBOL2022108933,
title = {Learning software requirements syntax: An unsupervised approach to recognize templates},
journal = {Knowledge-Based Systems},
volume = {248},
pages = {108933},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108933},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122004518},
author = {Riad Sonbol and Ghaida Rebdawi and Nada Ghneim},
keywords = {Requirements Engineering, Requirements templates recognition, Natural Language Processing (NLP), Syntax learning, Graph community detection},
abstract = {Requirements are textual representations of the desired software capabilities. Many templates have been used to standardize the structure of requirement statements such as Rupps, EARS, and User Stories. Templates provide a good solution to improve different Requirements Engineering (RE) tasks since their well-defined syntax facilitates the different text processing steps in RE automation researches. However, many empirical studies have concluded that there is a gap between these RE researches and their implementation in industrial and real-life projects. The success of RE automation approaches strongly depends on the consistency of the requirements with the syntax of the predefined templates. Such consistency cannot be guaranteed in real projects, especially in large development projects, or when one has little control over the requirements authoring environment. In this paper, we propose an unsupervised approach to recognize templates from the requirements themselves by extracting their common syntactic structures. The resultant templates reflect the actual syntactic structure of requirements; hence it can recognize both standard and non-standard templates. Our approach uses techniques from Natural Language Processing and Graph Theory to handle this problem through three main stages (1) we formulate the problem as a graph problem, where each requirement is represented as a vertex and each pair of requirements has a structural similarity, (2) We detect main communities in the resultant graph by applying a hybrid technique combining limited dynamic programming and greedy algorithms, (3) finally, we reinterpret the detected communities as templates. Our experiments show that the suggested approach can detect templates that follow well-known standards with a 0.90 F1-measure. Moreover, the approach can detect common syntactic features for non-standard templates in more than 73.5% of the cases. Our evaluation indicates that these results are robust regardless of the number and the length of the processed requirements.}
}
@article{SILVA2019111,
title = {CPN simulation-based test case generation from controlled natural-language requirements},
journal = {Science of Computer Programming},
volume = {181},
pages = {111-139},
year = {2019},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167642319300516},
author = {Bruno Cesar F. Silva and Gustavo Carvalho and Augusto Sampaio},
keywords = {Model-based testing, Controlled natural language, Data-flow reactive system, Coloured Petri nets, Model simulation},
abstract = {We propose a test generation strategy from natural language (NL) requirements via translation into Coloured Petri Nets (CPN), an extension of Petri Nets that supports model structuring and provides a mature theory and powerful tool support. This approach extends our previous work on the NAT2TEST framework, which involves syntactic and semantic analyses of NL requirements and the generation of Data-Flow Reactive Systems (DFRS) as an intermediate representation, from which target formal models can be obtained for the purpose of test case generation. Our contributions include automating a systematic translation of DFRSs into CPN models, an extension to deal with time aspects, besides an empirical analysis of the CPN-based test generation strategy. The analyses considered examples both from the literature (a vending machine and a nuclear power plant control system), and from the aerospace and the automotive domain (a priority command control system and a turn indicator control system, respectively). We analysed performance and the ability to detect defects generated via mutation. The results provide evidence that the contribution proposed here is more efficient, besides being able to detect at least as many defects as our previous efforts.}
}
@article{ALDWYAN20191081,
title = {Latency-aware failover strategies for containerized web applications in distributed clouds},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {1081-1095},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.032},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19304224},
author = {Yasser Aldwyan and Richard O. Sinnott},
keywords = {Distributed Clouds, High availability, Performance, Container technologies, Cloud outages, Web applications, Distributed deployment},
abstract = {Despite advances in Cloud computing, ensuring high availability (HA) remains a challenge due to varying loads and the potential for Cloud outages. Deploying applications in distributed Clouds can help overcome this challenge by geo-replicating applications across multiple Cloud data centers (DCs). However, this distributed deployment can be a performance bottleneck due to network latencies between users and DCs as well as inter-DC latencies incurred during the geo-replication process. For most web applications, both HA and Performance (HAP) are essential and need to meet pre-agreed Service Level Objectives (SLOs). Efficiently placing and managing primary and backup replicas of applications in distributed Clouds to achieve HAP is a challenging task. Existing solutions consider either HA or performance but not both. In this paper we propose an approach for automating the process of providing a latency-aware failover strategy through a server placement algorithm leveraging genetic algorithms that factor in the proximity of users and inter-DC latencies. To facilitate the distributed deployment of applications and avoid the overheads of Clouds, we utilize container technologies. To evaluate our proposed approach, we conduct experiments on the Australia-wide National eResearch Collaboration Tools and Resources (NeCTAR - www.nectar.org.au) Research Cloud. Our results show at least a 23.3% and 22.6% improvement in response times under normal and failover conditions respectively compared to traditional, latency-unaware approaches. Also, the 95th percentile of response times in our approach are at most1.5 ms above the SLO compared to 11–32 ms using other approaches.}
}
@article{AI2020118819,
title = {Low-carbon product conceptual design from the perspectives of technical system and human use},
journal = {Journal of Cleaner Production},
volume = {244},
pages = {118819},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.118819},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619336893},
author = {Xianfeng Ai and Zhigang Jiang and Hua Zhang and Yan Wang},
keywords = {Low-carbon conceptual design, Function matching, Sustainable use, TRIZ laws, Regular expression, Part-of-speech tagging},
abstract = {Product conceptual design plays a decisive role in carbon emission of the products. Unfortunately, the traditional design methods based on carbon footprint calculation are not suitable for the conceptual design stage, and the latest low-carbon conceptual design research mainly focus on technology development to reduce carbon emissions at the manufacturing stage and less on carbon emissions caused by the unsustainable human use. This makes low-carbon product conceptual design less effective. To address this, a low-carbon conceptual design method is proposed for improving existing products, in which an improved process of requirements elicitation and analysis is implemented firstly, and then the improvement strategies are proposed from the perspectives of technical system and human use to help establish a low-carbon function structure. The conceptual design of a boiling water dispenser is taken as a case study. As a result, 5 low-carbon design strategies and a low-carbon function structure were comprehensively obtained. Next, by calculating the energy consumption of the assumed ideal scenario, it can be found that the re-designed product can save 39.4% energy compared to the existing product in the use stage. The results showed that the proposed method is effective in the generation of low-carbon design schemes at the conceptual design stage.}
}
@article{KHARMOUM2020819,
title = {Transformations’ Study Between Requirements Models and Business Process Models in MDA Approach},
journal = {Procedia Computer Science},
volume = {170},
pages = {819-824},
year = {2020},
note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.150},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920306062},
author = {Nassim Kharmoum and Karim El Bouchti and Naziha Laaz and Wajih Rhalem and Yassine Rhazali},
keywords = {Model-Driven Architecture, Model transformation, Requirements model, Business Process model, Computation Independent Model, Platform Independent Model},
abstract = {Models transformation has become in the last decade, the primary key in the Model Driven Architecture (MDA) approach. Most of these models’ transformation are made between one or many abstraction levels (higher level, average level, or lower level). For that, the Object Management Group (OMG) offers for the MDA approach three abstraction levels, which are “Computation Independent Model” (CIM: the higher abstraction level), “Platform Independent Model” (PIM: average abstraction level) and “Platform Specific Model” (PSM: lower abstraction level). Hitherto, most researchers focused on the transformation between average abstraction level and lower abstraction level because those levels have multiple common points. However, the transformation between higher abstraction level and average abstraction level is rarely discussed because they are two distinct levels that can contain higher abstraction model nature, such as Requirements models and Business Process models. Therefore, our contribution in this paper is to study different transformations approaches between Requirements and Business Process models in the higher and average MDA abstraction levels. To do so, we study the selected approaches, analyze the results descriptively, then discuss them, plus propose new taxonomies based on the deduced evaluation criteria.}
}
@article{ALSOLAI2024e23252,
title = {Automated sign language detection and classification using reptile search algorithm with hybrid deep learning},
journal = {Heliyon},
volume = {10},
number = {1},
pages = {e23252},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e23252},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023104609},
author = {Hadeel Alsolai and Leen Alsolai and Fahd N. Al-Wesabi and Mahmoud Othman and Mohammed Rizwanullah and Amgad Atta Abdelmageed},
keywords = {Sign language, Deep learning, Computer vision, Reptile search algorithm, Intelligent models},
abstract = {Sign language recognition (SLR) contains the capability to convert sign language gestures into spoken or written language. This technology is helpful for deaf persons or hard of hearing by providing them with a way to interact with people who do not know sign language. It is also be utilized for automatic captioning in live events and videos. There are distinct methods of SLR comprising deep learning (DL), computer vision (CV), and machine learning (ML). One general approach utilises cameras for capturing the signer's hand and body movements and processing the video data for recognizing the gestures. One of challenges with SLR comprises the variability in sign language through various cultures and individuals, the difficulty of certain signs, and require for realtime processing. This study introduces an Automated Sign Language Detection and Classification using Reptile Search Algorithm with Hybrid Deep Learning (SLDC-RSAHDL). The presented SLDC-RSAHDL technique detects and classifies different types of signs using DL and metaheuristic optimizers. In the SLDC-RSAHDL technique, MobileNet feature extractor is utilized to produce feature vectors, and its hyperparameters can be adjusted by manta ray foraging optimization (MRFO) technique. For sign language classification, the SLDC-RSAHDL technique applies HDL model, which incorporates the design of Convolutional Neural Network (CNN) and Long-Short Term Memory (LSTM). At last, the RSA was exploited for the optimal hyperparameter selection of the HDL model, which resulted in an improved detection rate. The experimental result analysis of the SLDC-RSAHDL technique on sign language dataset demonstrates the improved performance of the SLDC-RSAHDL system over other existing DL techniques.}
}
@article{MOKOS2020100030,
title = {A survey on the formalisation of system requirements and their validation},
journal = {Array},
volume = {7},
pages = {100030},
year = {2020},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2020.100030},
url = {https://www.sciencedirect.com/science/article/pii/S2590005620300151},
author = {Konstantinos Mokos and Panagiotis Katsaros},
keywords = {Requirement specification, Requirement formalisation, Semantic analysis, Model-based design, Component-based design, Formal verification},
abstract = {System requirements define conditions and capabilities to be met by a system under design. They are a partial definition in natural language, with inevitable ambiguities. Formalisation concerns with the transformation of requirements into a specification with unique interpretation, for resolving ambiguities, underspecified references and for assessing whether requirements are consistent, correct (i.e. valid for an acceptable solution) and attainable. Formalisation and validation of system requirements provides early evidence of adequate specification, for reducing the validation tests and high-cost corrective measures in the later system development phases. This article has the following contributions. First, we characterise the specification problem based on an ontology for some domain. Thus, requirements represent a particular system among many possible ones, and their specification takes the form of mapping their concepts to a semantic model of the system. Second, we analyse the state-of-the-art of pattern-based specification languages, which are used to avoid ambiguity. We then discuss the semantic analyses (missing requirements, inconsistencies etc.) supported in such a framework. Third, we survey related research on the derivation of formal properties from requirements, i.e. verifiable specifications that constrain the system’s structure and behaviour. Possible flaws in requirements may render the derived properties unsatisfiable or not realizable. Finally, this article discusses the important challenges for the current requirements analysis tools, towards being adopted in industrial-scale projects.}
}
@article{VOLEGZHANINA20221743,
title = {Methodology for the design of didactic tools for the future transport engineers’ competencies formation on the ontology basis},
journal = {Transportation Research Procedia},
volume = {63},
pages = {1743-1751},
year = {2022},
note = {X International Scientific Siberian Transport Forum — TransSiberia 2022},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2022.06.189},
url = {https://www.sciencedirect.com/science/article/pii/S2352146522004446},
author = {Irina Volegzhanina and Vladimir Adol’f and Svetlana Chusovlyanova},
keywords = {future engineer, railway transport, ontology, knowledge representation, questionnaire, didactic tools},
abstract = {The change in the essence of engineers’ functional activity activates the problem of creating new didactic tools adequate to the conditions of digital transformation of production and university education. The authors consider the ontological-semantic approach based on the use of ontological models and semantic technologies, as well as the principles of interoperability and interdisciplinarity corresponding to this approach as the methodological basis for the development of such tools. The feasibility of developing didactic tools on the ontology basis was determined using the questionnaire survey method, which was conducted among future railway transport engineers from 2016 to 2021. The results of the data analysis indicated a tendency for respondents to use compression tools for better assimilation of new learning material. Among such means students indicated not only tables, drawings and diagrams, but also a fundamentally new form of educational knowledge presentation - ontology. The conclusions made on the results of the questionnaire served as a basis for the development of didactic tools for the formation of communicative competence of future railway transport engineers. The novelty of this educational solution consisted in the complex use of textual, hypertext and ontological forms of branch knowledge representation, including the foreign language studied by students.}
}
@article{WANG2022101901,
title = {Business process and rule integration approaches—An empirical analysis of model understanding},
journal = {Information Systems},
volume = {104},
pages = {101901},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101901},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921001162},
author = {Wei Wang and Tianwa Chen and Marta Indulska and Shazia Sadiq and Barbara Weber},
keywords = {Business process modeling, Business rule modeling, Eye-tracking, Cognitive process, Model understanding, Controlled experiment},
abstract = {Business process models are widely used in organizations by information systems analysts to represent complex business requirements. They are also used by business users to understand business operations and constraints. This understanding is extracted from graphical process models as well as business rules. Prior research advocated integrating business rules and business process models to improve the effectiveness of various organizational activities, such as developing a shared understanding of practices, process improvement, and mitigating risks of compliance and policy breaches. However, whether such integrated modeling can improve the understanding of business processes, which is a fundamental benefit of integrated modeling, has not been empirically evaluated. In this paper, first, we report on an experiment investigating whether rule linking, a representative integrated modeling method, can improve understanding performance. We use eye tracking technology to understand the cognitive process by which model readers use models to perform understanding tasks. Our results show that rule linking outperforms separated modeling in terms of understanding effectiveness, efficiency, perceived mental effort, and visual attention. Further, cognitive process analysis reveals that the form of rule representation does not affect the extent of deep processing, but rule linking significantly decreases the occurrence of rule scanning and screening processes. Moreover, our results show that rule linking leads to an increase of visual association suggesting improved information integration, leading to improved task performance.}
}
@article{LORKIEWICZ20203163,
title = {Grounding of Modal Responses in Question Answering System Equipped with Hierarchical Categorisation},
journal = {Procedia Computer Science},
volume = {176},
pages = {3163-3172},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.172},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920320743},
author = {Wojciech Lorkiewicz and Grzegorz Popek},
keywords = {question answering systems, language grounding, concept hierarchy, cognitive agent},
abstract = {Intelligent and dialogue systems highly utilise natural language interfaces. Such systems do not only process linguistic questions, but also formulate proper linguistic responses. Often neglected and important aspect of such responses lies in the ability to express and communicate systems internal beliefs. The proposed model fills in the current research gap in the grounding theory by enriching empirical experiences with hierarchical semantic structures in establishing agents internal belief stance. Such an extension significantly influences the process of grounding and allows for a dedicated computational mechanism of moving the conversations focus, which is incorporated into grounding mechanisms and fully presented.}
}
@article{ABBADANDALOUSSI2020101505,
title = {On the declarative paradigm in hybrid business process representations: A conceptual framework and a systematic literature study},
journal = {Information Systems},
volume = {91},
pages = {101505},
year = {2020},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2020.101505},
url = {https://www.sciencedirect.com/science/article/pii/S0306437920300168},
author = {Amine {Abbad Andaloussi} and Andrea Burattin and Tijs Slaats and Ekkart Kindler and Barbara Weber},
keywords = {Hybrid process model, Understandability of process models, Process flexibility, Declarative process modeling, Business process modeling},
abstract = {Process modeling plays a central role in the development of today’s process-aware information systems both on the management level (e.g., providing input for requirements elicitation and fostering communication) and on the enactment level (providing a blue-print for process execution and enabling simulation). The literature comprises a variety of process modeling approaches proposing different modeling languages (i.e., imperative and declarative languages) and different types of process artifact support (i.e., process models, textual process descriptions, and guided simulations). However, the use of an individual modeling language or a single type of process artifact is usually not enough to provide a clear and concise understanding of the process. To overcome this limitation, a set of so-called “hybrid” approaches combining languages and artifacts have been proposed, but no common grounds have been set to define and categorize them. This work aims at providing a fundamental understanding of these hybrid approaches by defining a unified terminology, providing a conceptual framework and proposing an overarching overview to identify and analyze them. Since no common terminology has been used in the literature, we combined existing concepts and ontologies to define a “Hybrid Business Process Representation” (HBPR). Afterwards, we conducted a Systematic Literature Review (SLR) to identify and investigate the characteristics of HBPRs combining imperative and declarative languages or artifacts. The SLR resulted in 30 articles which were analyzed. The results indicate the presence of two distinct research lines and show common motivations driving the emergence of HBPRs, a limited maturity of existing approaches, and diverse application domains. Moreover, the results are synthesized into a taxonomy classifying different types of representations. Finally, the outcome of the study is used to provide a research agenda delineating the directions for future work.}
}
@article{ALHROOB20181,
title = {The use of artificial neural networks for extracting actions and actors from requirements document},
journal = {Information and Software Technology},
volume = {101},
pages = {1-15},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918300752},
author = {Aysh Al-Hroob and Ayad Tareq Imam and Rawan Al-Heisa},
keywords = {NLP, ANN, I-CASE, Software requirements, GATE, MATLAB},
abstract = {Context
The automatic extraction of actors and actions (i.e., use cases) of a system from natural language-based requirement descriptions, is considered a common problem in requirements analysis. Numerous techniques have been used to resolve this problem. Examples include rule-based (e.g., inference), keywords, query (e.g., bi-grams), library maintenance, semantic business vocabularies, and rules. The question remains: can combination of natural language processing (NLP) and artificial neural networks (ANNs) perform this job successfully and effectively?
Objective
This paper proposes a new approach to automatically identify actors and actions in a natural language-based requirements’ description of a system. Included are descriptions of how NLP plays an important role in extracting actors and actions, and how ANNs can be used to provide definitive identification.
Method
We used an NLP parser with a general architecture for text engineering, producing lexicons, syntaxes, and semantic analyses. An ANN was developed using five different use cases, producing different results due to their complexity and linguistic formation.
Results
Binomial classification accuracy techniques were used to evaluate the effectiveness of this approach. Based on the five use cases, the results were 17–63% for precision, 5–6100% for recall, and 29–71% for F-measure.
Conclusion
We successfully used a combination of NLP and ANN artificial intelligence techniques to reveal specific domain semantics found in a software requirements specification. An Intelligent Technique for Requirements Engineering (IT4RE) was developed to provide a semi-automated approach, classified as Intelligent Computer Aided Software Engineering (I-CASE).}
}
@article{STRASS201955,
title = {EMIL: Extracting Meaning from Inconsistent Language: Towards argumentation using a controlled natural language interface},
journal = {International Journal of Approximate Reasoning},
volume = {112},
pages = {55-84},
year = {2019},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2019.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X18300793},
author = {Hannes Strass and Adam Wyner and Martin Diller},
keywords = {Argumentation, Non-monotonic reasoning, Controlled natural language, Defeasible reasoning},
abstract = {There are well-developed formal and computational theories of argumentation to reason in the face of inconsistency, some with implementations; there are recent efforts to extract arguments from large textual corpora. Both developments are leading towards automated processing and reasoning with inconsistent, linguistically expressed knowledge in order to provide explanations and justifications in a form accessible to humans. However, there remains a gap between the knowledge-bases of computational theories of argumentation, which are generally coarse-grained and semi-structured (e.g. propositional logic), and inferences from knowledge-bases derived from natural language, which are fine-grained and highly structured (e.g. predicate logic). Arguments that occur in textual corpora are very rich, highly various, and incompletely understood. We identify several subproblems which must be addressed in order to bridge the gap, requiring the development of a computational foundation for argumentation coupled with natural language processing. For the computational foundation, we provide a direct semantics, a formal approach for argumentation, which is implemented and suitable to represent and reason with an associated natural language expression for defeasibility. It has attractive properties with respect to expressivity and complexity; we can reason by cases; we can structure higher level argumentation components such as cases and debates. With the implementation, we output experimental results which emphasise the importance of our efficient approach. To motivate our formal approach, we identify a range of issues found in other approaches. For the natural language processing, we adopt and adapt an existing controlled natural language (CNL) to interface with our computational theory of argumentation; the tool takes natural language input and automatically outputs expressions suitable for automated inference engines. A CNL, as a constrained fragment of natural language, helps to control variables, highlights key problems, and provides a framework to engineer solutions. The key adaptation incorporates the expression ‘it is usual that’, which is a plausibly ‘natural’ linguistic expression of defeasibility. This is an important, albeit incremental, step towards the incorporation of linguistic expressions of defeasibility; yet, by engineering such specific solutions, a range of other, relevant issues arise to be addressed. Overall, we can input arguments expressed in a controlled natural language, translate them to a formal knowledge base, represent the knowledge in a rule language, reason with the rules, generate argument extensions, and finally convert the arguments in the extensions into natural language. Our approach makes for fine-grained, highly structure, accessible, and linguistically represented argumentation evaluation. The overall novel contribution of the paper is an integrated, end-to-end argumentation system which bridges a gap between automated defeasible reasoning and a natural language interface. The component novel contributions are the computational theory of ‘direct semantics’, the motivation for our theory, the results with respect to the direct semantics, the implementation, the experimental results, the tie between the formalisation and the CNL, the adaptation of a CNL defeasibility, and an ‘engineering’ approach to fine-grained argument analysis.}
}
@article{MO2021453,
title = {Incorporating sentimental trend into gated mechanism based transformer network for story ending generation},
journal = {Neurocomputing},
volume = {453},
pages = {453-464},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.01.040},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221000618},
author = {Linzhang Mo and Jielong Wei and Qingbao Huang and Yi Cai and Qingguang Liu and Xingmao Zhang and Qing Li},
keywords = {Story ending generation, Sentiment trend, Transformer, Gated mechanism},
abstract = {Story ending generation is a challenging and under-explored task, which aims at generating a coherent, reasonable, and logical story ending given a context. Previous studies mainly focus on utilizing the contextual information and commonsense knowledge to generate story endings. However, there are still some issues must be addressed in the story endings generation processing, such as sentimental consistency and interference from secondary information. In this paper, we propose a Gated Mechanism based Transformer Network (GMTF). The GMTF model utilizes the sentimental trend to make story ending generation more sentimentally consistent with the context. For a given story context, we utilize a sentiment analysis tool VADER to obtain the sentimental trend. Then, the sentimental information and contextual information are input jointly into the transformer network to capture the key clues. Furthermore, the gated mechanism is applied to filter irrelative information and the weights of attention layers for encoder and decoder are shared to make the most of the contextual clues. The experimental results on ROCStories dataset demonstrate that the proposed method achieves 27.03% on BLEU-1, 7.62% on BLEU-2, 1.71 on Grammar, and 1.31 on Logicality, respectively. Specifically, our model outperforms the state-of-the-art model IE+MSA by 0.23%, 0.22%, 1.78%, 5.64%, respectively and the Transformer model by 3.06%, 1.05%, 5.55%, 48.86%, respectively. Both automatic and manual evaluations show that our model can generate more reasonable and appropriate story endings compared with the related well-established approaches.}
}
@article{ARRUDA2020102377,
title = {Automation and consistency analysis of test cases written in natural language: An industrial context},
journal = {Science of Computer Programming},
volume = {189},
pages = {102377},
year = {2020},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2019.102377},
url = {https://www.sciencedirect.com/science/article/pii/S0167642319301698},
author = {Filipe Arruda and Flávia Barros and Augusto Sampaio},
keywords = {Test automation, Controlled natural language, Alloy, Test case consistency},
abstract = {We present here a novel test automation strategy that receives as input a freestyle natural language (NL) test case (consisting of a sequence of test steps) and produces executable test scripts. This strategy relies on a database of previously automated seed test steps, available for reuse. New steps are automated via a capturing process by a tester, without requiring any programming knowledge. Automated tests can be executed by a replay facility. We discuss the reuse improvement, implementation effort, and user feedback regarding the industrial applicability and usability of our capture & replay tool. We then show that restricting the input textual description to obey a proposed Controlled NL (CNL) brings significant advantages: (1) reuse improvement; (2) the possibility of integration with a test generation framework; and (3) definition of consistency notions for test actions and test action sequences, that ensure, respectively, well-formedness of each action and a proper configuration to safely execute a sequence of actions. We formalize these consistency notions in Alloy and use the Alloy Analyzer to carry out the consistency check; the scalability of the analysis is assessed via an evaluation considering a repository with real test cases; the practical context of our work is mobile device testing, involving a partnership with Motorola Mobility, a Lenovo company.}
}
@article{BIRD2020113402,
title = {Optimisation of phonetic aware speech recognition through multi-objective evolutionary algorithms},
journal = {Expert Systems with Applications},
volume = {153},
pages = {113402},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113402},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420302268},
author = {Jordan J. Bird and Elizabeth Wanner and Anikó Ekárt and Diego R. Faria},
keywords = {Speech recognition, Phoneme classification, Applied hyperheuristics, Multi-objective evolutionary computation},
abstract = {Recent advances in the availability of computational resources allow for more sophisticated approaches to speech recognition than ever before. This study considers Artificial Neural Network and Hidden Markov Model methods of classification for Human Speech Recognition through Diphthong Vowel sounds in the English Phonetic Alphabet rather than the classical approach of the classification of whole words and phrases, with a specific focus on both single and multi-objective evolutionary optimisation of bioinspired classification methods. A set of audio clips are recorded by subjects from the United Kingdom and Mexico and the recordings are transformed into a static dataset of statistics by way of their Mel-Frequency Cepstral Coefficients (MFCC) at sliding window length of 200ms as well as a reshaped MFCC timeseries format for forecast-based models. An deep neural network with evolutionary optimised topology achieves 90.77% phoneme classification accuracy in comparison to the best HMM that achieves 86.23% accuracy with 150 hidden units, when only accuracy is considered in a single-objective optimisation approach. The obtained solutions are far more complex than the HMM taking around 248 seconds to train on powerful hardware versus 160 for the HMM. A multi-objective approach is explored due to this. In the multi-objective approaches of scalarisation presented, within which real-time resource usage is also considered towards solution fitness, far more optimal solutions are produced which train far quicker than the forecast approach (69 seconds) with classification ability retained (86.73%). Weightings towards either maximising accuracy or reducing resource usage from 0.1 to 0.9 are suggested depending on the resources available, since many future IoT devices and autonomous robots may have limited access to cloud resources at a premium in comparison to the GPU used in this experiment.}
}
@article{MONACO2022127,
title = {Linked open data in authoring virtual exhibitions},
journal = {Journal of Cultural Heritage},
volume = {53},
pages = {127-142},
year = {2022},
issn = {1296-2074},
doi = {https://doi.org/10.1016/j.culher.2021.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S1296207421001667},
author = {Daniele Monaco and Maria Angela Pellegrino and Vittorio Scarano and Luca Vicidomini},
keywords = {Virtual exhibitions, Linked open data, Knowledge graph, Virtual reality, Query builder, Natural language interface},
abstract = {In the last years, virtual exhibitions have been widely adopted to enhance traditional museums and enable active interaction with culture without posing any physical constraints. Nevertheless, people interested in cultural heritage still behave as visitors. To fully engage them, we propose to let cultural heritage lovers play the role of exhibition curators. In authoring virtual exhibitions, users have to perform a data selection phase that poses several challenges, including finding data sources and extracting data of interest. We aim to take advantage of data published as Knowledge Graphs in the Linked Open Data format. Users can query geographically distributed artworks thanks to their linking nature, manipulate heterogeneous data, and easily customise their exhibitions by exploiting the wide range of available cultural heritage knowledge graphs. However, the complexity of linked open data query languages (such as SPARQL) threatens their exploitation. Consequently, we need to mask SPARQL technical challenges and guide users in naturally posing questions to unlock the potentialities of linked open data to a broader audience. We propose a virtual exhibition authoring tool that guides users from knowledge graphs querying to the automatic generation of virtual experiences. The Knowledge Graph query phase relies on ELODIE, a natural language interface to scaffold users in retrieving data of interest without asking for technical skills in query languages. We introduce our prototype by describing its operating mechanism and by detailing its components. We present a Van Gogh’s experience as a use case by collecting all the artist’s artworks published on DBpedia (a well-known and general purpose knowledge graph) and organise them in a virtual reality-based virtual exhibition. Finally, we conclude by overviewing advantages and technical challenges posed by linked open data in designing and developing knowledge graph exploitation tools.}
}
@article{CORNELIO201982,
title = {Guest editorial for the special issue from the 18th Brazilian Symposium on Formal Methods (SBMF 2015)},
journal = {Science of Computer Programming},
volume = {181},
pages = {82-83},
year = {2019},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2019.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167642319300747},
author = {M.L. Cornélio and A.W. Roscoe}
}
@article{DELANEY2022107836,
title = {SiteFinder: A geospatial scoping tool to assist the siting of external water harvesting structures},
journal = {Agricultural Water Management},
volume = {272},
pages = {107836},
year = {2022},
issn = {0378-3774},
doi = {https://doi.org/10.1016/j.agwat.2022.107836},
url = {https://www.sciencedirect.com/science/article/pii/S0378377422003833},
author = {R.G. Delaney and G.A. Blackburn and J.D. Whyatt and A.M. Folkard},
keywords = {Water harvesting, GIS, Digital elevation model, Remote sensing, Drylands},
abstract = {Water harvesting has a long history, but still plays an important role today by increasing crop productivity, combatting erosion, and improving water supplies. Geographical Information Systems (GIS) are used extensively to assess the suitability of sites for water harvesting but available tools fail to consider the synoptic topography of sites. Here, we report the creation of a novel, automated tool – “SiteFinder” – that evaluates potential locations by automatically calculating site-specific information, including structure parameters (height, length, and volume) and descriptors of the zone affected by the structure (storage capacity and area of influence) and the catchment area. Innovatively, compared to existing tools of this kind, SiteFinder works within a GIS environment. Thus, it allows the possibility of combining its outputs with larger Multi-Criteria Decision-Making processes to consider other bio-physical, socio-economic, and environmental factors. It utilises a Digital Elevation Model (DEM) and automatically analyses thousands of potential sites, computing site characteristics for different barrier heights that are dependent on the surrounding topography. It outputs values of eight parameters to aid planners in assessing the characteristics of sites as to their suitability for water harvesting. We conducted case studies using 30 × 30 m gridded DEMs to automatically evaluate several thousand sites and, by filtering the tool outputs, successfully identified sites with characteristics appropriate for scenarios at three spatial scales: large dams for nationally significant water supply reservoirs (383 sites analysed; 5 filtered sites with barriers up to 30 m in height); large gully erosion control dams for regional-scale interventions (4,586 sites analysed; 6 filtered sites with barriers up to 3.6 m in height); and local, community-based earth embankment projects (801 sites analysed; 6 filtered sites with barriers up to 2 m in height). A higher resolution (1 × 1 m) terrain elevation model, derived from open-source airborne survey data, was used to assess the veracity of these results. Correlations between the barrier length, impounded area and storage volume capacity derived from the two different resolution data sets were all strongly significant (Spearman’s rank correlation, p < 0.001); and normalised root mean square errors were 9%, 15% and 16% for these parameters, respectively.}
}
@article{SHOLIQ202210079,
title = {Generating BPMN diagram from textual requirements},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {10, Part B},
pages = {10079-10093},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822003585},
author = {Sholiq Sholiq and Riyanarto Sarno and Endang Siti Astuti},
keywords = {BPMN diagram, Natural language, Textual requirement},
abstract = {An interesting challenge in software requirements engineering is converting textual requirements to Business Process Model and Notation (BPMN) diagrams. In this study, the BPMN diagram is used as an intermediate representation before measuring the functional software size from Natural Language (NL) input. The methods currently used for converting NL input to BPMN diagrams are not able to generate complete BPMN diagrams, nor can they handle complex and compound-complex sentences in the NL input. This study proposes conversion from textual requirements to a BPMN diagram for improving the weaknesses of existing methods. The proposed method has two stages: 1) analyzing the textual requirements using natural language processing and 2) generating the BPMN diagram. The output of the first stage is fact types as the basis for generating the BPMN diagram in the second phase. The BPMN diagram is generated using a set of informal mapping rules that were created in this study. The proposed method was applied to ten textual requirements of an enterprise application, which involved simple, compound, complex, and compound-complex sentences. The experiments resulted in a suitable BPMN diagram with higher accuracy than obtained by other methods.}
}
@article{BARBA2021114857,
title = {Flexible runtime support of business processes under rolling planning horizons},
journal = {Expert Systems with Applications},
volume = {177},
pages = {114857},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114857},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421002980},
author = {Irene Barba and Andrés Jiménez-Ramírez and Manfred Reichert and Carmelo {Del Valle} and Barbara Weber},
keywords = {Process flexibility, Rolling planning horizon, Declarative process, Healthcare process},
abstract = {This work has been motivated by the needs we discovered when analyzing real-world processes from the healthcare domain that have revealed high flexibility demands and complex temporal constraints. When trying to model these processes with existing languages, we learned that none of the latter was able to fully address these needs. This motivated us to design TConDec-R, a declarative process modeling language enabling the specification of complex temporal constraints. Enacting business processes based on declarative process models, however, introduces a high complexity due to the required optimization of objective functions, the handling of various temporal constraints, the concurrent execution of multiple process instances, the management of cross-instance constraints, and complex resource allocations. Consequently, advanced user support through optimized schedules is required when executing the instances of such models. In previous work, we suggested a method for generating an optimized enactment plan for a given set of process instances created from a TConDec-R model. However, this approach was not applicable to scenarios with uncertain demands in which the enactment of newly created process instances starts continuously over time, as in the considered healthcare scenarios. Here, the process instances to be planned within a specific timeframe cannot be considered in isolation from the ones planned for future timeframes. To be able to support such scenarios, this article significantly extends our previous work by generating optimized enactment plans under a rolling planning horizon. We evaluate the approach by applying it to a particularly challenging healthcare process scenario, i.e., the diagnostic procedures required for treating patients with ovarian carcinoma in a Woman Hospital. The application of the approach to this sophisticated scenario allows avoiding constraint violations and effectively managing shared resources, which contributes to reduce the length of patient stays in the hospital.}
}
@article{DVORAK2022102747,
title = {Tackling rapid technology changes by applying enterprise engineering theories},
journal = {Science of Computer Programming},
volume = {215},
pages = {102747},
year = {2022},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2021.102747},
url = {https://www.sciencedirect.com/science/article/pii/S0167642321001404},
author = {Ondřej Dvořák and Robert Pergl},
keywords = {ADA, Component-based systems, Enterprise engineering, Technology acceleration, Evolvability},
abstract = {Moore's law states that the number of transistors on a chip will double every two years. A similar force appears to drive the progress of information technology (IT). IT companies tend to struggle to keep up with the latest technological developments, and software solutions are becoming increasingly outdated. The ability for software to change easily is defined as evolvability. One of the major fields researching evolvability is enterprise engineering (EE). The EE research paradigm applies theories from other fields to the evolvability of organisations. We argue that such theories can be applied to software engineering (SE) as well, which can contribute to the construction of software with a clear separation of dynamically changing technologies based on a relatively stable description of functions required for a specific user. EE theories introduce notions of function, construction, and affordance. We reify these concepts in terms of SE. Based on this reification, we propose affordance-driven assembling (ADA) as a software design approach that can aid in the construction of more evolvable software solutions. We exemplify the implementation of ADA in a case study on a commercial system and measure its effectiveness in terms of the impact of changes, as defined by the normalised systems theory.}
}
@article{IQBAL201873,
title = {A mathematical evaluation for measuring correctness of domain ontologies using concept maps},
journal = {Measurement},
volume = {118},
pages = {73-82},
year = {2018},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2018.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0263224118300083},
author = {Rizwan Iqbal and Masrah Azrifah {Azmi Murad} and Layth Sliman and Clay Palmeira {da Silva}},
keywords = {Ontology engineering, Concept mapping, Ontology evaluation, Closeness index, Similarity index},
abstract = {There is a need for further research in the area of ontology evaluation specifically dealing with ontology development exploiting concept maps. The existing literature on ontology evaluation primarily emphasis on ontology formalisation as well as on performing logical inferences, which is usually not directly relevant for concept maps as they are commonly exploited as communication instruments for learning purposes. Commonly used techniques for evaluating concept maps for knowledge assessment may be adopted for a kind of criteria-based evaluation of a domain concept map with respect to a particular aspect. However, this makes its validity limited to a particular aspect or criteria. This paper presents a mathematical ontology evaluation technique to measure the correctness of domain ontologies engineered using concept maps. It is based on the notion of merging two different mathematical measures, namely closeness index and similarity index to come up with a combined index that takes different criteria or aspects into account while performing ontology evaluation. Therefore, the proposed technique makes the evaluation process more reliable and robust. Two case studies were conducted employing the proposed technique for evaluating two different domain ontologies that were engineered using concept maps. Calculations and results from the case studies showed that depending on the correctness of individual ontology, different values of combined Index was calculated manifesting the measure of correctness of each individual ontology in a quantifiable form. Moreover, the results depict that the technique provides in-depth evaluation, it is easy to adopt, requires no special skills, and is conveniently replicable.}
}
@article{LIM2024112005,
title = {Test case information extraction from requirements specifications using NLP-based unified boilerplate approach},
journal = {Journal of Systems and Software},
volume = {211},
pages = {112005},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112005},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224000487},
author = {Jin Wei Lim and Thiam Kian Chiew and Moon Ting Su and Simying Ong and Hema Subramaniam and Mumtaz Begum Mustafa and Yin Kia Chiam},
keywords = {Natural language processing, Test case generation, Automation, Software requirements, Software Testing, Test Case},
abstract = {Automated testing which extracts essential information from software requirements written in natural language offers a cost-effective and efficient solution to error-free software that meets stakeholders’ requirements in the software industry. However, natural language can cause ambiguity in requirements and increase the challenges of automated testing such as test case generation. Negative requirements also cause inconsistency and are often neglected. This research aims to extract test case information (actors, conditions, steps, system response) from positive and negative requirements written in natural language (i.e. English) using natural language processing (NLP). We present a unified boilerplate that combines Rupp's and EARS boilerplates, and serves as the grammar guideline for requirements analysis. Extracted information is populated in a test case template, becoming the building blocks for automated test case generation. An experiment was conducted with three public requirements specifications from PURE datasets to investigate the correctness of information extracted using this proposed approach. The results presented correctness of 50 % (Mdot), 61.7 % (Pointis) and 10 % (Npac) on information extracted. The lower correctness on negative over positive requirements was observed. The correctness by specific categories is also analysed, revealing insights into actors, steps, conditions, and system response extracted from positive and negative requirements.}
}
@article{BLANCO20221341,
title = {Human-Generated Web Data Disentanglement for Complex Event Processing},
journal = {Procedia Computer Science},
volume = {207},
pages = {1341-1349},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.190},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922010730},
author = {José Miguel Blanco and Mouzhi Ge and Tomáš Pitner},
keywords = {Complex Event Processing, Semantic Web, Data Disentanglement, Web Data Preprocessing},
abstract = {In social media, human-generated web data from real-world events have become exponentially complex due to the chaotic and spontaneous features of natural language. This may create an information overload for the information consumers, and in turn not easily digest a large amount of information in a limited time. To tackle this issue, we propose to use Complex Event Processing (CEP) and semantic web reasoners to disentangle the human-generated data and present users with only relevant and important data. However, one of the key obstacles is that the human-generated data can have no structured meaning sometimes even for the speaker, hindering the output of the CEP. Therefore, in order to adapt to the CEP inputs, we present two different techniques that allow for the discrimination and digestion of value of human-generated data. The first one relies on the Variable Sharing Property that was developed for relevance logics, while the second one is based on semantic equivalence and natural language processing. The results can be given to CEP for further semantic reasoning and generate digested information for users.}
}
@article{LAGO2021101324,
title = {Managing non-trivial internet-of-things systems with conversational assistants: A prototype and a feasibility experiment},
journal = {Journal of Computational Science},
volume = {51},
pages = {101324},
year = {2021},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2021.101324},
url = {https://www.sciencedirect.com/science/article/pii/S1877750321000223},
author = {André Sousa Lago and João Pedro Dias and Hugo Sereno Ferreira},
keywords = {Internet-of-Things, Conversational assistants, Software engineering, Natural language processing, Visual programming},
abstract = {Internet-of-Things has reshaped the way people interact with their surroundings and automatize the once manual actions. In a smart home, controlling the Internet-connected lights is as simple as speaking to a nearby conversational assistant. However, specifying interaction rules, such as making the lamp turn on at specific times or when someone enters the space is not a straightforward task. The complexity of doing such increases as the number and variety of devices increases, along with the number of household members. Thus, managing such systems becomes a problem, including finding out why something has happened. This issue lead to the birth of several low-code development solutions that allow users to define rules to their systems, at the cost of discarding the easiness and accessibility of voice interaction. In this paper we extend the previous published work on Jarvis [1], a conversational interface to manage IoT systems that attempts to address these issues by allowing users to specify time-based rules, use contextual awareness for more natural interactions, provide event management and support causality queries. A proof-of-concept is presented, detailing its architecture and natural language processing capabilities. A feasibility experiment was carried with mostly non-technical participants, providing evidence that Jarvis is intuitive enough to be used by common end-users, with participants showcasing an overall preference by conversational assistants over visual low-code solutions.}
}
@article{ABDALAZEIM2021328,
title = {A review of the generation of requirements specification in natural language using objects UML models and domain ontology},
journal = {Procedia Computer Science},
volume = {189},
pages = {328-334},
year = {2021},
note = {AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.05.102},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921012266},
author = {Alaa Abdalazeim and Farid Meziane},
keywords = {Requirements Specification, Natural Language Generation, Object UML Model, Ontology},
abstract = {In the software development life cycle, requirements engineering is the main process that is derived from users by informal interviews written in natural language by requirements engineers (analysts). The requirements may suffer from incompleteness and ambiguity when transformed into formal or semi-formal models that are not well understood by stakeholders. Hence, the stakeholder cannot verify if the formal or semi-formal models satisfy their needs and requirements. Another problem faced by requirements is that when code and/or designs are updated, it is often the case that requirements and specifically the requirements document are not updated. Hence ending with a requirements document not reflecting the implemented software. Generating requirements from the design and/or implementation document is seen by many researchers as a way to address the latter issue. This paper presents a survey of some works undertaken in the field of generation natural language specifications from object UML model using the support of an ontology. and analyzing the robustness and limitations of these existing approaches. This includes studying the generation of natural language from a formal model, review the generation of natural language from ontologies, and finally reviews studies about check to generate natural language from OntoUML.}
}
@article{GOMEZ2019100297,
title = {Digital storytelling for good with Tappetina game},
journal = {Entertainment Computing},
volume = {30},
pages = {100297},
year = {2019},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2019.100297},
url = {https://www.sciencedirect.com/science/article/pii/S1875952118300909},
author = {Javier Gomez and Letizia Jaccheri and Manolis Maragoudakis and Kshitij Sharma},
keywords = {Digital storytelling, Data analysis, Machine Learning, Games for Good},
abstract = {Context
Storytelling is an important asset in today’s society. Digital platforms for storytelling can facilitate collaborative development of stories. The storytelling process, if properly facilitated, can lead to the creation of stories that improve the relations between the players. Moreover, stories convey important information about the players and their interaction. Extended knowledge and better tools are needed about how to facilitate storytelling for good and analysis to exploit the power of the generated data.
Research question
How to facilitate Digital Storytelling for good?
Method
The investigation is based on a case study approach in which participants have been engaged in the creation of stories. The study is based on empirical data collection and analysis: from the stories recorded, we extract the storytelling features and performance. We have provided qualitative (Domain Expert) and quantitative (Machine Learning) analysis of the stories. In total, 58 users played the game in 15 sessions.
Results and conclusions
The main result is a framework for analysing digital stories. The analysis gives an indication of which game building blocks lead to stories for good. Future work will include a redesign of the game and its building blocks which lead to stories for good and further analyses.}
}
@article{CABALLERO2022102058,
title = {BR4DQ: A methodology for grouping business rules for data quality evaluation},
journal = {Information Systems},
volume = {109},
pages = {102058},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2022.102058},
url = {https://www.sciencedirect.com/science/article/pii/S0306437922000485},
author = {Ismael Caballero and Fernando Gualo and Moisés Rodríguez and Mario Piattini},
keywords = {Business rules, Data quality, Data quality evaluation, Data quality measurement, Data quality characteristics, Data quality properties, ISO/IEC 25012, ISO/IEC 25024},
abstract = {Data quality evaluation is built upon data quality measurement results. “Data quality evaluation” uses the “data quality rules” representing the risk appetite of the organization to decide on the usability of the data; “data quality measurement” uses the business rules describing the “data requirements” or “data specifications” to determine the validity of the data. Consequently, to conduct meaningful and useful data quality evaluations, business rules must be first completely identified and captured at the beginning of the evaluation to perform sound measurements. We propose that the evaluation leads to better and more interpretable and useful results when the potential contribution of these business rules to the measurement of the data quality characteristics is first evaluated, avoiding the inclusion in the evaluation of those not having potential contribution and the resulting waste of resources. Considering this, we feel that for a better management of business rules for data quality evaluation, it makes sense to group all business rules having an important contribution to the evaluation of data quality characteristics, something that other business rules management methodologies have not covered yet. Through our experiences in conducting industrial projects of data quality evaluations we identified six problems when collecting and grouping the business rules. These problems make data quality evaluation processes less efficient and more costly. The main contribution of this paper is a methodology to systematically collect, group and validate the business rules to avoid or to alleviate these problems. For the sake of generalization, comparability, and reusability, we propose to do the grouping for data quality characteristics and properties defined in ISO/IEC 25012 and ISO/IEC 25024, respectively. Lastly, we validate the methodology in three case studies of real projects. From this validation, it is possible to raise the conclusion that the methodology is useful, applicable in the real world, and valid to capture and group the business rules used as a basis for data quality evaluation.}
}
@article{BUTTNER20222629,
title = {Applied Machine Learning for Production Planning and Control: Overview and Potentials},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {2629-2634},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.106},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322021152},
author = {Konstantin Büttner and Oliver Antons and Julia C. Arlinghaus},
keywords = {machine learning, production planning, control, production control},
abstract = {Manufacturing companies are under constant pressure to increase efficiency and to achieve logistical objectives. Improving production planning and control (PPC) has significant impact on these efforts. At the same time, increasing complexity and dynamics of PPC environments make PPC more difficult. One way to cope with this situation is the application of machine learning (ML) methods. In this article, we therefore address the current state of PPC-ML research and show, based on the Aachen PPC model, in which PPC tasks and subtasks ML is already applied and to what degree the task is covered by ML. The analysis is limited to core and cross-sectional tasks of the Aachen PPC model, procurement and network tasks are not included. Furthermore, a broad analysis of the targeted data mining, business and logistic objectives is conducted. In addition, we also identify motivations which prompted researchers to apply ML in PPC.}
}
@article{KACFAHEMANI2019130,
title = {NALDO: From natural language definitions to OWL expressions},
journal = {Data & Knowledge Engineering},
volume = {122},
pages = {130-141},
year = {2019},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2019.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X18306086},
author = {Cheikh {Kacfah Emani} and Catarina {Ferreira Da Silva} and Bruno Fiès and Parisa Ghodous},
keywords = {Ontologies, Natural language definitions, Ontology enrichment, OWL DL, Semantic web},
abstract = {Domain ontologies are pivotal for Semantic Web applications. The richness of an ontology goes in hand with its usefulness and efficiency. Unfortunately, manually enriching an ontology is very time-consuming. In this paper, we propose to enrich an ontology automatically by obtaining logical expressions of concepts. We present NALDO, a novel approach that provides an OWL DL (Web Ontology Language Description Logics) expression of a concept from two inputs: (1) the natural language definition of the concept and (2) an ontology describing the domain of this concept. NALDO uses as much as possible entities provided by the domain ontology, however it can suggest, when needed, new entities. The expressiveness of expressions provided by NALDO covers value and cardinality restrictions, subsumption and equivalence. We evaluate our approach against the definitions and the corresponding ontologies of the BEAUFORD benchmark. Our results show that NALDO is able to perform the correct identification of formal entities with an F1-measure up to 0.79.}
}
@article{JONEK20231203,
title = {Automated Analysis of Assembly Processes in Human-Robot Collaboration: Research Approaches and Challenges},
journal = {Procedia CIRP},
volume = {120},
pages = {1203-1208},
year = {2023},
note = {56th CIRP International Conference on Manufacturing Systems 2023},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.09.149},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123008818},
author = {Michael Jonek and Dario Niermann and Christoph Petzoldt and Martin Manns and Michael Freitag},
keywords = {Collaborative robot, Human-Machine Cooperation, Process-Planning, Process-Analysis, Digital Twin},
abstract = {Due to the increasing trends of individualization and variant diversity, cobotic production systems are becoming more important. However, cobots are hardly used in small and medium-sized enterprises (SME) because of a lack in expertise, experience or resources. In order to be able to assess whether the use of a cobot is reasonable, there are already many methods in research with different focuses such as ergonomics, productivity or economic efficiency. However, many methods are not targeted for use by SME, thus still require the involvement of experts, and often only consider economic efficiency without consideration of human-centered aspects. Furthermore, these are mostly not integrated into commonly used process planning tools. This work provides an overview of methods for automated analysis of assembly processes in HRC. We present a method for HRC process analysis with an individual weighting of fatigue, time, costs and safety that can be set depending on an application-specific focus defined by the user. It allows non-experts to perform a process analysis based on the individual weighting and a digital twin of the workstation. An evaluation method developed for this purpose calculates the benefit of a cobot in the analyzed process depending on the preset focus. To keep the effort low, the method includes a workflow to easily create a digital twin of the workplace with sufficient accuracy. In addition to the pre-estimation of the human-cobot process, the method allows an easy planning of collaborative production processes, which is why we suggest a conceptual integration into a process planning and control framework. The proposed approach is demonstrated in an use case by analyzing the potential for introduction of collaborative robots for the production of currently manually assembled solar inverters.}
}
@article{LEONG2024112009,
title = {Translating meaning representations to behavioural interface specifications},
journal = {Journal of Systems and Software},
volume = {211},
pages = {112009},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112009},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224000529},
author = {Iat Tou Leong and Raul Barbosa},
keywords = {Software engineering, Formal specification, Software verification, Java modelling language, Compilers},
abstract = {Higher-order logic can be used for meaning representation in natural language processing to encode the semantic relationships in text. Alternatively, using a formal specification language for meaning representation is more precise for specifying programs and widely supported by automatic theorem provers, while deductive verification based on higher order logic is less common for mainstream programming languages. This paper addresses the research question of translating higher-order logic meaning representations generated from method-level code comments into a formal specification language that extends first-order logic. Doing so requires resolving possible ambiguities in determining the appropriate semantics for predicates. This is an open challenge in the path toward using natural language processing with formal methods. To address this, the paper proposes an approach and constructs a compiler for translating meaning representations, generated from Java programs with method-level comments, into Java Modelling Language. We evaluate the compiler on a set of representative benchmarks, including programs and specifications from the Java API, by generating Java Modelling Language specifications and statically checking them with a theorem prover. Results show that in 94% of the cases Java Modelling Language is accurately generated and in 97% of those cases it can be automatically checked with a state-of-the-art theorem prover.}
}
@article{LOMBARDO201812,
title = {Semantics–informed geological maps: Conceptual modeling and knowledge encoding},
journal = {Computers & Geosciences},
volume = {116},
pages = {12-22},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2018.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0098300416302321},
author = {Vincenzo Lombardo and Fabrizio Piana and Dario Mimmo},
keywords = {Geologic knowledge encoding, Geologic unit ontology, Geodatabase, Geological map, Conceptual modeling of geologic knowledge, Automatic reasoning},
abstract = {This paper introduces a novel, semantics-informed geologic mapping process, whose application domain is the production of a synthetic geologic map of a large administrative region. A number of approaches concerning the expression of geologic knowledge through UML schemata and ontologies have been around for more than a decade. These approaches have yielded resources that concern specific domains, such as, e.g., lithology. We develop a conceptual model that aims at building a digital encoding of several domains of geologic knowledge, in order to support the interoperability of the sources. We apply the devised terminological base to the classification of the elements of a geologic map of the Italian Western Alps and northern Apennines (Piemonte region). The digitally encoded knowledge base is a merged set of ontologies, called OntoGeonous. The encoding process identifies the objects of the semantic encoding, the geologic units, gathers the relevant information about such objects from authoritative resources, such as GeoSciML (giving priority to the application schemata reported in the INSPIRE Encoding Cookbook), and expresses the statements by means of axioms encoded in the Web Ontology Language (OWL). To support interoperability, OntoGeonous interlinks the general concepts by referring to the upper part level of ontology SWEET (developed by NASA), and imports knowledge that is already encoded in ontological format (e.g., ontology Simple Lithology). Machine-readable knowledge allows for consistency checking and for classification of the geological map data through algorithms of automatic reasoning.}
}
@article{KAUR201956,
title = {Investigation on test effort estimation of mobile applications: Systematic literature review and survey},
journal = {Information and Software Technology},
volume = {110},
pages = {56-77},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S095058491930031X},
author = {Anureet Kaur and Kulwant Kaur},
keywords = {Mobile applications, Test effort estimation, Software engineering, Systematic literature review (SLR), Survey},
abstract = {Context
In the last few years, the exigency of mobile devices has proliferated to prodigious heights. The process of developing the mobile software/application proceeds amidst testing phase to verify the correctness of the mobile app. The estimation of testing plays a vital role in the effective completion of testing.
Objective
To identify how estimation of test effort for mobile applications is distinct from other software via published literature and from mobile software organizations. Second is to recognize different issues in adapting traditional test estimation methods to the mobile domain and if suggestions from survey results could be helpful in providing an improved test estimation model for mobile applications.
Method
A systematic literature review is conducted followed by a survey through an online questionnaire filled from experienced mobile application developers and testers.
Results
The results from SLR cover identification of mobile app specific characteristics and reports test effort estimation techniques in the mobile domain. Findings from survey corroborate that a) Function Point/Test Point Analysis is highly adapted traditional test estimation technique to mobile domain; b) Challenges like uncertain requirements, no tool support for test estimation, complexity in testing, client miscommunication etc. are reported; c)Suggestions to improve test estimation process include proper test planning, adoption of agile methodology, healthier communication among client, developer, and tester etc.; d) On the basis of responses, Analytical Hierarchical Process (AHP) identifies “Diverse Devices and OS” along with “Type of App” as highly influential mobile app characteristic on the test estimation process.
Conclusion
Results conclude that the importance of identified mobile app characteristics from SLR cannot be ignored in the estimation process of mobile software testing. There might be a possibility to improve existing test estimation techniques for mobile apps by giving weight to mobile app specific characteristics and by considering suggestions from experienced developers and testers.}
}
@article{MORGAN2023102205,
title = {Modelling temporal goals in runtime goal models},
journal = {Data & Knowledge Engineering},
volume = {147},
pages = {102205},
year = {2023},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102205},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23000654},
author = {Rebecca Morgan and Simon Pulawski and Matt Selway and Aditya Ghose and Georg Grossmann and Wolfgang Mayer and Markus Stumptner and Ross Kyprianou},
keywords = {Goal modelling, Self-adaptive systems, Context awareness},
abstract = {Achieving real-time agility and adaptation with respect to changing requirements in existing IT infrastructure can pose a complex challenge. We describe a goal-oriented approach to manage this complexity. We argue that a goal-oriented perspective can form an effective basis for devising and deploying responses to changed requirements at runtime. We offer an extended vocabulary of goal types by presenting two novel conceptions: differential goals and integral goals, which we formalize in both linear-time and branching-time settings. We describe goal lifecycles and interactions and the extended notion of context for the representation of rapidly changing, complex operating environments. We then illustrate the working of the approach by presenting a detailed scenario of adaptation in a Kubernetes setting, in the face of a Distributed Denial-of-Service (DDoS) attack.}
}
@article{COMPANY2023103486,
title = {A Functional Classification of Text Annotations for Engineering Design},
journal = {Computer-Aided Design},
volume = {158},
pages = {103486},
year = {2023},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2023.103486},
url = {https://www.sciencedirect.com/science/article/pii/S0010448523000180},
author = {Pedro Company and Jorge D. Camba and Stanislao Patalano and Ferdinando Vitolo and Antonio Lanzotti},
keywords = {Annotations, Model-based definition, Text annotations},
abstract = {Describing and supplementing geometric shapes (parts) and layouts (assemblies) with relevant information is key for successful product design communication. 3D annotation tools are widely available in commercial systems, but they are generally used in the same manner as 2D annotations in traditional engineering drawings. The gap between technology and practices is particularly evident in plain text annotations. In this paper, we introduce a functional classification of text annotations to provide an information framework for shifting traditional annotation practices towards the Model-Based Definition (MBD) paradigm. In our view, the current classification of dimensions, tolerances, symbols, notes, and text does not stress the inherent properties of two broader categories: symbols and text. Symbol-based annotations use a symbolic language (mostly standardized) such as Geometric Dimensioning and Tolerancing (GD&T) to provide precise information about the implications of geometric imperfections in manufacturing, whereas notes and text are based on non-standardized and unstructured plain text, and can be used to convey design information. We advocate that text annotations can be characterized in four different functional types (objectives, requirements, rationale, and intent), which should be classified as such when annotations are added to a model. The identification and definition of a formalized structure and syntax can enable the management of the annotations as separate entities, thus leveraging their individual features, or as a group to gain a global and collective view of the design problem. The proposed classification was tested with a group of users in a redesign task that involved a series of geometric changes to an annotated assembly model.}
}
@article{CITAK20214552,
title = {A note on the applications of artificial intelligence in the hospitality industry: preliminary results of a survey},
journal = {Procedia Computer Science},
volume = {192},
pages = {4552-4559},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.233},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921019724},
author = {Joanna Citak and Mieczysław L. Owoc and Paweł Weichbroth},
keywords = {Artificial Intelligence, Hotel Industry, Applications},
abstract = {Intelligent technologies are widely implemented in different areas of modern society but specific approaches should be applied in services. Basic relationships refer to supporting customers and people responsible for services offering for these customers. The aim of the paper is to analyze and evaluate the state-of-the art of artificial intelligence (AI) applications in the hospitality industry. Our findings show that the major deployments concern in-person customer services, chatbots and messaging tools, business intelligence tools powered by machine learning, and virtual reality & augmented reality. Moreover, we performed a survey (n = 178), asking respondents about their perceptions and attitudes toward AI, including its implementation within a hotel space. The paper attempts to discuss how the hotel industry can be motivated by potential customers to apply selected AI solutions. In our opinion, these results provide useful insights for understanding the phenomenon under investigation. Nevertheless, since the results are not conclusive, more research is still needed on this topic. Future studies may concern both qualitative and quantitative methods, devoted to developing models that: a) quantify the potential benefits and risks of AI implementations, b) determine and evaluate the factors affecting the AI adoption by the customers, and c) measure the user (guest) experience of the hotel services, fueled by AI-based technologies.}
}
@article{MACKEY2023102222,
title = {Mapping singly-linked rules to linear temporal logic formulas},
journal = {Information Systems},
volume = {117},
pages = {102222},
year = {2023},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2023.102222},
url = {https://www.sciencedirect.com/science/article/pii/S0306437923000583},
author = {Isaac Mackey and Jianwen Su},
keywords = {Business process management, Service provisioning, Runtime monitoring, Rules, Linear temporal logic, Violation detection},
abstract = {Business services are provided by enacting interrelated business processes. Service providers must ensure enactments comply with policies, regulations, and business rules, including rules with quantitative time constraints. Enforcing such rules at design-time may be too restrictive, so effective service provisioning includes expressing rules in a formal specification language and detecting violations of these rules at runtime. Many specification languages do not include quantitative time constraints; for languages with such constraints, it is often unknown if they have runtime monitors whose auxiliary data storage is of bounded size. In this paper, we formulate a technical model of services, a logic language with quantitative time constraints for specifying rules, and develop techniques for automatically generating monitors to detect rule violations. This approach involves two steps, translating: (1) rules to formulas in linear temporal logic (LTL) on finite traces, and (2) LTL formulas to finite state machines. Since algorithms exist for step (2), we focus on step (1), i.e., mapping rules to equivalent LTL formulas. We present and establish the correctness of two translation techniques for “singly-linked” rules. We also compare the size of formulas produced by these techniques with a method of translation derived from Kamp’s Theorem, showing an improvement from hyper-exponential to exponential size.}
}
@article{GENNARI2022100359,
title = {The evolution of a toolkit for smart-thing design with children through action research},
journal = {International Journal of Child-Computer Interaction},
volume = {31},
pages = {100359},
year = {2022},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2021.100359},
url = {https://www.sciencedirect.com/science/article/pii/S2212868921000660},
author = {Rosella Gennari and Maristella Matera and Alessandra Melonio and Mehdi Rizvi and Eftychia Roumelioti},
keywords = {Smart thing, Toolkit, Action research, Card, Game, Design, Reflection, Child},
abstract = {Several workshops use toolkits to engage children in the design of smart things, that is, everyday things like toys enhanced with computing devices and capabilities. In general, the toolkits focus on one design stage or another, e.g., ideation or programming. Few toolkits are created to guide children through an entire design process. This paper presents a toolkit for smart-thing design with children. It revolves around SNaP, a card-based board game for children. The toolkit serves to frame the entire design process and guide them through their exploration, ideation, programming and prototyping of their own smart things. By embracing action research, the toolkit was adopted in actions with children, namely, design workshops. Results of actions were reflected over by considering children’s benefits, and they were used to make the toolkit evolve across cycles of action, reflection and development. The paper reports on the latest evolution cycles, ending with the 2020 cycle for continuing smart-thing design during COVID-19 times. The paper concludes with general reflections concerning action research and design with children, toolkits for framing smart-thing design with children, on-going and future work.}
}
@article{VALENCIAPARRA2021113450,
title = {DMN4DQ: When data quality meets DMN},
journal = {Decision Support Systems},
volume = {141},
pages = {113450},
year = {2021},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113450},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620302050},
author = {Álvaro Valencia-Parra and Luisa Parody and Ángel Jesús Varela-Vaca and Ismael Caballero and María Teresa Gómez-López},
keywords = {Data usability, Data quality, Decision model and notation, Data quality rule, Data quality assessment, Data quality measurement},
abstract = {To succeed in their business processes, organizations need data that not only attains suitable levels of quality for the task at hand, but that can also be considered as usable for the business. However, many researchers ground the potential usability of the data on its quality. Organizations would benefit from receiving recommendations on the usability of the data before its use. We propose that the recommendation on the usability of the data be supported by a decision process, which includes a context-dependent data-quality assessment based on business rules. Ideally, this recommendation would be generated automatically. Decision Model and Notation (DMN) enables the assessment of data quality based on the evaluation of business rules, and also, provides stakeholders (e.g., data stewards) with sound support for the automation of the whole process of generation of a recommendation regarding usability based on data quality. The main contribution of the proposal involves designing and enabling both DMN-driven mechanisms and a guiding methodology (DMN4DQ) to support the automatic generation of a decision-based recommendation on the potential usability of a data record in terms of its level of data quality. Furthermore, the validation of the proposal is performed through the application of a real dataset.}
}
@article{SAHAY2019736,
title = {CyberShip-IoT: A dynamic and adaptive SDN-based security policy enforcement framework for ships},
journal = {Future Generation Computer Systems},
volume = {100},
pages = {736-750},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.05.049},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1930367X},
author = {Rishikesh Sahay and Weizhi Meng and D.A. Sepulveda Estay and Christian D. Jensen and Michael Bruhn Barfod},
keywords = {Internet-of-Things, OpenFlow, SDN, Ship system, Policy language and enforcement, SCADA system},
abstract = {With the wide adoption of Information and Communication Technology (ICT) in the marine environment, ship systems are increasingly similar to other networked computing systems. The integration of positioning systems with navigational and propulsion control systems and the increasing reliance on Supervisory Control And Data Acquisition (SCADA) systems for monitoring the ship’s performance makes modern ships vulnerable to a wide range of cyber security issues. Moreover, frequent or permanent onshore connection makes the ship’s communication network a potential target for cyber-criminals. Such attacks can incapacitate the vessel, i.e., through a ransomware attack, or greatly degrade the performance of the ship systems, i.e., causing delays in the propagation of control messages between critical components within the ship. Furthermore, crew members and marine engineers are challenged with the task of configuring security policies for networked devices, using low-level device specific syntax, which is an error prone and time consuming process. In addition to this, crew members must also be familiar with the specific syntax for low-level network management task, which exacerbates the problem. The emergence of Software-Defined Networking (SDN) helps reduce the complexity of the network management tasks and we believe that a similar approach may be used to address the larger problem. We therefore propose the CyberShip-IoT framework to provide a network level defense for the communication network component of ship systems. CyberShip-IoT offers a high-level policy language and a translation mechanism for automated policy enforcement in the ship’s communication network. The modular design of the framework provides flexibility to deploy detection mechanism according to their requirements. To evaluate the feasibility and effectiveness of this framework, we develop a prototype for a scenario involving the communication network of a typical ship. The experimental results demonstrate that our framework can effectively translate high-level security policies into OpenFlow rules of the switches without incurring much latency, ultimately leading to efficient attack mitigation and reduced collateral damage.}
}
@article{KHABAROV20221899,
title = {An impact of ontology-based service-oriented ecosystems on digital transformation of railway transport and engineering education},
journal = {Transportation Research Procedia},
volume = {63},
pages = {1899-1908},
year = {2022},
note = {X International Scientific Siberian Transport Forum — TransSiberia 2022},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2022.06.210},
url = {https://www.sciencedirect.com/science/article/pii/S2352146522004653},
author = {Valeriy Khabarov and Irina Volegzhanina},
keywords = {railway transport, intelligent tutoring agent, education course ontology, industry-related university, service-oriented business ecosystem, digital transformation},
abstract = {The scientific novelty of the article is to ground the digital transformation of railway universities through the development of an ontology-based service-oriented ecosystem. The analysis of scientific publications and results of technical and technological initiatives in the field of railway transport revealed that the industry business ecosystem based on ontologies would determine the nature of digital transformation in railway universities. This idea, however, has not yet been sufficiently developed in education studies. It is obvious that ontologisation will require new conceptual solutions and tools to be implemented. The authors propose a new didactic concept called "knowledge factory". The implementation of this didactic concept requires the development of ontology-based didactic tools. In particular, standardisation of education content through the concepts and relations in ontologies makes it possible to develop a web-application such as an intelligent tutoring agent. The study involved two stages. The first stage - conceptual - grounded the new "knowledge factory" didactic concept and the ontological model of interaction between the railway industry and railway universities. For this purpose, the authors applied the methods of ecosystem and ontological-semantic approaches. At the second stage - practical - an attempt was made to develop a prototype of an intelligent tutoring agent. With this purpose a fragment of ontology for the education course "Artificial Intelligence Systems in Transport: Agent-Based Approach" was developed. The top-down method was applied to develop this ontology. What is also emphasized is that the findings are to a large extent transferable to many industries.}
}
@article{ONISHI20231161,
title = {Temporal relation identification in functional requirements},
journal = {Procedia Computer Science},
volume = {225},
pages = {1161-1170},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.104},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923012620},
author = {Maiko Onishi and Shinpei Ogata and Kozo Okano and Daisuke Bekki},
keywords = {Software Engineering, Natural Language Processing, Temporal Relation Identification, Requirements Specification},
abstract = {In this study, we propose a method for applying a temporal relation identification model to functional requirements. We discuss the limited availability of data in the requirements engineering domain compared to other fields when used for supervised learning, and therefore employ a corpus from the news domain for training. The experimental results demonstrate that the types of temporal relations present in functional requirements are limited, indicating that focusing on learning with a narrowed set of labels is effective. Additionally, We incorporate Dependency Path (DP) into the temporal relation identification model and report, through comparative experiments, that leveraging DP is effective, but minor modifications to DP do not lead to significant improvements in accuracy. By demonstrating specific application methods of temporal relation identification in requirements engineering, we anticipate contributing to the analysis of functional requirements in software development.}
}
@article{AMNA2022106824,
title = {Ambiguity in user stories: A systematic literature review},
journal = {Information and Software Technology},
volume = {145},
pages = {106824},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106824},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922000040},
author = {Anis R. Amna and Geert Poels},
keywords = {Requirements engineering, Agile software development, User story, Ambiguity, Systematic Literature Review},
abstract = {Context
Ambiguity in user stories is a problem that has received little research attention. Due to the absence of review studies, it is not known how and to what extent this problem, which impacts the effectiveness of user stories in supporting systems development, has been solved.
Objectives
We review the studies that investigate or develop solutions for problems related to ambiguity in user stories. We investigate how these problems manifest themselves, what their causes and consequences are, what solutions have been proposed and what evidence of their effectiveness has been presented. Based on the insights we obtain from this review, we identify research gaps and suggest opportunities for future research.
Methods
We followed Systematic Literature Review guidelines to review problems investigated, solutions proposed, and validation/evaluation methods used. We classified the reviewed studies according to the four linguistic levels of ambiguity (i.e., lexical, syntactic, semantic, pragmatic) proposed by Berry and Kamsties to obtain insights from patterns that we observe in the classification of problems and solutions.
Results
A total of 36 studies published in 2001–2020 investigated ambiguity in user stories. Based on four patterns we discern, we identify three research gaps. First, we need more research on human behaviors and cognitive factors causing ambiguity. Second, ambiguity is seldom studied as a problem of a set of related user stories, like a theme or epic in Scrum. Third, there is a lack of holistic solution approaches that consider ambiguity at multiple linguistic levels.
Conclusion
Ambiguity in user stories is a known problem. However, a comprehensive solution for addressing ambiguity in a set of related user stories as it manifests itself at different linguistic levels as a cognitive problem is lacking.}
}
@article{SCHEUER2022127780,
title = {A trait-based typification of urban forests as nature-based solutions},
journal = {Urban Forestry & Urban Greening},
volume = {78},
pages = {127780},
year = {2022},
issn = {1618-8667},
doi = {https://doi.org/10.1016/j.ufug.2022.127780},
url = {https://www.sciencedirect.com/science/article/pii/S1618866722003235},
author = {Sebastian Scheuer and Jessica Jache and Martina Kičić and Thilo Wellmann and Manuel Wolff and Dagmar Haase},
keywords = {Urban forest, Nature-based solution, Typology, Trait-based modelling, Semantics, Ontology},
abstract = {Urban forests as nature-based solutions (UF-NBS) are important tools for climate change adaptation and sustainable development. However, achieving both effective and sustainable UF-NBS solutions requires diverse knowledge. This includes knowledge on UF-NBS implementation, on the assessment of their environmental impacts in diverse spatial contexts, and on their management for the long-term safeguarding of delivered benefits. A successful integration of such bodies of knowledge demands a systematic understanding of UF-NBS. To achieve such an understanding, this paper presents a conceptual UF-NBS model obtained through a semantic, trait-based modelling approach. This conceptual model is subsequently implemented as an extendible, re-usable and interoperable ontology. In so doing, a formal, trait-based vocabulary on UF-NBS is created, that allows expressing spatial, morphological, physical, functional, and institutional UF-NBS properties for their typification and a subsequent integration of further knowledge and data. Thereby, ways forward are opened for a more systematic UF-NBS impact assessment, management, and decision-making.}
}
@article{BINDER2022501,
title = {Extensible Worker Assistance (EWA): Presenting a Comprehensive Framework for Context-Aware Assistance in Manual Assembly},
journal = {Procedia CIRP},
volume = {112},
pages = {501-506},
year = {2022},
note = {15th CIRP Conference on Intelligent Computation in ManufacturingEngineering, 14-16 July 2021},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.09.055},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122012094},
author = {Eva Binder and Michael Romer and Patrick Engesser and Jannes Lehwald},
keywords = {Worker Assistance, Manual Assembly, Framework, Context-Aware},
abstract = {To cope with the complexification of manual assembly, new assistance methods are developed continuously. However, those hardware-dependent methods are not deployed context-aware. Hence, workers are not supported situationally and new methods have to be implemented at great expense on a heterogeneous system landscape, evoking an inappropriate maintenance effort. As known from the plant engineering, standardized encapsulation of specific methods provides a solution to integrate heterogeneous applications into one generic system. Therefore, besides the propose of a novel Extensible Worker Assistance (EWA) framework, the underlying novel concepts of so-called Assistance Model Units (AMUs) is utilized as a standardized way to abstract from specific implementations and thus, enable the integration of various assistance methods into one generic system. Furthermore, the applicability of the EWA framework with its underlying core concepts is shown by a use case-specific implementation within the bus assembly. Hence, a first step towards the provision of an optimal worker assistance tailored to the individual needs is done, by the presentation of the EWA framework with the ability to integrate different assistance methods, devices and consider various contextual knowledge within the context-aware assistance selection. Future work has to be done to further develop and investigate the single components of the comprehensive framework.}
}
@article{ROUMELIOTI2022100482,
title = {Smart-thing design by children at a distance: How to engage them and make them learn},
journal = {International Journal of Child-Computer Interaction},
volume = {33},
pages = {100482},
year = {2022},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2022.100482},
url = {https://www.sciencedirect.com/science/article/pii/S2212868922000198},
author = {Eftychia Roumelioti and Maria Angela Pellegrino and Mehdi Rizvi and Mauro D’Angelo and Rosella Gennari},
keywords = {Design, Programming, At a distance, Online, Child, Smart thing, Learning, Engagement, Guidelines, Challenges, Framework, Toolkit, COVID-19 pandemic, Software metrics},
abstract = {In recent years, research in Child–ComputerInteraction has shifted the focus from design with children, giving them a voice in the design process, to design by children to bring child participants different benefits, such as engagement and learning. However, design workshops, encompassing different stages, are challenging in terms of engagement and learning, e.g., they require prolonged commitment and concentration. They are potentially more challenging when held at a distance, as in recent years due to the COVID-19 pandemic. This paper explores at-a-distance smart-thing design by children, how it can engage different children and support their learning in programming. The paper reports a series of design workshops with 20 children, aged from 8 to 16 years old, all held at a distance. They were all organised with the DigiSNaP design framework and toolkit. The first workshop enabled children to explore what smart things are, to start ideating their own smart things and to scaffold their programming. The other workshops enabled children to evolve their own smart-thing ideas and programs. Data were gathered in relation to children’s engagement and learning from different sources. Results are promising for future editions of smart-thing design at a distance or in a hybrid modality. They are discussed along with guidelines for smart-thing design by children at a distance.}
}
@article{HUANG2022108248,
title = {Flexible entity marks and a fine-grained style control for knowledge based natural answer generation},
journal = {Knowledge-Based Systems},
volume = {243},
pages = {108248},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108248},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122000752},
author = {Yongjie Huang and Meng Yang and Ni Yang},
keywords = {Entity marks, Style control, Natural answer generation},
abstract = {Knowledge-based natural answer generation systems are generally difficult to train because numerous entities rarely appear. One way is to replace the entities with their respective types. However, the entity type requires additional recognition with limited accuracy and ignores semantic meanings. Consequently, we propose a question answering system with flexible marks, copying, and retrieving mechanisms (MarkCRQA) to generate natural and accurate answers. By requiring random marks to be shared among all entity types, MarkCRQA attaches the marks to the entities in questions, answers, and knowledge base, which avoids the additional recognition process for entity types and reduces the training difficulty. In addition, we propose to finely control the naturalness and knowledge level of each answer for different real-world scenarios and user needs. Experiments show that MarkCRQA achieves state-of-the-art performance on two open-domain question answering datasets.}
}
@incollection{HARMON2019343,
title = {Chapter 14 - Rental Cars-R-Us case study},
editor = {Paul Harmon},
booktitle = {Business Process Change (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {343-366},
year = {2019},
isbn = {978-0-12-815847-0},
doi = {https://doi.org/10.1016/B978-0-12-815847-0.00014-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128158470000145},
author = {Paul Harmon},
keywords = {As-Is diagram, Business Process Model and Notation (BPMN) diagram, Cause-effect diagram, Concept model, Human performance problems, Organization analysis, Policies and business rules, Problem analysis worksheet, Process management problems, Scope diagramming, Stakeholder analysis, Subprocess analysis, To-Be diagram, Use case diagram},
abstract = {This chapter walks us through the phases of an auto rental business redesign project showing how a process redesign team applies the redesign methodology to a real process problem.}
}
@article{PATEL2018542,
title = {Formalisms of Representing Knowledge},
journal = {Procedia Computer Science},
volume = {125},
pages = {542-549},
year = {2018},
note = {The 6th International Conference on Smart Computing and Communications},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.12.070},
url = {https://www.sciencedirect.com/science/article/pii/S187705091732834X},
author = {Archana Patel and Sarika Jain},
keywords = {Knowledge base, Knowledge Representation, Semantic Web, OWL, Logic},
abstract = {Incomplete, imprecise and large volume of data generates the concept of knowledge base. Knowledge base which is collection of facts, procedures and meaning is much better than database because it provides the power of reasoning, with the help of which the complicated questions are solved. Knowledge representation is a method to encode knowledge, beliefs, action, feeling, goals, desires, preferences and all other mental states in the Knowledge base. Semantic web defines standards for exchanging knowledge via coherent knowledge base. To develop a good knowledge base it is necessary to have good knowledge representation. For this reason, knowledge representation is our main consideration. This paper gives an overview on knowledge representation aspects in the context of semantic web.}
}
@article{CHANTIT2021797,
title = {Towards an automatic model-based Scrum Methodology},
journal = {Procedia Computer Science},
volume = {184},
pages = {797-802},
year = {2021},
note = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.03.099},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921007407},
author = {Salima Chantit and Imane ESSEBAA},
keywords = {Model Driven Engineering, Model Driven Architecture, Model-Based Testing, Agile Methodologies, Scrum, V development Life Cycle},
abstract = {Software systems evolve continuously and must be developed quickly to fit user requirements and new advances in technology. This has led the software engineering to propose several methods and approaches to overcome the development and maintenance of these software systems. In this regard, Agile Methodologies and Model-Driven Engineering (MDE) are two main approaches that have emerged in recent years and suggest a solution to some of the issues associated with Software systems developments. MDE focuses on software reuse through models and on generative approaches based on separation of concerns whereas Agile Methods promote the use of simpler models and best practices for programming to achieve quick feedback from clients within a development process. However, these two approaches have evolved separately and there are only a few works related to their combination. This paper presents a customized V development life cycle based on models which combines the two MDE variants: The MDA approach in the V left branch with the MBT approach to generate tests of the V right branch. In addition, we integrate this customized V life cycle in the agile Scrum methodology to facilitate the management of each Scrum sprint.}
}
@article{JINDAL202062,
title = {Construction of domain ontology utilizing formal concept analysis and social media analytics},
journal = {International Journal of Cognitive Computing in Engineering},
volume = {1},
pages = {62-69},
year = {2020},
issn = {2666-3074},
doi = {https://doi.org/10.1016/j.ijcce.2020.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2666307420300103},
author = {Rajni Jindal and K.R. Seeja and Shivani Jain},
keywords = {Ontology, Semantic web, Formal concept analysis, Fluent editor, Twitter, Social media analysis},
abstract = {Semantic Web, deals with the meaning of information in a defined domain and Ontologies are the backbone of Semantic Web. Domain Ontologies are crucial source of information for knowledge-based system. Still, domain ontology development is a labor- intensive process and is highly dependent on developer's knowledge. In this work, a novel semiautomatic method is proposed to build an ontology on terrorism domain. Terrorism activities provide crucial information to enhance security system for any country worldwide. Social media data, namely, Twitter text data is extracted to attain latest information related to domain and next, concepts and relationships are identified and mapped using formal concept analysis. Several user-defined relationships are presented through fluent editor tool. Also, knowledge is extracted with the help of query-based system through a reasoner window of fluent editor. The developed domain ontology is published on web using ontology web language which can be utilized in other related application areas. The proposed work is significant as it develops a wide-coverage domain ontology for terrorism domain using a tool named Fluent Editor, in place of standard tool protégé and semantic information is extracted similar to query-based system with 100% accuracy.}
}
@article{ROTH2021107530,
title = {Importing participatory practices of the socio-environmental systems community to the process system engineering community: An application to supply chain},
journal = {Computers & Chemical Engineering},
volume = {155},
pages = {107530},
year = {2021},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2021.107530},
url = {https://www.sciencedirect.com/science/article/pii/S0098135421003082},
author = {Anastasia Roth and François Pinta and Stéphane Negny and Ludovic Montastruc},
keywords = {Supply chain, Multi agent modeling, Design},
abstract = {The Process System Engineering community has an extensive knowledge and skills on supply chain design: from the time dimension (production and flow planning) to the space dimension (geographic position of facilities). Nevertheless, supply chains are also social networks where multiple stakeholders have to collaborate while they have different, and sometimes, diverging objectives. For this reason, having a more realistic model representing collaboration between the various stakeholders involved is necessary and new methods that facilitate the development of a shared representation of the system must be introduced. We propose to import a participatory method, PARDI (Problematic, Actors, Resources, Dynamics and Interactions), from the Socio-Environmental System community to the practices of the Process System Engineering community. Based on this method, we develop a participatory process in order to collect the necessary knowledge on the supply chain and its context. Following this participatory process, we then develop an Agent-Based Model as a simulation and decision making tool to support collective scenario analyses and collectively draw solutions with stakeholders. Our participatory modeling approach necessarily imposes a multi stakeholders vision (within the modeling but also in the result analyses) and therefore the search for a modeling consensus. Thus, it brings a better inclusion of social aspects in problem solving which are usually poorly considered leading to implementation failure sometimes. By comparing our approach with the classic one of the Process Systems Engineering community, we highlight the strengths and weaknesses of both and how complementary they can be. A case study on the already existing supply chain of the chestnut wood in Cévennes area (France) illustrates the capabilities of our participatory methodology. It focuses on the socio-economic model design of the first two steps (forestry activities to harvest) in the supply chain as the latter is locked because of economic and social organisation issues. The objective is to find the best action levers to unlock the resistance that forest plot owners have to remove declining wood from their land.}
}
@article{NESIC2021110922,
title = {Product-line assurance cases from contract-based design},
journal = {Journal of Systems and Software},
volume = {176},
pages = {110922},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.110922},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221000194},
author = {Damir Nešić and Mattias Nyberg and Barbara Gallina},
keywords = {Assurance cases, Product line engineering, Contract-based design},
abstract = {Assurance cases are used to argue in a structured, and evidence-supported way, that a property such as safety or security is satisfied by a system. In some domains however, instead of single systems, product lines with many system-variants are engineered, to satisfy the needs of different customers. In such context, single-system methods for assurance-case creation suffer from scalability issues because the underlying assumption is that the evidence and arguments can be created per system variant. This paper presents a novel method for product-line assurance-case creation where all the arguments and the evidence are created without analyzing each system variant. Consequently, the effort to create an assurance case scales with the complexity of system variants, instead with their number. The method is based on a contract-based design framework for cyber–physical systems, which is extended to define the conditions under which all system variants satisfy a particular property. These conditions are used to define an assurance-case pattern, which can be instantiated for arbitrary product lines. Moreover, the defined pattern is modular to enable step-wise assurance-case creation. Finally, an exploratory case study is performed on a real product-line from the heavy-vehicle manufacturer Scania to evaluate the applicability of the presented method.}
}
@article{MANNS2021924,
title = {Identifying human intention during assembly operations using wearable motion capturing systems including eye focus},
journal = {Procedia CIRP},
volume = {104},
pages = {924-929},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.155},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121010532},
author = {Martin Manns and Tadele Belay Tuli and Florian Schreiber},
keywords = {Operator 4.0, human motion capture, Eye tracker, human-robot collaboration},
abstract = {Simulating human motion behavior in assembly operations helps to create efficient collaboration plans for humans and robots. However, identifying human intention may require high quality human motion capture data in order to discriminate micro-actions and human attention. In this regard, a human motion capture setup that combines various systems such as joint body, finger, and eye trackers is proposed in combination with a methodology of identifying the intention of human operators as well as for predicting sequences of activities. The approach may lead to safer human-robot collaboration.}
}
@article{GOOSSENS2023118667,
title = {Extracting Decision Model and Notation models from text using deep learning techniques},
journal = {Expert Systems with Applications},
volume = {211},
pages = {118667},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118667},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422017043},
author = {Alexandre Goossens and Johannes {De Smedt} and Jan Vanthienen},
keywords = {Deep learning, Decision Model and Notation, DMN, Decision model extraction},
abstract = {Companies and organizations often use manuals and guidelines to communicate and execute operational decisions. Decision Model and Notation (DMN) models can be used to model and automate these decisions. Modeling a decision from a textual source, however, is a time intensive and complex activity hence a need for shorter modeling times. This paper studies how NLP deep learning techniques can extract decision models from text faster. In this paper, we study and evaluate an automatic sentence classifier and a decision dependency extractor using NLP deep learning models (BERT and Bi-LSTM-CRF). A large labeled and tagged dataset was collected from real use cases to train these models. We conclude that BERT can be used for the (semi)-automatic extraction of decision models from text.}
}
@article{BELHI2021817,
title = {Integration of Business Applications with the Blockchain: Odoo and Hyperledger Fabric Open Source Proof of Concept},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {817-824},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.185},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321009551},
author = {Abdelhak Belhi and Houssem Gasmi and Abdelaziz Bouras and Belaid Aouni and Ibrahim Khalil},
keywords = {Blockchain, Supply Chain, ERP, Odoo, Hyperledger Fabric, Software Integration},
abstract = {Since its revolution in the financial sector, the blockchain technology disrupted the majority of collaboration-based applications including supply chain management. The supply chain is one of the most important sectors benefiting from all the advantages of blockchain. Through this paper, we are mainly focusing on the practical aspects of integrating blockchain technology with traditional and existing business applications. Indeed, most businesses and corporations will find it hard to shift from traditional architectures to a decentralized one with fears related to upgrading risks, unknown tools, and resistance to change. Thus, we mainly propose several scenarios for blockchain integration focusing on the most used Enterprise Resource Planning (ERP) platforms. Besides, we present our proof of concept integration that uses the HyperLedger Fabric blockchain platform and the Odoo ERP framework. The selection of these two solutions for our study was mainly influenced by the fact that Hyperledger is the most used open-source blockchain platform in the industry and that Odoo is the most used open-source ERP framework. Nevertheless, we believe that our proposed solution can easily be used with proprietary ERP platforms and business applications.}
}
@article{TAHVILI201826,
title = {ESPRET: A tool for execution time estimation of manual test cases},
journal = {Journal of Systems and Software},
volume = {146},
pages = {26-41},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218301778},
author = {Sahar Tahvili and Wasif Afzal and Mehrdad Saadatmand and Markus Bohlin and Sharvathul Hasan Ameerjan},
keywords = {Software testing, Execution time, Test specification, Optimization, Manual testing, Regression analysis},
abstract = {Manual testing is still a predominant and an important approach for validation of computer systems, particularly in certain domains such as safety-critical systems. Knowing the execution time of test cases is important to perform test scheduling, prioritization and progress monitoring. In this work, we present, apply and evaluate ESPRET (EStimation and PRediction of Execution Time) as our tool for estimating and predicting the execution time of manual test cases based on their test specifications. Our approach works by extracting timing information for various steps in manual test specification. This information is then used to estimate the maximum time for test steps that have not previously been executed, but for which textual specifications exist. As part of our approach, natural language parsing of the specifications is performed to identify word combinations to check whether existing timing information on various test steps is already available or not. Since executing test cases on the several machines may take different time, we predict the actual execution time for test cases by a set of regression models. Finally, an empirical evaluation of the approach and tool has been performed on a railway use case at Bombardier Transportation (BT) in Sweden.}
}
@incollection{JILANI2019135,
title = {Chapter Three - Advances in Applications of Object Constraint Language for Software Engineering},
editor = {Atif M. Memon},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {112},
pages = {135-184},
year = {2019},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0065245817300554},
author = {Atif A. Jilani and Muhammad Z. Iqbal and Muhammad U. Khan and Muhammad Usman},
keywords = {Object Constraint Language, Model-driven engineering, Secondary study, Software engineering},
abstract = {Object Constraint Language (OCL) is a standard language defined by Object Management Group for specifying constraints on models. Since its introduction as part of Unified Modeling Language, OCL has received significant attention by researchers with works in the literature ranging from temporal extensions of OCL to automated test generation by solving OCL constraints. In this chapter, we provide a survey of the various works discussed in literature related to OCL with the aim of highlighting the advances made in the field. We classify the literature into five broad categories and provide summaries for various works in the literature. The chapter also provides insights and highlights the potentials areas of further research in the field.}
}
@article{COSTA2020110720,
title = {Towards the adoption of OMG standards in the development of SOA-based IoT systems},
journal = {Journal of Systems and Software},
volume = {169},
pages = {110720},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110720},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220301588},
author = {Bruno Costa and Paulo F. Pires and Flávia C. Delicato},
keywords = {Internet of Things, Application, Model-Driven Development, Service-Oriented Architecture},
abstract = {A common feature of the Internet of Things (IoT) is the high heterogeneity, regarding network protocols, data formats, hardware and software platforms. Aiming to deal with such a degree of heterogeneity, several frameworks have applied the Model-Driven Development (MDD) to build IoT applications. On the software architecture viewpoint, the literature has shown that the Service-Oriented Architecture (SOA) is a promising style to address the interoperability of entities composing these solutions. Some features of IoT make it challenging to analyze the impact of design decisions on the SOA-based IoT applications behavior. Thus, it is a key requirement to simulate the model to verify whether the system performs as expected before its implementation. Although the literature has identified that the SOA style is suitable for addressing the interoperability, existing modeling languages do not consider SOA elements as first-class citizens when designing IoT applications. Furthermore, although existing MDD frameworks provide modeling languages comprising well-defined syntax, they lack execution semantics, thus, are not suitable for model execution and analysis. This work aims at addressing these issues by introducing IoTDraw. The framework provides a fully OMG-compliant executable modeling language for SOA-based IoT systems; thus, its specifications can be implemented by any tool implementing OMG standards.}
}
@article{WALI2023104187,
title = {Recent progress in digital image restoration techniques: A review},
journal = {Digital Signal Processing},
volume = {141},
pages = {104187},
year = {2023},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2023.104187},
url = {https://www.sciencedirect.com/science/article/pii/S1051200423002828},
author = {Aamir Wali and Asma Naseer and Maria Tamoor and S.A.M. Gilani},
keywords = {Digital image, Image restoration, Degradation, Noise, Transformation},
abstract = {Digital images are playing a progressively important role in almost all the fields such as computer science, medicine, communications, transmission, security, surveillance, and many more. Digital images are susceptible to a number of distortions due to faulty imaging instruments, transmission channels, atmospheric and environmental conditions, etc. resulting in degraded images. Degradation can be of different types such as noise, backscattering, low saturation, low contrast, tilt, spectral absorption, blurring, etc. The degradation reduces digital images' effectiveness and therefore needs to be restored. In this paper, we present an extensive review of image restoration tasks. It addresses problems like image deblurring, denoising, dehazing and super-resolution. Image restoration is fundamentally an image processing problem, but deep learning techniques, based mainly on convolutional neural networks have received a lot of attention in almost all areas of computer science. Along with deep learning, other machine learning methods have also been tried for restoring digital images. In this review, we have therefore categorized digital image restoration techniques as either image processing-based, machine learning-based or deep learning-based. For each category, a variety of approaches presented in recent years have been reviewed. This review also includes a summary of the data sets used for image restoration along with a baseline reference that can be used by future researchers to compare and improve their results. We also suggest some interesting research directions for future work in this area.}
}
@article{MEFTEH201889,
title = {Towards naturalistic programming: Mapping language-independent requirements to constrained language specifications},
journal = {Science of Computer Programming},
volume = {166},
pages = {89-119},
year = {2018},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2018.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167642318301941},
author = {Mariem Mefteh and Nadia Bouassida and Hanêne Ben-Abdallah},
keywords = {Natural language processing, Patterns, Semantics, Multilingual requirements, Use case scenarios},
abstract = {This research paper presents a new approach that constitutes a first step towards programming using language-independent requirements. To leverage the needed programming effort, our approach takes requirements in the form of language-independent use case scenarios. Then, it generates the inputs of a code generator which, in turn, produces the corresponding code. To provide for the language-independence, our approach uses an enriched version of the semantic model, as a means to represent similar ideas possibly in different ways and in different natural languages. The enrichment consists of a set of patterns that it implements as XML code representing the information embedded in the use case scenarios. This intermediate representation can be processed to derive the inputs required by any code generator to produce code in a particular programming language. This paper illustrates the approach and its tool support for use case scenarios written in English and French, and semantic model patterns implemented as XML code that can be processed by the ReDSeeDS code generator. In addition, it presents the results of an experimental evaluation of the approach on use case scenarios (written in English and in French) belonging to five different systems. This evaluation quantitatively shows the ability of our approach: to extract ReDSeeDS inputs conforming to the expert's inputs with a high precision; to generate XML code elements conforming to the input with an encouraging performance as evaluated by the participating students (an F-measure ranging between 87.43% and 92.31%); and to generate Java code judged efficient by the participating programmers (an F-measure ranging between 66.4% and 93.43%).}
}
@article{SKERSYS2018111,
title = {Extracting SBVR business vocabularies and business rules from UML use case diagrams},
journal = {Journal of Systems and Software},
volume = {141},
pages = {111-130},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.03.061},
url = {https://www.sciencedirect.com/science/article/pii/S016412121830061X},
author = {Tomas Skersys and Paulius Danenas and Rimantas Butleris},
keywords = {SBVR business vocabulary, SBVR business rules, UML use case diagram, Model-to-model transformation},
abstract = {In model-driven information systems engineering, model transformations reside at the very core of this paradigm. Indeed, model transformations (in particular, model-to-model, or M2M) are a must-have feature of any modern model-driven approach supported by CASE technology. Model transformations are intended to raise quality of the models under development, and also speed-up the modeling itself by bringing in certain level of automation into the development process. Nevertheless, due to certain objective reasons, the level of such automation is spread unevenly throughout the development process – in this respect, Business Modeling and System Analysis are, arguably, the most underdeveloped phases of the model-driven information systems development life cycle. In this paper, we show how M2M transformation technology was used to extract well-structured business vocabularies and business rules from formal use case models represented through a set of use case diagrams; Object Management Group's (OMG) standards Semantics for Business Vocabulary and Rules (SBVR) and Unified Modeling Language (UML) were used for this purpose. The proposed solution consists of two concurrent approaches, namely, automatic and semi-automatic, which may be used selectively to achieve the best expected result. Basic implementation aspects of the solution integrating both approaches are also briefly presented in the paper. While UML use case models is the main subject in this research, the proposed solution may be adopted for other UML and MOF-based models as well.}
}
@article{BOOTH20191,
title = {Editorial: Defeasible and Ampliative Reasoning},
journal = {International Journal of Approximate Reasoning},
volume = {112},
pages = {1-3},
year = {2019},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X19302944},
author = {Richard Booth and Giovanni Casini and Szymon Klarman and Gilles Richard and Ivan Varzinczak}
}
@article{KUTIN2018470,
title = {Simulation Modeling of Assembly Processes in Digital Manufacturing},
journal = {Procedia CIRP},
volume = {67},
pages = {470-475},
year = {2018},
note = {11th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 19-21 July 2017, Gulf of Naples, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2017.12.246},
url = {https://www.sciencedirect.com/science/article/pii/S2212827117311927},
author = {Andrey Kutin and Vitaly Dolgov and Alexey Podkidyshev and Alexander Kabanov},
keywords = {Simulation modeling, Assembly processes, Digital manufacturing},
abstract = {Method is proposed for simulation modeling of assembly processes in digital manufacturing, allowing considering influence of intersections of main material flows of different products at same workplaces, component supply interruptions, as well as non-productive time losses due to organizational or technical causes on performance parameters of assembly processes. Simulation modeling has shown 34% increase of manufacturing system productivity.}
}
@article{BARBAGONZALEZ2024107378,
title = {BIGOWL4DQ: Ontology-driven approach for Big Data quality meta-modelling, selection and reasoning},
journal = {Information and Software Technology},
volume = {167},
pages = {107378},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107378},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923002331},
author = {Cristóbal Barba-González and Ismael Caballero and Ángel Jesús Varela-Vaca and José A. Cruz-Lemus and María Teresa Gómez-López and Ismael Navas-Delgado},
keywords = {Data quality evaluation and measurement, Data quality information model, Big Data, Ontology, Decision model and notation},
abstract = {Context:
Data quality should be at the core of many Artificial Intelligence initiatives from the very first moment in which data is required for a successful analysis. Measurement and evaluation of the level of quality are crucial to determining whether data can be used for the tasks at hand. Conscientious of this importance, industry and academia have proposed several data quality measurements and assessment frameworks over the last two decades. Unfortunately, there is no common and shared vocabulary for data quality terms. Thus, it is difficult and time-consuming to integrate data quality analysis within a (Big) Data workflow for performing Artificial Intelligence tasks. One of the main reasons is that, except for a reduced number of proposals, the presented vocabularies are neither machine-readable nor processable, needing human processing to be incorporated.
Objective:
This paper proposes a unified data quality measurement and assessment information model. This model can be used in different environments and contexts to describe data quality measurement and evaluation concerns.
Method:
The model has been developed as an ontology to make it interoperable and machine-readable. For better interoperability and applicability, this ontology, BIGOWL4DQ, has been developed as an extension of a previously developed ontology for describing knowledge management in Big Data analytics.
Conclusions:
This extended ontology provides a data quality measurement and assessment framework required when designing Artificial Intelligence workflows and integrated reasoning capacities. Thus, BIGOWL4DQ can be used to describe Big Data analysis and assess the data quality before the analysis.
Result:
Our proposal has been validated with two use cases. First, the semantic proposal has been assessed using an academic use case. And second, a real-world case study within an Artificial Intelligence workflow has been conducted to endorse our work.}
}
@article{GARCIAGARCIA201952,
title = {Characterizing and evaluating the quality of software process modeling language: Comparison of ten representative model-based languages},
journal = {Computer Standards & Interfaces},
volume = {63},
pages = {52-66},
year = {2019},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2018.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0920548918302757},
author = {J.A. García-García and J.G. Enríquez and F.J. Domínguez-Mayo},
keywords = {Quality Evaluation Framework (QuEF), Quality Model, Software Process Modeling Language, comparative study},
abstract = {Software organizations are very conscious that deployments of well-defined software processes improve software product development and its quality. Over last decade, many Software Process Modeling Languages (SPMLs) have been proposed to describe and manage software processes. However, each one presents advantages and disadvantages. The main challenge for an organization is to choose the best and most suitable SPML to meet its requirements. This paper proposes a Quality Model (QM) which has been defined conforms to QuEF (Quality Evaluation Framework). This QM allows to compare model-based SPMLs and it could be used by organizations to choose the most useful model-based SPML for their particular requirements. This paper also instances our QM to evaluate and compare 10 representative SPMLs of the various alternative approaches (metamodel-level approaches; SPML based on UML and approaches based on standards). Finally, this paper concludes there are many model-based proposals for SPM, but it is very difficult to establish with could be the commitment to follow. Some non-considered aspects until now have been identified (e.g., validation within enterprise environments, friendly support tools, mechanisms to carry out continuous improvement, mechanisms to establish business rules and elements for software process orchestrating).}
}
@article{TOMSETT2020100049,
title = {Rapid Trust Calibration through Interpretable and Uncertainty-Aware AI},
journal = {Patterns},
volume = {1},
number = {4},
pages = {100049},
year = {2020},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2020.100049},
url = {https://www.sciencedirect.com/science/article/pii/S266638992030060X},
author = {Richard Tomsett and Alun Preece and Dave Braines and Federico Cerutti and Supriyo Chakraborty and Mani Srivastava and Gavin Pearson and Lance Kaplan},
keywords = {DSML 1: Concept: Basic principles of a new data science output observed and reported},
abstract = {Artificial intelligence (AI) systems hold great promise as decision-support tools, but we must be able to identify and understand their inevitable mistakes if they are to fulfill this potential. This is particularly true in domains where the decisions are high-stakes, such as law, medicine, and the military. In this Perspective, we describe the particular challenges for AI decision support posed in military coalition operations. These include having to deal with limited, low-quality data, which inevitably compromises AI performance. We suggest that these problems can be mitigated by taking steps that allow rapid trust calibration so that decision makers understand the AI system's limitations and likely failures and can calibrate their trust in its outputs appropriately. We propose that AI services can achieve this by being both interpretable and uncertainty-aware. Creating such AI systems poses various technical and human factors challenges. We review these challenges and recommend directions for future research.}
}
@article{FORTINEAU201922,
title = {Automated business rules and requirements to enrich product-centric information},
journal = {Computers in Industry},
volume = {104},
pages = {22-33},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0166361518301477},
author = {Virginie Fortineau and Thomas Paviot and Samir Lamouri},
keywords = {Business rule, Requirements, Ontology, PLM, BIM},
abstract = {Current PLM or BIM based information systems suffer from a lack of checking components for business rules. One reason is the misunderstanding of the role and nature of business rules, and how they should be treated in a product-centric information system. This paper intends to provide both a process and a related model to build such a component and enrich future systems. Rules and requirements process management enables the unambiguous formalization of implicit knowledge contained in business rules, generally expressed in easily understandable language, and leads to the formal expression of requirements. In this paper, the requirements are considered a consequence of the application of a business rule. A conceptual model is then introduced, called DALTON (DAta Linked Through Occurrences Network), which supports this process. In this ontology, concepts and product data, coming for instance from an existing product database, are represented using instances and occurrences, connected together with triples built from business rules and requirements according to previous management processes. An experiment involving a set of SWRL rules is conducted in the Protégé environment that validates the model and the process.}
}
@article{LU2024111522,
title = {CrossPrune: Cooperative pruning for camera–LiDAR fused perception models of autonomous driving},
journal = {Knowledge-Based Systems},
volume = {289},
pages = {111522},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111522},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124001576},
author = {Yantao Lu and Bo Jiang and Ning Liu and Yilan Li and Jinchao Chen and Ying Zhang and Zifu Wan},
keywords = {Camera–LiDAR fusion, DNN pruning, Perception of autonomous driving},
abstract = {Deep neural network pruning is effective in enabling high-performance perception models to be deployed on autonomous driving platforms with limited computation and memory resources. With their rapid development, state-of-the-art autonomous driving perception (ADP) models have advocated the use of multimodal sensors to extract diverse feature categories. However, existing pruning studies in the ADP area focus on single-modal models and neglect multimodal models. Compared with conventional pruning, multimodal pruning presents a new type of redundancy, namely modal-wise redundancy that is caused by multimodal branches extracting similar perception information. When a specific type of information is extracted by more than one modal branch, although this extraction is deemed essential from an individual single-modal standpoint, it becomes redundant when viewed from a modal-wise perspective. Therefore, modal branches must be handled cooperatively to eliminate modal-wise redundancy while concurrently preserving the original perception accuracy. Building on this, we propose CrossPrune, a modal cooperative pruning framework designed for camera–LiDAR fused perception in autonomous driving. The primary objective of CrossPrune is to effectively eliminate redundancy and achieve nondestructive pruning for multimodal ADP models. This was accomplished by approaching the problem as a multi-objective optimization task, encompassing both weight pruning and the restriction of feature distortions caused by pruning. Experiments conducted on the nuScenes and KITTI datasets demonstrated that CrossPrune attained superior pruning ratios while minimizing accuracy loss, surpassing the performance of the baselines. The key results indicated that the proposed CrossPrune achieved relative improvements of 9.6% in mAP and 11.5% in NDS under 89.8% pruning sparsity.}
}
@article{DURAN2021102565,
title = {Preface – 22nd Brazilian Symposium on Formal Methods – SBMF 2019},
journal = {Science of Computer Programming},
volume = {201},
pages = {102565},
year = {2021},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2020.102565},
url = {https://www.sciencedirect.com/science/article/pii/S0167642320301738},
author = {Adolfo Duran and Philip Wadler}
}
@article{YADAV202185,
title = {A comprehensive review on resolving ambiguities in natural language processing},
journal = {AI Open},
volume = {2},
pages = {85-92},
year = {2021},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2021.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666651021000127},
author = {Apurwa Yadav and Aarshil Patel and Manan Shah},
keywords = {Natural language processing, Requirement engineering, Machine learning, Ambiguity, Disambiguation},
abstract = {Natural language processing is a known technology behind the development of some widely known AI assistants such as: SIRI, Natasha, and Watson. However, NLP is a diverse technology used for numerous purposes. NLP based tools are widely used for disambiguation in requirement engineering which will be the primary focus of this paper. A requirement document is a medium for the user to deliver one's expectations from the software. Hence, an ambiguous requirement document may eventually lead to misconceptions in a software. Various tools are available for disambiguation in RE based on different techniques. In this paper, we analyzed different disambiguation tools in order to compare and evaluate them. In our survey, we noticed that even though some disambiguation tools reflect promising results and can supposedly be relied upon, they fail to completely eliminate the ambiguities. In order to avoid ambiguities, the requirement document has to be written using formal language, which is not preferred by users due to its lack of lucidity and readability. Nevertheless, some of the tools we mentioned in this paper are still under development and in future might become capable of eliminating ambiguities. In this paper, we attempt to analyze some existing research work and present an elaborative review of various disambiguation tools.}
}
@article{CARVALHO2021102537,
title = {Validating, verifying and testing timed data-flow reactive systems in Coq from controlled natural-language requirements},
journal = {Science of Computer Programming},
volume = {201},
pages = {102537},
year = {2021},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2020.102537},
url = {https://www.sciencedirect.com/science/article/pii/S0167642320301453},
author = {Gustavo Carvalho and Igor Meira},
keywords = {Timed data-flow reactive system, Interactive theorem proving, Property-based testing, Controlled natural language},
abstract = {Data-flow reactive systems (DFRSs) form a class of embedded systems whose inputs and outputs are always available as signals. Input signals can be seen as data provided by sensors, whereas the output data are provided to system actuators. In previous works, verifying well-formedness properties of DFRS models was accomplished in a programmatic way, with no formal guarantees, and test cases were generated by translating these models into other notations. Here, we use Coq as a single framework to specify, validate and verify DFRS models. Moreover, the specification of DFRSs in Coq is automatically derived from controlled natural-language requirements, and well-formedness properties are formally verified with no user intervention. System validation is supported by bounded exploration of models; general and domain-specific system property verification is supported by the development of proof scripts, and test generation is achieved with the aid of the QuickChick tool. Considering examples from the literature, but also from the aerospace (Embraer) and the automotive (Mercedes) industries, our automatic testing strategy was evaluated in terms of performance and the ability to detect defects generated by mutation. Within seconds, test cases were generated automatically from the requirements, achieving an average mutation score of about 75%.}
}
@article{RENCIS201827,
title = {On Keyword-Based Ad-Hoc Querying of Hospital Data Stored in Semistar Data Ontologies},
journal = {Procedia Computer Science},
volume = {138},
pages = {27-32},
year = {2018},
note = {CENTERIS 2018 - International Conference on ENTERprise Information Systems / ProjMAN 2018 - International Conference on Project MANagement / HCist 2018 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316363},
author = {Edgars Rencis},
keywords = {Keyword-Based Querying, Query Languages, Semistar Ontologies, Natural Language Querying, Query Translation, Hospital Management},
abstract = {This paper sketches a possible solution to the problem of the currently growing necessity in various domains for domain experts to be able to query the database of the organization in a convenient manner. The paper focuses on the domain of hospital management where the normal practice is to involve a programmer as an intermediary between the managers and the database. This is an error-prone and cumbersome solution. The decision-making process of domain experts would hugely benefit if they could retrieve the information from the database themselves. There have been attempts to develop natural language-based query languages for this exact purpose, but the ultimate goal of the simplicity of querying has not yet been reached. The approach presented in this paper involves letting users define their queries in a very weakly-controlled natural language and then to choose among the offered query translations into a more strongly-controlled natural language which already has an efficient implementation for semistar ontologies. Experiments show that the implemented query language is very readable for non-programmers, because it lacks technical details (thanks to the nature of semistar ontologies), and because it is very intuitive. This phenomenon creates a conviction that the proposed approach is highly viable.}
}
@article{HUANG2020344,
title = {Research on Vehicle Service Simulation Dispatching Telephone System Based on Natural Language Processing},
journal = {Procedia Computer Science},
volume = {166},
pages = {344-349},
year = {2020},
note = {Proceedings of the 3rd International Conference on Mechatronics and Intelligent Robotics (ICMIR-2019)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.087},
url = {https://www.sciencedirect.com/science/article/pii/S187705092030209X},
author = {Da Ji Huang and Hai Xiang Lin},
keywords = {Vehicle simulation, NLP, Voice interaction, System recognition},
abstract = {In view of the current situation that the training process of the existing vehicle simulation training system is not complete and quite different under abnormal circumstances, a vehicle simulation dispatching telephone system based on natural language processing (NLP) is developed. The system uses ASR and TTS, combined with embedded design, to simulate the actual on-site traffic attendant’s voice interaction with each post under abnormal conditions, and secondly recognize the received speech recognition text through NLP which is converted to railway standard terminology for system evaluation. Through the actual case application, it is verified that the system can better identify railway terminology, and ensure that the single person completes the whole process of training and assessment. The system realizes system identification and human-computer interaction, which is important for the efficient and reliable training of the vehicle duty attendant and the expansion of the training system function.}
}
@article{BERRIOS2023100081,
title = {Implementing the map task in applied linguistics research: What, how, and why},
journal = {Research Methods in Applied Linguistics},
volume = {2},
number = {3},
pages = {100081},
year = {2023},
issn = {2772-7661},
doi = {https://doi.org/10.1016/j.rmal.2023.100081},
url = {https://www.sciencedirect.com/science/article/pii/S2772766123000411},
author = {Juan Berríos and Angela Swain and Melinda Fricke},
keywords = {Map task, Written communication, Informal language, Corpus design, Computer-mediated communication},
abstract = {The “map task” is an interactive, goal-driven, real-time conversational task used to elicit semi-controlled natural language production data. We present recommendations for creating a bespoke map task that can be tailored to individual research projects and administered online using a chat interface. As proof of concept, we present a case study exemplifying our own implementation, designed to elicit informal written communication in either English or Spanish. Eight experimental maps were created, manipulating linguistic factors including lexical frequency, cognate status, and semantic ambiguity. Participants (N = 40) completed the task in pairs and took turns (i) providing directions based on a pre-traced route, or (ii) following directions to draw the route on an empty map. Computational measures of image similarity (e.g., structural similarity index) between pre-traced and participant-traced routes showed that participants completed the task successfully; we describe use of this method for measuring task success quantitatively. We also provide a comparative analysis of the language elicited in English and Spanish. The most frequently used words were roughly equivalent in both languages, encompassing primarily commands and items on the maps. Similarly, abbreviations, swear words, and slang present in both datasets indicated that the task successfully elicited informal communication. Interestingly, Spanish turns were longer and displayed a wider range of morphologically complex forms. English, conversely, displayed strategies mostly absent in Spanish, such as the use of cardinal directions as a communicative strategy. We consider the online map task as a promising method for examining a variety of phenomena in applied linguistics research.}
}
@article{YAOPINGPENG2023e13302,
title = {Enhancing students’ English language learning via M-learning: Integrating technology acceptance model and S-O-R model},
journal = {Heliyon},
volume = {9},
number = {2},
pages = {e13302},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e13302},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023005091},
author = {Michael {Yao-Ping Peng} and Yunying Xu and Cheng Xu},
keywords = {Curiosity, Perceived convenience, Perceived ease of use, Perceived usefulness, Self-efficacy, SOR model, Technology acceptance model},
abstract = {With the impact of COVID-19, many university students may not be able to learn English in the physical classroom in a traditional way. Students' English learning effectiveness and outcome were threatened when English learning was forced to turn online. Thus, a variety of technological media and platforms to improve their learning outcomes are in need. Mobile learning (M-learning) that involves interacting with other devices through mobile devices and wireless networks can also be a solution to improve students' online English learning effectiveness. In order to explore the learning behaviors and attitudes of university students when learning English with M-learning, this study integrated technology acceptance model and Stimulus Organism Response model including the concepts of perceived convenience, curiosity and self-efficacy in addition to the original technology acceptance model to verify university students’ usage cognition and attitude toward English M-learning. This study disseminated surveys to 10 targeted universities/colleges and collected 1432 valid surveys. This study implemented Smart-PLS 4.0 to examine structural model and verify the hypotheses. Results indicated that perceived convenience have positive impact on perceived ease of use, perceived usefulness and attitude toward using; there is a significant and positive relationship among perceived ease of use, perceived usefulness, attitude toward using and intention to using; curiosity and self-efficacy have positive impact on intention to using. Based on the findings, this study further provides abundant theoretical insights and practical significance on language learning.}
}
@article{GARCIA2020100610,
title = {A benchmark for end-user structured data exploration and search user interfaces},
journal = {Journal of Web Semantics},
volume = {65},
pages = {100610},
year = {2020},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2020.100610},
url = {https://www.sciencedirect.com/science/article/pii/S1570826820300469},
author = {Roberto García and Rosa Gil and Eirik Bakke and David R. Karger},
keywords = {Benchmark, User experience, Usability, Semantic data, Exploration, Relational data},
abstract = {During the years, it has been possible to assess significant improvements in the computational efficiency of Semantic Web search and exploration systems. However, it has been much harder to assess how well different semantic systems’ user interfaces help their users. One of the key factors facilitating the advancement of research in a particular field is the ability to compare the performance of different approaches. Though there are many such benchmarks in Semantic Web fields that have experienced significant improvements, this is not the case for Semantic Web user interfaces for data exploration. We propose and demonstrate the use of a benchmark for evaluating such user interfaces, which includes a set of typical user tasks and a well-defined procedure for assigning a measure of performance on those tasks to a semantic system. We have applied the benchmark to four such systems. Moreover, all the required resources to apply the benchmark are openly available online. We intend to initiate a community conversation that will lead to a generally accepted framework for comparing systems and for measuring, and thus encouraging, progress towards better semantic search and exploration tools.}
}
@article{GATTULLO2019276,
title = {Towards augmented reality manuals for industry 4.0: A methodology},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {56},
pages = {276-286},
year = {2019},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0736584518301236},
author = {Michele Gattullo and Giulia Wally Scurati and Michele Fiorentino and Antonio Emmanuele Uva and Francesco Ferrise and Monica Bordegoni},
keywords = {Augmented reality, Industry 4.0, Maintenance support, Technical documentation},
abstract = {Augmented Reality (AR), is one of the most promising technology for technical manuals in the context of Industry 4.0. However, the implementation of AR documentation in industry is still challenging because specific standards and guidelines are missing. In this work, we propose a novel methodology for the conversion of existing “traditional” documentation, and for the authoring of new manuals in AR in compliance to Industry 4.0 principles. The methodology is based on the optimization of text usage with the ASD Simplified Technical English, the conversion of text instructions into 2D graphic symbols, and the structuring of the content through the combination of Darwin Information Typing Architecture (DITA) and Information Mapping (IM). We tested the proposed approach with a case study of a maintenance manual of hydraulic breakers. We validated it with a user test collecting subjective feedbacks of 22 users. The results of this experiment confirm that the manual obtained using our methodology is clearer than other templates.}
}
@article{DIXIT2022100314,
title = {Towards user-centered and legally relevant smart-contract development: A systematic literature review},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100314},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100314},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21001072},
author = {Abhishek Dixit and Vipin Deval and Vimal Dwivedi and Alex Norta and Dirk Draheim},
keywords = {Blockchain, Smart contract, Ricardian contract, Business collaboration, Legal relevance},
abstract = {Smart contracts (SC) run on blockchain technology (BCT) to implement agreements between several parties. As BCT grows, organizations aim to automate their processes and engage in business collaborations using SCs. The translation of contract semantics into SC language semantics is difficult due to ambiguous contractual interpretation by the several parties and the developers. Also, an SC language itself misses the language constructs needed for semantically expressing collaboration terms. This leads to SC coding errors that result in contractual conflicts over transactions during the performance of SCs and thus, novel SC solutions incur high development and maintenance costs. Various model-based and no/low code development approaches address this issue by enabling higher abstractions in SC development. Still, the question remains unanswered how contractual parties, i.e., end-users with non-IT skills, manage to develop legally relevant SCs with ease. This study aims to (1) identify and categorize the state of the art of SC automation models, in terms of their technical features, and their legal significance, and to (2) identify new research opportunities. The review has been conducted as a systematic literature review (SLR) that follows the guidelines proposed by Kitchenham for performing SLRs in software-engineering. As a result of the implementation of the review protocol, 1367 papers are collected, and 33 of them are selected for extraction and analysis. The contributions of this article are threefold: (1) 10 different SC automation models/frameworks are identified and classified according to their technical and implementation features; (2) 11 different legal contract parameters are identified and categorized into 4 legal criteria classes; (3) a comparative analysis of SC-automation models in the context of their legal significance is conducted that identifies the degrees to which the SC-automation models are considered legally relevant. As a conclusion, we produce a comprehensive and replicable overview of the state of the art of SC automation models and a systematic measure of their legal significance to benefit practitioners in the field.}
}
@article{ANGEL201890,
title = {Automated modelling assistance by integrating heterogeneous information sources},
journal = {Computer Languages, Systems & Structures},
volume = {53},
pages = {90-120},
year = {2018},
issn = {1477-8424},
doi = {https://doi.org/10.1016/j.cl.2018.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1477842417301690},
author = {Mora Segura Ángel and Juan {de Lara} and Patrick Neubauer and Manuel Wimmer},
keywords = {Modelling, (Meta-)modelling, Modelling assistance, Domain-specific languages, Language engineering},
abstract = {Model-Driven Engineering (MDE) uses models as its main assets in the software development process. The structure of a model is described through a meta-model. Even though modelling and meta-modelling are recurrent activities in MDE and a vast amount of MDE tools exist nowadays, they are tasks typically performed in an unassisted way. Usually, these tools cannot extract useful knowledge available in heterogeneous information sources like XML, RDF, CSV or other models and meta-models. We propose an approach to provide modelling and meta-modelling assistance. The approach gathers heterogeneous information sources in various technological spaces, and represents them uniformly in a common data model. This enables their uniform querying, by means of an extensible mechanism, which can make use of services, e.g., for synonym search and word sense analysis. The query results can then be easily incorporated into the (meta-)model being built. The approach has been realized in the Extremo tool, developed as an Eclipse plugin. Extremo has been validated in the context of two domains – production systems and process modelling – taking into account a large and complex industrial standard for classification and product description. Further validation results indicate that the integration of Extremo in various modelling environments can be achieved with low effort, and that the tool is able to handle information from most existing technological spaces.}
}
@article{CICCONETO2022105005,
title = {GeoReservoir: An ontology for deep-marine depositional system geometry description},
journal = {Computers & Geosciences},
volume = {159},
pages = {105005},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2021.105005},
url = {https://www.sciencedirect.com/science/article/pii/S0098300421002880},
author = {Fernando Cicconeto and Lucas Valadares Vieira and Mara Abel and Renata dos Santos Alvarenga and Joel Luis Carbonera and Luan Fonseca Garcia},
keywords = {Ontology, Deep-marine depositional system, Turbidite, Artificial intelligence},
abstract = {An ontology is a logical theory that accounts for a domain vocabulary’s intended meaning, allowing us to develop a computational artifact that explicitly and formally represents the community’s conceptualization related to a vocabulary. This paper presents the GeoReservoir ontology — the result of the ontological analysis of the terminology adopted by geologists in sedimentological studies about deep-marine depositional systems, which is one of the most relevant types of oil and gas reservoirs around the world. Despite the variety of studies describing the patterns of productive reservoirs, this domain demanded new approaches in conceptual modeling methodologies because the available terminology presents issues such as ambiguity and contamination of different interpretations in the terms’ definitions. Previous work has dealt with these issues in geology, resulting in the GeoCore, an ontology that explicitly defines the most generic entities in geology. However, the domain in focus still demanded a specialized ontology containing the particular terms found in reports about deep-marine deposits. GeoReservoir is an extension of GeoCore, which, in its turn, extends the Basic Formal Ontology (BFO), a foundational ontology for scientific domains. GeoReservoir takes advantage of both BFO and GeoCore’s foundations, allowing us to focus our effort on defining the domain-specific entities. A team of professional reservoir geologists supported the knowledge acquisition process. We developed the ontology in iterative steps from an initial prototype to a complete artifact containing the taxonomy of entities and the relations between the entities, being increasingly refined by the experts. We further present a case study demonstrating how one could describe a depositional system in GeoReservoir terms. Moreover, we validated the ontology against defined competency questions (CQs). This work’s final result is offering a sound, consistent and unambiguous terminology to support the integration of data and knowledge about deep-marine depositional system geometry and lithology.}
}
@article{FISCHBACH2023111549,
title = {Automatic creation of acceptance tests by extracting conditionals from requirements: NLP approach and case study},
journal = {Journal of Systems and Software},
volume = {197},
pages = {111549},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111549},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222002254},
author = {Jannik Fischbach and Julian Frattini and Andreas Vogelsang and Daniel Mendez and Michael Unterkalmsteiner and Andreas Wehrle and Pablo Restrepo Henao and Parisa Yousefi and Tedi Juricic and Jeannette Radduenz and Carsten Wiecher},
keywords = {Acceptance testing, Automatic test case creation, Requirements engineering, Natural language processing, Causality extraction},
abstract = {Acceptance testing is crucial to determine whether a system fulfills end-user requirements. However, the creation of acceptance tests is a laborious task entailing two major challenges: (1) practitioners need to determine the right set of test cases that fully covers a requirement, and (2) they need to create test cases manually due to insufficient tool support. Existing approaches for automatically deriving test cases require semi-formal or even formal notations of requirements, though unrestricted natural language is prevalent in practice. In this paper, we present our tool-supported approach CiRA (Conditionals in Requirements Artifacts) capable of creating the minimal set of required test cases from conditional statements in informal requirements. We demonstrate the feasibility of CiRA in a case study with three industry partners. In our study, out of 578 manually created test cases, 71.8 % can be generated automatically. Additionally, CiRA discovered 80 relevant test cases that were missed in manual test case design. CiRA is publicly available at www.cira.bth.se/demo/.}
}
@article{FANG2022677,
title = {Image-based thickener mud layer height prediction with attention mechanism-based CNN},
journal = {ISA Transactions},
volume = {128},
pages = {677-689},
year = {2022},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2021.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0019057821005620},
author = {Chenyu Fang and Dakuo He and Kang Li and Yan Liu and Fuli Wang},
keywords = {Convolutional neural network, Attention mechanism, Thickener mud layer height, Image processing},
abstract = {Mud layer height of thickener is the key quality index of thickening process which is difficult to achieve real-time detection with existing methods in reality. While the need of developing a soft sensor model which can be used for real-time detection of mud layer height, we proposed an end-to-end mud layer height prediction method with attention mechanism-based convolutional neural network (CNN). The dynamic features are firstly extracted from the image samples based on CNN, and then two types of attention mechanism are embedded sequentially to contribute to more precise prediction results. Compared with the traditional spatial attention mechanism, the regional spatial attention mechanism we proposed selectively divides the spatial feature map into regions, while regions containing important features are assigned larger weights. Adding the channel and regional spatial attention mechanism in CNN not only effectively improve both the precision and calculation speed, but also affect the dimension of the output feature map, so as to avoid the loss of channel or spatial attention information of the feature map. To verify the validity of the proposed method, different attention mechanisms are embedded in the CNN, and the corresponding experiments are carried out on the dataset of the thickener mud layer. The experimental results demonstrate the feasibility and effectiveness of the mud layer height prediction method.}
}
@article{OUARET201960,
title = {AuMixDw: Towards an automated hybrid approach for building XML data warehouses},
journal = {Data & Knowledge Engineering},
volume = {120},
pages = {60-82},
year = {2019},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2019.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X18302477},
author = {Zoubir Ouaret and Doulkifli Boukraa and Omar Boussaid and Rachid Chalal},
keywords = {XML, SBVR, BIR, Data warehouse, User requirement, Dimensional modelling},
abstract = {In this paper, we present a mixed approach for building XML data warehouses from both XML data sources and user requirements. Our proposed approach aims at obtaining a unique multidimensional schema of theXML data warehouse. The approach follows three steps. During the first step, an intermediate SBVR model extended with template rules is used to accommodate a data warehousing system and to facilitate the automatic identification of facts and dimensions from the user requirements. After modelling XML data sources in UML, the second step corresponds to identifying candidate DW schemata from such data sources. The third step compares these candidate schemata with the reference model obtained from the user requirements. In this step, we propose to adapt similarity metric-extended Boolean models (BIR) and to use them in order to measure, rank and select the most appropriate data warehouse schema. Such a schema should best describe the data sources and exhaustively cover all the needed user requirements. To demonstrate our approach, we present a case study of the bibliographic database dblp.}
}
@article{NOGUEIRA201984,
title = {Test case generation, selection and coverage from natural language},
journal = {Science of Computer Programming},
volume = {181},
pages = {84-110},
year = {2019},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2019.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167642319300024},
author = {Sidney Nogueira and Hugo Araujo and Renata Araujo and Juliano Iyoda and Augusto Sampaio},
keywords = {Test generation, Natural language, Use case models},
abstract = {In Model-based Testing (MBT), test cases are automatically generated from a formal model of the system. A disadvantage of MBT is that developers must deal with formal notations. This limitation is addressed in this paper, where use cases are used to model the system. In previous work, we have proposed an automatic strategy for generating test cases from use cases written in a Controlled Natural Language (CNL), which is an English textual notation with a well-defined grammar. Due to its precise syntax, it can be processed and translated into a formal representation for the purpose of automatic test case generation. This paper extends our previous work by proposing a state-based CNL for describing use case control flows enriched with state and data operations. We translate state-based use case descriptions into CSP processes from which test cases can be automatically generated. In addition, we show how a similar notation can be used to specify test selection via the definition of state-based test purposes, which are also translated into CSP processes. Test generation and selection are mechanised by refinement checking using the CSP tool FDR. Despite the fact that we work at a purely process algebraic level to define a test generation strategy, we are able to address model coverage criteria. Particularly, by using FDR, it is possible to have access to the underlying LTS models; we then implemented algorithms for covering events or transitions, possibly combined with selection using test purposes. We also discuss several ways of improving the efficiency of the test generation strategy. As far as we are aware, this integration between an algebraic approach to test case generation with an operational approach for coverage criteria is an original and promising insight. All steps of the strategy are integrated into a tool that provides a GUI for authoring use cases and test purposes described in the proposed CNL, so the formal CSP notation is completely hidden from the test designer. We illustrate our tool and techniques with a running example and a more elaborate case study taken from an industrial setting.}
}
@article{KUTT2023119968,
title = {Loki – the semantic wiki for collaborative knowledge engineering},
journal = {Expert Systems with Applications},
volume = {224},
pages = {119968},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119968},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423004700},
author = {Krzysztof Kutt and Grzegorz J. Nalepa},
keywords = {Knowledge engineering, Semantic wiki, Software engineering, Unit tests, Prolog},
abstract = {We present Loki, a semantic wiki designed to support the collaborative knowledge engineering process with the use of software engineering methods. Designed as a set of DokuWiki plug-ins, it provides a variety of knowledge representation methods, including semantic annotations, Prolog clauses, and business processes and rules oriented to specific tasks. Knowledge stored in Loki can be retrieved via SPARQL queries, in-line Semantic MediaWiki-like queries, or Prolog goals. Loki includes a number of useful features for a group of experts and knowledge engineers developing the wiki, such as knowledge visualization, ontology storage, or code hint and completion mechanism. Reasoning unit tests are also introduced to validate knowledge quality. The paper is complemented by the formulation of the collaborative knowledge engineering process and the description of experiments performed during Loki development to evaluate its functionality. Loki is available as free software at https://loki.re.}
}
@article{HU2023126823,
title = {Token-level disentanglement for unsupervised text style transfer},
journal = {Neurocomputing},
volume = {560},
pages = {126823},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126823},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223009463},
author = {Yahao Hu and Wei Tao and Yifei Xie and Yi Sun and Zhisong Pan},
keywords = {Natural language generation, Text style transfer, Unsupervised learning, Disentanglement, Adversarial training},
abstract = {Text style transfer is the task of altering the style of a source text to a desired style while preserving the style-independent content. A common approach involves disentangling a given sentence into a style-agnostic content representation within the latent space and then generating the text in the desired style, guided by a separate style embedding. However, previous methods have a limitation in that they assume the input sentence is encoded by a fix-sized latent vector at the sentence level, making it challenging to capture rich semantic information at the token level, especially when dealing with longer texts. Consequently, this leads to suboptimal preservation of non-stylistic semantic content. In this paper, we address this challenge, and propose TED, a fine-grained model for disentangling content and style representation at the token level to enhance content preservation. Specifically, TED use tf–idfs to estimate the pivot tokens for different styles and incorporates multi-task and adversarial objectives to disentangle the content and style information of each token within the latent space. Experimental results on two popular text style transfer datasets show that our proposed model significantly outperforms state-of-the-art baselines, particularly in terms of content preservation. Moreover, the quantitative and qualitative experiments demonstrate the effectiveness of our model in achieving token-level disentanglement within the latent space.}
}