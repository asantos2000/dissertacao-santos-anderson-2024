{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir configuration && touch configuration/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile configuration/main.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import glob\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "DEFAULT_CONFIG_DIR: str = '../config.yaml' # Google drive: \"/content/drive/MyDrive/cfr2sbvr/config.yaml\n",
    "\n",
    "def get_next_filename(file_dir: str, file_prefix: str, extension: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates the next filename in a sequence based on existing files in a directory,\n",
    "    considering the file extension.\n",
    "\n",
    "    The filename format is: `{file_prefix}-{YYYY-MM-DD}-{N}.{extension}`,\n",
    "    where `N` is an incrementing integer for files with the same date.\n",
    "\n",
    "    Args:\n",
    "        file_dir (str): The directory where the files are stored.\n",
    "        file_prefix (str): The prefix used in the filenames.\n",
    "        extension (str): The file extension (e.g., 'json', 'txt').\n",
    "\n",
    "    Returns:\n",
    "        str: The full path to the next filename in the sequence.\n",
    "\n",
    "    Example:\n",
    "        next_file = get_next_filename(DEFAULT_CHECKPOINTS_DIR, 'documents', 'json')\n",
    "        print(next_file)\n",
    "        # Output might be: ../checkpoints/documents-2024-10-19-5.json\n",
    "    \"\"\"\n",
    "    today_str: str = datetime.today().strftime('%Y-%m-%d')\n",
    "    path: str = file_dir\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    # Create the pattern dynamically using file_prefix and extension\n",
    "    pattern = re.compile(\n",
    "        r'^' + re.escape(file_prefix) + r'-(\\d{4}-\\d{2}-\\d{2})-(\\d+)\\.' + re.escape(extension) + r'$'\n",
    "    )\n",
    "\n",
    "    file_info_list = []\n",
    "\n",
    "    for filename in files:\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            date_str: str = match.group(1)\n",
    "            number: int = int(match.group(2))\n",
    "            file_info_list.append({'filename': filename, 'date': date_str, 'number': number})\n",
    "\n",
    "    if file_info_list:\n",
    "        # Sort by date and number in descending order\n",
    "        sorted_files = sorted(\n",
    "            file_info_list,\n",
    "            key=lambda x: (x['date'], x['number']),\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        latest_file_info = sorted_files[0]\n",
    "        latest_date: str = latest_file_info['date']\n",
    "        latest_number: int = latest_file_info['number']\n",
    "\n",
    "        if latest_date == today_str:\n",
    "            new_number: int = latest_number + 1\n",
    "        else:\n",
    "            new_number = 1\n",
    "    else:\n",
    "        new_number = 1\n",
    "\n",
    "    new_filename: str = f'{file_prefix}-{today_str}-{new_number}.{extension}'\n",
    "    new_filepath: str = os.path.join(path, new_filename)\n",
    "\n",
    "    return new_filepath\n",
    "\n",
    "\n",
    "# Load the YAML config file\n",
    "def load_config(config_file: str = None):\n",
    "    if config_file is None:\n",
    "        config_file = DEFAULT_CONFIG_DIR\n",
    "    try:\n",
    "        with open(config_file, \"r\") as file:\n",
    "            config = yaml.safe_load(file)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Configuration file {config_file} not found.\")\n",
    "    except yaml.YAMLError as exc:\n",
    "        raise ValueError(f\"Error parsing YAML file {config_file}: {exc}\")\n",
    "\n",
    "    # Ensure config structure is correct\n",
    "    if \"LLM\" not in config or \"DEFAULT_CHECKPOINT_DIR\" not in config:\n",
    "        raise ValueError(\"Required configuration keys are missing in the config file.\")\n",
    "\n",
    "    # Set the OpenAI API key from environment variable if it's not set in config\n",
    "    config[\"LLM\"][\"OPENAI_API_KEY\"] = os.getenv(\n",
    "        \"OPENAI_API_KEY\", config[\"LLM\"].get(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "\n",
    "    # Dynamically set checkpoint and report files using the get_next_filename function\n",
    "    config[\"DEFAULT_CHECKPOINT_FILE\"] = get_next_filename(\n",
    "        config[\"DEFAULT_CHECKPOINT_DIR\"], \"documents\", \"json\"\n",
    "    )\n",
    "    config[\"DEFAULT_EXTRACTION_REPORT_FILE\"] = get_next_filename(\n",
    "        config[\"DEFAULT_OUTPUT_DIR\"], \"extraction_report\", \"html\"\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  # backup on Google Drive\n",
    "  !cp -r configuration /content/drive/MyDrive/cfr2sbvr/modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configuration.main as configuration\n",
    "\n",
    "# Development mode\n",
    "import importlib\n",
    "importlib.reload(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configuration.load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "Checkpoints are stored / retrieved at the directory `DEFAULT_CHECKPOINT_FILE` in the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir checkpoint && touch checkpoint/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting checkpoint/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile checkpoint/main.py\n",
    "\n",
    "from typing import List, Dict, Optional, Any, Tuple, Set\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "\n",
    "# Set up basic logging configuration for the checkpoint module\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to INFO or another level as needed\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Log format\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def convert_set_to_list(data: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Recursively converts sets to lists in the data structure.\n",
    "\n",
    "    Args:\n",
    "        data (Any): The data structure to process, which can be a dict, list, set, or other types.\n",
    "\n",
    "    Returns:\n",
    "        Any: The data structure with all sets converted to lists.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {key: convert_set_to_list(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_set_to_list(item) for item in data]\n",
    "    elif isinstance(data, set):\n",
    "        return list(data)\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n",
    "# Define a model for the Document\n",
    "class Document(BaseModel):\n",
    "    id: str\n",
    "    type: str  # New field to represent the type of the document\n",
    "    content: Any  # Content can be any data type: list, dict, string, etc.\n",
    "\n",
    "# Define the DocumentManager class\n",
    "class DocumentManager(BaseModel):\n",
    "    documents: Dict[Tuple[str, str], Document] = Field(default_factory=dict)  # Keys are tuples (id, type)\n",
    "\n",
    "    def add_document(self, doc: Document) -> None:\n",
    "        \"\"\"\n",
    "        Adds a document to the manager.\n",
    "\n",
    "        Args:\n",
    "            doc (Document): The document to add.\n",
    "        \"\"\"\n",
    "        key = (doc.id, doc.type)\n",
    "        self.documents[key] = doc\n",
    "\n",
    "    def retrieve_document(self, doc_id: str, doc_type: str) -> Optional[Document]:\n",
    "        \"\"\"\n",
    "        Retrieves a document by its id and type.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): The ID of the document.\n",
    "            doc_type (str): The type of the document.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Document]: The retrieved document, or None if not found.\n",
    "        \"\"\"\n",
    "        key = (doc_id, doc_type)\n",
    "        return self.documents.get(key)\n",
    "\n",
    "    def list_document_ids(self, doc_type: Optional[str] = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Lists all document ids, optionally filtered by type.\n",
    "\n",
    "        Args:\n",
    "            doc_type (Optional[str], optional): The type of documents to list. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of document ids.\n",
    "        \"\"\"\n",
    "        if doc_type:\n",
    "            return [doc_id for (doc_id, d_type) in self.documents.keys() if d_type == doc_type]\n",
    "        else:\n",
    "            return [doc_id for (doc_id, _) in self.documents.keys()]\n",
    "\n",
    "    def exclude_document(self, doc_id: str, doc_type: str) -> None:\n",
    "        \"\"\"\n",
    "        Excludes a document by its id and type.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): The ID of the document to exclude.\n",
    "            doc_type (str): The type of the document.\n",
    "        \"\"\"\n",
    "        key = (doc_id, doc_type)\n",
    "        if key in self.documents:\n",
    "            del self.documents[key]\n",
    "\n",
    "    def persist_to_file(self, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Persists the current state to a file, converting tuple keys to strings and sets to lists.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The filename to save the documents.\n",
    "        \"\"\"\n",
    "        serializable_documents = {f\"{doc_id}|{doc_type}\": convert_set_to_list(doc.dict()) for (doc_id, doc_type), doc in self.documents.items()}\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(serializable_documents, file, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def restore_from_file(cls, filename: str) -> 'DocumentManager':\n",
    "        \"\"\"\n",
    "        Restores the state from a file, converting string keys back to tuples.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The filename to restore the documents from.\n",
    "\n",
    "        Returns:\n",
    "            DocumentManager: The restored DocumentManager instance.\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            documents = {(doc_id.split('|')[0], doc_id.split('|')[1]): Document(**doc_data) for doc_id, doc_data in data.items()}\n",
    "            return cls(documents=documents)\n",
    "\n",
    "def restore_checkpoint(filename: Optional[str]) -> DocumentManager:\n",
    "    \"\"\"\n",
    "    Restores the document manager from a checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        filename (str, optional): The path to the checkpoint file. Defaults to DEFAULT_CHECKPOINT_FILE.\n",
    "\n",
    "    Returns:\n",
    "        DocumentManager: The restored DocumentManager instance.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the checkpoint file does not exist.\n",
    "\n",
    "    See Also:\n",
    "        - Reset the values delete the documents.json file and run: manager = DocumentManager()\n",
    "        - Restore the state from the documents.json file, run: DocumentManager.restore_from_file(\"documents.json\")\n",
    "        - Exclue a document: manager.exclude_document(doc_id=\"§ 275.0-2\", doc_type=\"section\")\n",
    "        - List documents: manager.list_document_ids(doc_type=\"section\")\n",
    "        - Get a document: manager.retrieve_document(doc_id=doc, doc_type=\"section\")\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        restored_docs = DocumentManager.restore_from_file(filename)\n",
    "        logger.info(f\"Checkpoint restored from {filename}.\")\n",
    "    except (FileNotFoundError, JSONDecodeError):\n",
    "        restored_docs = DocumentManager()\n",
    "        logger.error(f\"Checkpoint file '{filename}' not found or is empty, initializing new checkpoint.\")\n",
    "    return restored_docs\n",
    "\n",
    "def save_checkpoint(filename: Optional[str], manager: DocumentManager) -> None:\n",
    "    \"\"\"\n",
    "    Saves the current state of the DocumentManager to a checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        manager (DocumentManager): The DocumentManager instance to save.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error saving the checkpoint.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        manager.persist_to_file(filename=filename)\n",
    "        logger.info(\"Checkpoint saved.\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(\"Error saving checkpoint. Check the directory path and permissions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir logging_setup && touch logging_setup/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logging_setup/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile logging_setup/main.py\n",
    "\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "\n",
    "\n",
    "def setting_logging(log_path: str, log_level: str):\n",
    "    # Ensure the ../logs directory exists\n",
    "    log_directory = Path.cwd() / log_path\n",
    "    log_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Path for the log file\n",
    "    log_file_path = log_directory / \"application.log\"\n",
    "\n",
    "    # Set up TimedRotatingFileHandler to rotate logs every day\n",
    "    file_handler = TimedRotatingFileHandler(\n",
    "        log_file_path,\n",
    "        when=\"midnight\",\n",
    "        interval=1,\n",
    "        backupCount=0,  # Rotate every midnight, keep all backups\n",
    "    )\n",
    "\n",
    "    # Set the file handler's log format\n",
    "    file_handler.setFormatter(\n",
    "        logging.Formatter(\n",
    "            \"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Set up logging configuration\n",
    "    logging.basicConfig(\n",
    "        level=log_level,  # Set to the desired log level\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Console log format\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",  # Custom date format\n",
    "        handlers=[\n",
    "            file_handler,  # Log to the rotating file in ../logs\n",
    "            logging.StreamHandler(),  # Log to console\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Example logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Log a test message to verify\n",
    "    logger.info(\"Logging is set up with daily rotation.\")\n",
    "\n",
    "    return logger"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipt-cfr2sbvr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
