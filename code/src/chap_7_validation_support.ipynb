{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation support\n",
    "\n",
    "Evaluating of consistency of the checkpoints.\n",
    "\n",
    "Dependencies:\n",
    "- Copy checkpoints files from evaluation and extraction to `cfr2sbvr_db`\n",
    "- Python DuckDB: `pip install duckdb --upgrade`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  !rm -rf cfr2sbvr configuration checkpoint\n",
    "  !git clone https://github.com/asantos2000/master-degree-santos-anderson.git cfr2sbvr\n",
    "  %pip install -r cfr2sbvr/code/requirements.txt\n",
    "  !cp -r cfr2sbvr/code/src/configuration .\n",
    "  !cp -r cfr2sbvr/code/src/checkpoint .\n",
    "  !cp -r cfr2sbvr/code/config.colab.yaml config.yaml\n",
    "  DEFAULT_CONFIG_FILE=\"config.yaml\"\n",
    "else:\n",
    "  DEFAULT_CONFIG_FILE=\"../config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import json\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import humanize\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Local modules\n",
    "import configuration.main as configuration\n",
    "import logging_setup.main as logging_setup\n",
    "\n",
    "DEV_MODE = True\n",
    "\n",
    "if DEV_MODE:\n",
    "    # Development mode\n",
    "    import importlib\n",
    "\n",
    "    importlib.reload(configuration)\n",
    "    importlib.reload(logging_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load configuration\n",
    "config = configuration.load_config(DEFAULT_CONFIG_FILE)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 11:39:05 - INFO - Logging is set up with daily rotation.\n"
     ]
    }
   ],
   "source": [
    "logger = logging_setup.setting_logging(config[\"DEFAULT_LOG_DIR\"], config[\"LOG_LEVEL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_horizontal_bar_chart(data, group_col, value_col, title, x_label, y_label, figsize=(10, 4), legend_position=\"best\", big_numbers={\"factor\":1000, \"suffix\":\"mil\"}):\n",
    "    \"\"\"    \n",
    "    :param data: DataFrame\n",
    "    :param group_col: Y values\n",
    "    :param value_col: X values\n",
    "    :param title: Title\n",
    "    :param x_label: X label\n",
    "    :param y_label: Y label\n",
    "    :param figsize: Picture size\n",
    "    :param legend_position: Legend position ('upper right', 'lower right', etc.), ou None para posicionar automaticamente\n",
    "    \"\"\"\n",
    "    # Agrupando os dados e somando os valores\n",
    "    filterd = data.groupby(group_col)[value_col].sum().reset_index()\n",
    "\n",
    "    # Calculando a média para o destaque\n",
    "    avg_value = filterd[value_col].mean()\n",
    "\n",
    "    # Configurando o tamanho do gráfico\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Adicionando linhas horizontais para conectar os marcadores ao eixo Y\n",
    "    colors = [\"#E69F00\" if val > avg_value else \"#56B4E9\" for val in filterd[value_col]]\n",
    "    for i, (group, val, color) in enumerate(zip(filterd[group_col], filterd[value_col], colors)):\n",
    "        plt.plot(\n",
    "            [0, val],\n",
    "            [group, group],\n",
    "            color=color,\n",
    "            linewidth=2,\n",
    "            zorder=1\n",
    "        )\n",
    "\n",
    "    # Adicionando os marcadores com cores amigáveis para daltônicos\n",
    "    plt.scatter(\n",
    "        filterd[value_col],\n",
    "        filterd[group_col],\n",
    "        color=colors,\n",
    "        s=100,  # Tamanho dos marcadores\n",
    "        zorder=2\n",
    "    )\n",
    "\n",
    "    # Adicionando uma linha vertical para a média\n",
    "    plt.axvline(\n",
    "        avg_value,\n",
    "        color=\"grey\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1.5,\n",
    "        label=f'Média: {avg_value / big_numbers[\"factor\"]:,.2f} {big_numbers[\"suffix\"]}'.replace('.', ',')\n",
    "    )\n",
    "\n",
    "    # Adicionando valores diretamente ao lado dos marcadores\n",
    "    for group, val in zip(filterd[group_col], filterd[value_col]):\n",
    "        plt.text(\n",
    "            val + 0.05 * avg_value,  # Posicionando à direita\n",
    "            group,\n",
    "            f'{val / big_numbers[\"factor\"]:,.2f} {big_numbers[\"suffix\"]}'.replace('.', ','),\n",
    "            va=\"center\",\n",
    "            fontsize=10,\n",
    "            zorder=3\n",
    "        )\n",
    "\n",
    "    # Adicionando linhas verticais no eixo X com cor cinza claro\n",
    "    plt.gca().xaxis.grid(color=\"lightgrey\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "    # Função personalizada para formatar os ticks do eixo X\n",
    "    def format_ticks(x, pos):\n",
    "        return f'{x / big_numbers[\"factor\"]:,.1f} {big_numbers[\"suffix\"]}'.replace('.', ',')\n",
    "\n",
    "    # Aplicando a formatação ao eixo X\n",
    "    plt.gca().xaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
    "\n",
    "    # Ajustando o título e os eixos\n",
    "    plt.title(title, fontsize=16, loc=\"left\", pad=20)\n",
    "    plt.xlabel(x_label, fontsize=14)\n",
    "    plt.ylabel(y_label, fontsize=14)\n",
    "\n",
    "    # Ajustando os limites do eixo Y para compactar os valores\n",
    "    plt.gca().set_ylim(-0.2, len(filterd[group_col]) - 0.8)\n",
    "\n",
    "    # Removendo as linhas superior, direita e esquerda (spines)\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    # Configurando o layout e a legenda\n",
    "    plt.legend(loc=legend_position, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Mostrando o gráfico\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_table_from_json(\n",
    "    conn,\n",
    "    table_name,\n",
    "    source,\n",
    "    key_value,\n",
    "    content_key,\n",
    "    alias,\n",
    "    drop=False,\n",
    "    doc_id_key=\"id\",\n",
    "    key_pattern1=\"\",\n",
    "    key_pattern2=\"\",\n",
    "    nested=True\n",
    ") -> bool:\n",
    "    \n",
    "    logger.info(f\"{table_name=}\")\n",
    "    logger.info(f\"{source=}\")\n",
    "    logger.info(f\"{key_value=}\")\n",
    "    logger.info(f\"{content_key=}\")\n",
    "    logger.info(f\"{alias=}\")\n",
    "    logger.info(f\"{drop=}\")\n",
    "    \n",
    "    _data = []\n",
    "    _source_file = Path(source).name\n",
    "\n",
    "    logger.info(f\"{_source_file=}\")\n",
    "\n",
    "    directory_path = Path(f\"{config['DEFAULT_DATA_DIR']}/temp\")\n",
    "\n",
    "    logger.info(f\"{directory_path=}\")\n",
    "\n",
    "    # Combine the directory path and the filename\n",
    "    _temp_file = directory_path / _source_file\n",
    "\n",
    "    logger.info(f\"{_temp_file=}\")\n",
    "\n",
    "    with open(source, \"r\") as f:\n",
    "        loaded_data = json.load(f)\n",
    "\n",
    "    keys = loaded_data.keys()\n",
    "\n",
    "    logger.info(f\"{len(keys)=}\")\n",
    "\n",
    "    for key in keys:\n",
    "        if (\n",
    "            key_pattern1 in key and key_pattern2 in key\n",
    "        ):  # key key.startswith(prefix_key_pattern) and key.endswith(suffix_key_pattern):\n",
    "            logger.info(key)\n",
    "            _data.append(loaded_data[key])\n",
    "\n",
    "    if not _data:\n",
    "        logger.info(\"No data found. Check the key pattern.\")\n",
    "        return False\n",
    "\n",
    "    with open(_temp_file, \"w\") as f:\n",
    "        json.dump(_data, f, indent=4)\n",
    "\n",
    "    if drop:\n",
    "        _query_drop_table = f\"\"\"\n",
    "        DROP TABLE IF EXISTS {table_name};\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"{_query_drop_table=}\")\n",
    "        logger.info(f\"Trying to drop table because drop parameter is {drop}\")\n",
    "\n",
    "        try:\n",
    "            conn.execute(_query_drop_table)\n",
    "            logger.info(f\"Table {table_name} dropped\")\n",
    "        except duckdb.CatalogException as e:\n",
    "            logger.info(e)\n",
    "\n",
    "    unnest_clause = f\"unnest({content_key}) as {alias}\" if nested else f\"{content_key} as {alias}\"\n",
    "\n",
    "    _query_insert_data = f\"\"\"\n",
    "    INSERT INTO {table_name} ({doc_id_key}, prompt, file_source, {alias}, created_at)\n",
    "    SELECT \n",
    "        {doc_id_key}, \n",
    "        '{key_value}' as prompt, \n",
    "        '{_source_file}' as file_source, \n",
    "        {unnest_clause},\n",
    "        now() as created_at\n",
    "    FROM \n",
    "        read_json_auto(\"{_temp_file}\");\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(f\"{_query_insert_data=}\")\n",
    "    logger.info(f\"Trying to insert into {table_name}\")\n",
    "\n",
    "    try:\n",
    "        conn.execute(_query_insert_data)\n",
    "        logger.info(f\"Data inserted into {table_name}.\")\n",
    "    except duckdb.CatalogException as e:\n",
    "        logger.info(e)\n",
    "        logger.info(f\"Failed to insert, trying create {table_name}\")\n",
    "        _query_create_table = f\"\"\"\n",
    "        CREATE TABLE {table_name} AS\n",
    "        SELECT {doc_id_key}, \n",
    "        '{key_value}' as prompt, \n",
    "        '{_source_file}' as file_source, \n",
    "        {unnest_clause},\n",
    "        now() as 'created_at'\n",
    "        FROM read_json_auto(\"{_temp_file}\");\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"{_query_create_table=}\")\n",
    "            conn.execute(_query_create_table)\n",
    "            logger.info(f\"Table {table_name} created and loaded.\")\n",
    "        except duckdb.CatalogException as e:\n",
    "            logger.info(e)\n",
    "            logger.info(f\"Failed to create {table_name}\")\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect local or cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DB=False\n",
    "DATABASE = \"cfr2sbvr_v5\"\n",
    "CLOUD_DATABASE = \"cfr2sbvr_v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 11:46:06 - INFO - Connected to ☁️ cfr2sbvr_v5 database\n"
     ]
    }
   ],
   "source": [
    "if LOCAL_DB:\n",
    "    conn = duckdb.connect(f'{config[\"DEFAULT_APP_DIR\"]}/data/{DATABASE}.db')\n",
    "else:\n",
    "    mother_duck_token=os.getenv(\"MOTHER_DUCK_TOKEN\")\n",
    "    conn = duckdb.connect(f'md:{CLOUD_DATABASE}?motherduck_token={mother_duck_token}')\n",
    "\n",
    "logger.info(f\"Connected to {'🖥️' if LOCAL_DB else '☁️'} {DATABASE} database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_UP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CHECKPOINT_METADATA',),\n",
       " ('RAW_CLASSIFY_P1_OPERATIVE_RULES',),\n",
       " ('RAW_CLASSIFY_P1_OPERATIVE_RULES_TRUE',),\n",
       " ('RAW_CLASSIFY_P1_OPERATIVE_RULES_VW',),\n",
       " ('RAW_CLASSIFY_P2_DEFINITIONAL_FACTS',),\n",
       " ('RAW_CLASSIFY_P2_DEFINITIONAL_FACTS_TRUE',),\n",
       " ('RAW_CLASSIFY_P2_DEFINITIONAL_FACTS_VW',),\n",
       " ('RAW_CLASSIFY_P2_DEFINITIONAL_NAMES',),\n",
       " ('RAW_CLASSIFY_P2_DEFINITIONAL_NAMES_TRUE',),\n",
       " ('RAW_CLASSIFY_P2_DEFINITIONAL_NAMES_VW',),\n",
       " ('RAW_CLASSIFY_P2_DEFINITIONAL_TERMS',),\n",
       " ('RAW_CLASSIFY_P2_DEFINITIONAL_TERMS_TRUE',),\n",
       " ('RAW_CLASSIFY_P2_DEFINITIONAL_TERMS_VW',),\n",
       " ('RAW_CLASSIFY_P2_OPERATIVE_RULES',),\n",
       " ('RAW_CLASSIFY_P2_OPERATIVE_RULES_TRUE',),\n",
       " ('RAW_CLASSIFY_P2_OPERATIVE_RULES_VW',),\n",
       " ('RAW_CLASSIFY_VW',),\n",
       " ('RAW_ELAPSED_TIME',),\n",
       " ('RAW_ELAPSED_TIME_VW',),\n",
       " ('RAW_LLM_COMPLETION',),\n",
       " ('RAW_LLM_COMPLETION_VW',),\n",
       " ('RAW_LLM_VALIDATION',),\n",
       " ('RAW_LLM_VALIDATION_BEST_VW',),\n",
       " ('RAW_LLM_VALIDATION_VW',),\n",
       " ('RAW_SECTION',),\n",
       " ('RAW_SECTION_EXTRACTED_ELEMENTS_VW',),\n",
       " ('RAW_SECTION_P1_EXTRACTED_ELEMENTS',),\n",
       " ('RAW_SECTION_P1_EXTRACTED_ELEMENTS_TRUE',),\n",
       " ('RAW_SECTION_P1_EXTRACTED_ELEMENTS_VW',),\n",
       " ('RAW_SECTION_P2_EXTRACTED_NOUN',),\n",
       " ('RAW_SECTION_P2_EXTRACTED_NOUN_TRUE',),\n",
       " ('RAW_SECTION_P2_EXTRACTED_NOUN_VW',),\n",
       " ('RAW_TRANSFORM_ELEMENTS_VW',),\n",
       " ('RAW_TRANSFORM_FACT_TYPES',),\n",
       " ('RAW_TRANSFORM_FACT_TYPES_VW',),\n",
       " ('RAW_TRANSFORM_NAMES',),\n",
       " ('RAW_TRANSFORM_NAMES_VW',),\n",
       " ('RAW_TRANSFORM_OPERATIVE_RULES',),\n",
       " ('RAW_TRANSFORM_OPERATIVE_RULES_VW',),\n",
       " ('RAW_TRANSFORM_TERMS',),\n",
       " ('RAW_TRANSFORM_TERMS_VW',)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SHOW TABLES;\n",
    "\"\"\"\n",
    "\n",
    "tables = conn.sql(query).fetchall()\n",
    "\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cfr2sbvr_v4', 'main', 'RAW_TRANSFORM_TERMS', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_TRANSFORM_FACT_TYPES', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_SECTION_P2_EXTRACTED_NOUN_TRUE', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_SECTION_P2_EXTRACTED_NOUN', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_SECTION_P1_EXTRACTED_ELEMENTS', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_SECTION', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4',\n",
       "  'main',\n",
       "  'RAW_SECTION_P1_EXTRACTED_ELEMENTS_TRUE',\n",
       "  'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_LLM_COMPLETION', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_ELAPSED_TIME', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4',\n",
       "  'main',\n",
       "  'RAW_CLASSIFY_P2_DEFINITIONAL_TERMS_TRUE',\n",
       "  'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_CLASSIFY_P2_OPERATIVE_RULES', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_CLASSIFY_P2_DEFINITIONAL_TERMS', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4',\n",
       "  'main',\n",
       "  'RAW_CLASSIFY_P2_DEFINITIONAL_NAMES_TRUE',\n",
       "  'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_LLM_VALIDATION', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_CLASSIFY_P2_OPERATIVE_RULES_TRUE', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_CLASSIFY_P2_DEFINITIONAL_NAMES', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_TRANSFORM_NAMES', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4',\n",
       "  'main',\n",
       "  'RAW_CLASSIFY_P2_DEFINITIONAL_FACTS_TRUE',\n",
       "  'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_CLASSIFY_P2_DEFINITIONAL_FACTS', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_TRANSFORM_OPERATIVE_RULES', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_CLASSIFY_P1_OPERATIVE_RULES_TRUE', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_CLASSIFY_P1_OPERATIVE_RULES', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'CHECKPOINT_METADATA', 'BASE TABLE'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_TRANSFORM_NAMES_VW', 'VIEW'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_CLASSIFY_P2_DEFINITIONAL_FACTS_VW', 'VIEW'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_CLASSIFY_P2_DEFINITIONAL_NAMES_VW', 'VIEW'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_CLASSIFY_P2_OPERATIVE_RULES_VW', 'VIEW'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_CLASSIFY_VW', 'VIEW'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_ELAPSED_TIME_VW', 'VIEW'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_LLM_VALIDATION_BEST_VW', 'VIEW'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_LLM_COMPLETION_VW', 'VIEW'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_CLASSIFY_P2_DEFINITIONAL_TERMS_VW', 'VIEW'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_LLM_VALIDATION_VW', 'VIEW'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_TRANSFORM_TERMS_VW', 'VIEW'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_SECTION_EXTRACTED_ELEMENTS_VW', 'VIEW'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_SECTION_P1_EXTRACTED_ELEMENTS_VW', 'VIEW'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_SECTION_P2_EXTRACTED_NOUN_VW', 'VIEW'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_TRANSFORM_ELEMENTS_VW', 'VIEW'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_CLASSIFY_P1_OPERATIVE_RULES_VW', 'VIEW'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_TRANSFORM_OPERATIVE_RULES_VW', 'VIEW'),\n",
       " ('cfr2sbvr_v4', 'main', 'RAW_TRANSFORM_FACT_TYPES_VW', 'VIEW')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "SELECT table_catalog, table_schema, table_name, table_type\n",
    "FROM information_schema.tables\n",
    "WHERE table_type in ('BASE TABLE', 'VIEW')\n",
    "AND table_catalog = '{DATABASE if LOCAL_DB else CLOUD_DATABASE}'\n",
    "AND table_schema = 'main';\n",
    "\"\"\"\n",
    "\n",
    "tables_or_views = conn.sql(query).fetchall()\n",
    "\n",
    "tables_or_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tables_or_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.CHECKPOINT_METADATA;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_CLASSIFY_P1_OPERATIVE_RULES;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_CLASSIFY_P1_OPERATIVE_RULES_TRUE;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_CLASSIFY_P2_DEFINITIONAL_FACTS;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_CLASSIFY_P2_DEFINITIONAL_FACTS_TRUE;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_CLASSIFY_P2_DEFINITIONAL_NAMES;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_CLASSIFY_P2_DEFINITIONAL_NAMES_TRUE;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_CLASSIFY_P2_DEFINITIONAL_TERMS;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_CLASSIFY_P2_DEFINITIONAL_TERMS_TRUE;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_CLASSIFY_P2_OPERATIVE_RULES;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_CLASSIFY_P2_OPERATIVE_RULES_TRUE;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_ELAPSED_TIME;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_LLM_COMPLETION;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_LLM_VALIDATION;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_SECTION;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_SECTION_P1_EXTRACTED_ELEMENTS;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_SECTION_P1_EXTRACTED_ELEMENTS_TRUE;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_SECTION_P2_EXTRACTED_NOUN;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_SECTION_P2_EXTRACTED_NOUN_TRUE;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_TRANSFORM_FACT_TYPES;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_TRANSFORM_NAMES;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_TRANSFORM_OPERATIVE_RULES;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP TABLE IF EXISTS main.RAW_TRANSFORM_TERMS;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP VIEW IF EXISTS main.RAW_CLASSIFY_P1_OPERATIVE_RULES_VW;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP VIEW IF EXISTS main.RAW_CLASSIFY_P2_DEFINITIONAL_FACTS_VW;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP VIEW IF EXISTS main.RAW_CLASSIFY_P2_DEFINITIONAL_NAMES_VW;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP VIEW IF EXISTS main.RAW_CLASSIFY_P2_DEFINITIONAL_TERMS_VW;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP VIEW IF EXISTS main.RAW_CLASSIFY_P2_OPERATIVE_RULES_VW;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP VIEW IF EXISTS main.RAW_CLASSIFY_VW;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP VIEW IF EXISTS main.RAW_ELAPSED_TIME_VW;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP VIEW IF EXISTS main.RAW_LLM_COMPLETION_VW;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP VIEW IF EXISTS main.RAW_LLM_VALIDATION_BEST_VW;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP VIEW IF EXISTS main.RAW_LLM_VALIDATION_VW;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP VIEW IF EXISTS main.RAW_SECTION_EXTRACTED_ELEMENTS_VW;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP VIEW IF EXISTS main.RAW_SECTION_P1_EXTRACTED_ELEMENTS_VW;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP VIEW IF EXISTS main.RAW_SECTION_P2_EXTRACTED_NOUN_VW;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP VIEW IF EXISTS main.RAW_TRANSFORM_ELEMENTS_VW;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP VIEW IF EXISTS main.RAW_TRANSFORM_FACT_TYPES_VW;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP VIEW IF EXISTS main.RAW_TRANSFORM_NAMES_VW;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP VIEW IF EXISTS main.RAW_TRANSFORM_OPERATIVE_RULES_VW;\n",
      "    \n",
      "2025-01-16 11:39:53 - INFO - \n",
      "    DROP VIEW IF EXISTS main.RAW_TRANSFORM_TERMS_VW;\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for table_or_view_name in tables_or_views:\n",
    "    object_schema = table_or_view_name[1]\n",
    "    object_name = table_or_view_name[2]\n",
    "    object_type = table_or_view_name[3]\n",
    "\n",
    "    query = f\"\"\"\n",
    "    DROP {'TABLE' if object_type == 'BASE TABLE' else 'VIEW'} IF EXISTS {object_schema}.{object_name};\n",
    "    \"\"\"\n",
    "\n",
    "    # If True then drop tables and views\n",
    "    if CLEAN_UP:\n",
    "        logger.info(query)\n",
    "        conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "DROP TABLE IF EXISTS CHECKPOINT_METADATA;\n",
    "\n",
    "CREATE TABLE CHECKPOINT_METADATA AS\n",
    "SELECT process,\n",
    "doc_source,\n",
    "doc_id,\n",
    "doc_type,\n",
    "table_name, \n",
    "now() as 'created_at'\n",
    "FROM read_csv_auto(\"{config['DEFAULT_APP_DIR']}/data/metadata/checkpoints_metadata.csv\");\n",
    "\"\"\"\n",
    "\n",
    "if CLEAN_UP:\n",
    "    conn.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *,\n",
    "FROM CHECKPOINT_METADATA;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT TABLE_NAME,\n",
    "FROM CHECKPOINT_METADATA\n",
    "WHERE DOC_SOURCE = 'both'\n",
    "ORDER BY 1;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "CFR sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_table_from_json(\n",
    "    conn,\n",
    "    key_pattern1=\"|section\",\n",
    "    key_pattern2=\"§\",\n",
    "    table_name=\"RAW_SECTION\",\n",
    "    source=f\"{config['DEFAULT_DATA_DIR']}/documents_true_table.json\",\n",
    "    key_value=\"section\",\n",
    "    drop=True,\n",
    "    content_key=\"content\",\n",
    "    alias=\"content\",\n",
    "    doc_id_key=\"id\",\n",
    "    nested=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *,\n",
    "FROM RAW_SECTION;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *,\n",
    "FROM CHECKPOINT_METADATA\n",
    "WHERE PROCESS='extraction'\n",
    "ORDER BY DOC_ID, DOC_TYPE DESC;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract elements from sections (P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_table_from_json(\n",
    "    conn,\n",
    "    key_pattern1=\"_P1|true_table\",\n",
    "    key_pattern2=\"§\",\n",
    "    table_name=\"RAW_SECTION_P1_EXTRACTED_ELEMENTS_TRUE\",\n",
    "    source=f\"{config['DEFAULT_DATA_DIR']}/documents_true_table.json\",\n",
    "    key_value=\"extract_p1\",\n",
    "    drop=True,\n",
    "    content_key=\"content.elements\",\n",
    "    alias=\"elements\",\n",
    "    doc_id_key=\"id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM RAW_SECTION_P1_EXTRACTED_ELEMENTS_TRUE;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoints_extraction to evaluate the original extraction\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_extraction\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"_P1|llm_response\",\n",
    "        key_pattern2=\"§\",\n",
    "        table_name=\"RAW_SECTION_P1_EXTRACTED_ELEMENTS\",\n",
    "        source=file_path,\n",
    "        key_value=\"extract_p1\",\n",
    "        drop=drop,\n",
    "        content_key=\"content.elements\",\n",
    "        alias=\"elements\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT count(distinct elements) as elements_count\n",
    "FROM RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT file_source, elements.classification, count(*) as count\n",
    "FROM RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "GROUP BY file_source, elements.classification\n",
    "ORDER BY file_source, elements.classification\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, elements.classification, count(*) as count\n",
    "FROM RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "GROUP BY id, elements.classification\n",
    "ORDER BY id, elements.classification\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, count(*) FROM RAW_SECTION_P1_EXTRACTED_ELEMENTS_TRUE\n",
    "GROUP BY id\n",
    "ORDER BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, count(*) FROM RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "GROUP BY id\n",
    "ORDER BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verb symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total verb symbols extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    COUNT(verb) AS total_verbs\n",
    "FROM (\n",
    "    SELECT\n",
    "        UNNEST(CAST(json_extract(elements, '$.verb_symbols') AS VARCHAR[])) AS verb\n",
    "    FROM\n",
    "        RAW_SECTION_P1_EXTRACTED_ELEMENTS_TRUE\n",
    ") AS flattened_verbs;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distinct verb symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    COUNT(DISTINCT verb) AS distinct_verbs_count\n",
    "FROM (\n",
    "    SELECT\n",
    "        UNNEST(CAST(json_extract(elements, '$.verb_symbols') AS VARCHAR[])) AS verb\n",
    "    FROM\n",
    "        RAW_SECTION_P1_EXTRACTED_ELEMENTS_TRUE\n",
    ") AS flattened_verbs;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT\n",
    "    verb,\n",
    "    doc_id,\n",
    "    source\n",
    "FROM (\n",
    "    SELECT\n",
    "        UNNEST(CAST(json_extract(elements, '$.verb_symbols') AS VARCHAR[])) AS verb,\n",
    "        id AS doc_id,\n",
    "        CAST(json_extract(elements, '$.sources') AS VARCHAR) AS source\n",
    "    FROM\n",
    "        RAW_SECTION_P1_EXTRACTED_ELEMENTS_TRUE\n",
    ") AS distinct_combinations;\n",
    "\"\"\"\n",
    "\n",
    "section_p1_extracted_elements_true_values = conn.sql(query).fetchall()\n",
    "\n",
    "conn.sql(query).fetchdf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total verb symbols extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, COUNT(verb) AS total_verbs\n",
    "FROM (\n",
    "    SELECT\n",
    "        id, UNNEST(CAST(json_extract(elements, '$.verb_symbols') AS VARCHAR[])) AS verb\n",
    "    FROM\n",
    "        RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    ") \n",
    "GROUP BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, term\n",
    "FROM (\n",
    "    SELECT id, UNNEST(terms) AS term\n",
    "    FROM (\n",
    "        SELECT\n",
    "            id, UNNEST(elements) AS element\n",
    "        FROM\n",
    "            RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "    )\n",
    ")\n",
    "ORDER BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, terms.classification AS term_classification, count(if(terms.term is NULL, NULL, 1)) as hasDefinition, COUNT(*) AS term_count\n",
    "FROM (\n",
    "    SELECT id, UNNEST(terms) AS terms\n",
    "    FROM (\n",
    "        SELECT\n",
    "            id, UNNEST(elements) AS element\n",
    "        FROM\n",
    "            RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "    )\n",
    ")\n",
    "GROUP BY id, terms.classification, if(terms.term is NULL, NULL, 1)\n",
    "ORDER BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id,\n",
    "    terms.classification AS term_classification,\n",
    "    terms.term AS term, \n",
    "    if(terms.term is NULL, NULL, 1) as hasDefinition\n",
    "FROM (\n",
    "    SELECT id, UNNEST(terms) AS terms\n",
    "    FROM (\n",
    "        SELECT\n",
    "            id, UNNEST(elements) AS element\n",
    "        FROM\n",
    "            RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "    )\n",
    ")\n",
    "ORDER BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distinct verb symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    COUNT(DISTINCT verb) AS distinct_verbs_count\n",
    "FROM (\n",
    "    SELECT\n",
    "        UNNEST(CAST(json_extract(elements, '$.verb_symbols') AS VARCHAR[])) AS verb\n",
    "    FROM\n",
    "        RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    ") AS flattened_verbs;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distinct verb symbols with doc_id and source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT\n",
    "    verb,\n",
    "    doc_id,\n",
    "    source\n",
    "FROM (\n",
    "    SELECT\n",
    "        UNNEST(CAST(json_extract(elements, '$.verb_symbols') AS VARCHAR[])) AS verb,\n",
    "        id AS doc_id,\n",
    "        CAST(json_extract(elements, '$.sources') AS VARCHAR) AS source\n",
    "    FROM\n",
    "        RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    ") AS distinct_combinations;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verb symbols with doc_id and source for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    verb,\n",
    "    doc_id,\n",
    "    source\n",
    "FROM (\n",
    "    SELECT\n",
    "        UNNEST(CAST(json_extract(elements, '$.verb_symbols') AS VARCHAR[])) AS verb,\n",
    "        id AS doc_id,\n",
    "        CAST(json_extract(elements, '$.sources') AS VARCHAR) AS source\n",
    "    FROM\n",
    "        RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    ") AS distinct_combinations;\n",
    "\"\"\"\n",
    "\n",
    "section_p1_extracted_elements_pred_values = conn.sql(query).fetchall()\n",
    "\n",
    "conn.sql(query).fetchdf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Terms and Names definitions (P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_table_from_json(\n",
    "    conn,\n",
    "    key_pattern1=\"_P2|true_table\",\n",
    "    key_pattern2=\"§\",\n",
    "    table_name=\"RAW_SECTION_P2_EXTRACTED_NOUN_TRUE\",\n",
    "    source=f\"{config['DEFAULT_DATA_DIR']}/documents_true_table.json\",\n",
    "    key_value=\"extract_p2\",\n",
    "    drop=True,\n",
    "    content_key=\"content.terms\",\n",
    "    alias=\"terms\",\n",
    "    doc_id_key=\"id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total terms per id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, COUNT(terms) AS total_terms, COUNT(DISTINCT terms) AS total_terms_distinct\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE\n",
    "GROUP BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, \n",
    "    terms.term as term,\n",
    "    terms.definition as definition,\n",
    "    if(terms.definition is NULL, 0, 1) as hasDefinition,\n",
    "    terms.isLocalScope as isLocalScope\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE\n",
    "-- GROUP BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many terms has definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, count(terms) as term, count(if(terms.definition is NULL, NULL, 1)) as hasDefinition, count(if(terms.definition is NULL, NULL, 1)) / count(terms) as ratio\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE\n",
    "GROUP BY id\n",
    "ORDER BY id\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "isLocalScope per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH TotalCounts AS (\n",
    "    SELECT\n",
    "        id,\n",
    "        COUNT(terms) AS total_terms\n",
    "    FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE\n",
    "    GROUP BY id\n",
    ")\n",
    "SELECT\n",
    "    e.id,\n",
    "    COUNT(e.terms) AS term,\n",
    "    e.terms.isLocalScope AS isLocalScope,\n",
    "    (COUNT(e.terms) * 100.0 / tc.total_terms) AS percentage\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE e\n",
    "JOIN TotalCounts tc ON e.id = tc.id\n",
    "GROUP BY e.id, e.terms.isLocalScope, tc.total_terms\n",
    "ORDER BY e.id, e.terms.isLocalScope;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, terms, terms.isLocalScope as isLocalScope\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE\n",
    "WHERE terms.isLocalScope is NULL\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_extraction\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    logger.info(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"_P2|llm_response\",\n",
    "        key_pattern2=\"§\",\n",
    "        table_name=\"RAW_SECTION_P2_EXTRACTED_NOUN\",\n",
    "        source=file_path,\n",
    "        key_value=\"extract_p2\",\n",
    "        drop=drop,\n",
    "        content_key=\"content.terms\",\n",
    "        alias=\"terms\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT * FROM RAW_SECTION_P2_EXTRACTED_NOUN;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, file_source, terms FROM RAW_SECTION_P2_EXTRACTED_NOUN;\n",
    "\"\"\"\n",
    "\n",
    "nouns = conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns[(nouns['id'] == '§ 275.0-2_P2')\n",
    "      #& (nouns['file_source'] == 'documents-2025-01-11-9.json')\n",
    "      & (nouns['terms'].apply(lambda x: 'Appointed agents' in x['term']))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT ID, COUNT(*)\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "GROUP BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DESCRIBE RAW_SECTION_P2_EXTRACTED_NOUN;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, terms.classification AS term_classification, terms.term AS term\n",
    "FROM (\n",
    "    SELECT id, UNNEST(terms) AS terms\n",
    "    FROM (\n",
    "        SELECT\n",
    "            id, UNNEST(elements) AS element\n",
    "        FROM\n",
    "            RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "    )\n",
    ")\n",
    "ORDER BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has definition ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, count(terms) as term, count(if(terms.definition is NULL, NULL, 1)) as hasDefinition, count(if(terms.definition is NULL, NULL, 1)) / count(terms) as ratio\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "GROUP BY id\n",
    "ORDER BY id\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terms and names classify by definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, terms.term as term, if(terms.definition is NULL, NULL, 1) as hasDefinition\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT \n",
    "        id,\n",
    "        terms.term AS term,\n",
    "        terms.classification AS term_classification,\n",
    "        count(terms.term) AS term_count\n",
    "    FROM (\n",
    "        SELECT \n",
    "            id, UNNEST(terms) AS terms\n",
    "        FROM (\n",
    "            SELECT \n",
    "                id, UNNEST(elements) AS element\n",
    "            FROM \n",
    "                RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "        )\n",
    "    )\n",
    "    GROUP BY id, terms.term, terms.classification\n",
    "    ORDER BY id, terms.term;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT\n",
    "        id, \n",
    "        terms.term AS term, \n",
    "        count(IF(terms.definition IS NULL, NULL, 1)) AS hasDefinition\n",
    "    FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "    GROUP BY id, terms.term\n",
    "    ORDER BY id, terms.term;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join results from P1 and P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH terms_classification AS (SELECT \n",
    "        REPLACE(id, '_P1', '') AS doc_id,\n",
    "        terms.term AS term,\n",
    "        terms.classification AS term_classification,\n",
    "        count(terms.term) AS term_count\n",
    "    FROM (\n",
    "        SELECT \n",
    "            id, UNNEST(terms) AS terms\n",
    "        FROM (\n",
    "            SELECT \n",
    "                id, UNNEST(elements) AS element\n",
    "            FROM \n",
    "                RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "        )\n",
    "    )\n",
    "    GROUP BY id, terms.term, terms.classification\n",
    "    ORDER BY id, terms.term),\n",
    "terms_definition AS (\n",
    "    SELECT\n",
    "        REPLACE(id, '_P2', '') AS doc_id,\n",
    "        terms.term AS term, \n",
    "        count(IF(terms.definition IS NULL, NULL, 1)) AS hasDefinition\n",
    "    FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "    GROUP BY id, terms.term\n",
    "    ORDER BY id, terms.term\n",
    ")\n",
    "SELECT\n",
    "    terms_classification.doc_id,\n",
    "    terms_classification.term,\n",
    "    terms_classification.term_classification,\n",
    "    terms_classification.term_count,\n",
    "    terms_definition.hasDefinition\n",
    "FROM terms_classification  \n",
    "LEFT JOIN terms_definition\n",
    "ON terms_classification.doc_id = terms_definition.doc_id AND terms_classification.term = terms_definition.term;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of terms extract P1 (Proper and Common), P2 (with and without definition)\n",
    "\n",
    "> Terms summary for dissertation's tables: \"Tabela 2  - Conjunto de dados ouro para validação (P1)\" and \"Tabela 3 – Termos e nomes com definição.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH terms_classification AS (SELECT \n",
    "        REPLACE(id, '_P1', '') AS doc_id,\n",
    "        terms.term AS term,\n",
    "        terms.classification AS term_classification,\n",
    "        count(terms.term) AS term_count\n",
    "    FROM (\n",
    "        SELECT \n",
    "            id, UNNEST(terms) AS terms\n",
    "        FROM (\n",
    "            SELECT \n",
    "                id, UNNEST(elements) AS element\n",
    "            FROM \n",
    "                RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "        )\n",
    "    )\n",
    "    GROUP BY id, terms.term, terms.classification\n",
    "    ORDER BY id, terms.term),\n",
    "terms_definition AS (\n",
    "    SELECT\n",
    "        REPLACE(id, '_P2', '') AS doc_id,\n",
    "        terms.term AS term, \n",
    "        if(count(IF(terms.definition IS NULL, NULL, 1)) > 0, 'Yes', 'No') AS hasDefinition\n",
    "    FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "    GROUP BY id, terms.term\n",
    "    ORDER BY id, terms.term\n",
    ")\n",
    "SELECT\n",
    "    terms_classification.doc_id,\n",
    "    -- terms_classification.term,\n",
    "    terms_classification.term_classification,\n",
    "    terms_definition.hasDefinition,\n",
    "    count(*) AS term_count\n",
    "FROM terms_classification  \n",
    "LEFT JOIN terms_definition\n",
    "ON terms_classification.doc_id = terms_definition.doc_id AND terms_classification.term = terms_definition.term\n",
    "GROUP BY terms_classification.doc_id, terms_classification.term_classification, terms_definition.hasDefinition\n",
    "ORDER BY terms_classification.doc_id, terms_classification.term_classification, terms_definition.hasDefinition;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTERSECTION(TRUE, PRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, prompt, term, definition, isLocalScope\n",
    "FROM (\n",
    "SELECT DISTINCT id, prompt, file_source, created_at, UNNEST(terms) as terms, terms.term, terms.definition, terms.isLocalScope\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE\n",
    ")\n",
    "EXCEPT\n",
    "SELECT DISTINCT id, prompt, term, definition, isLocalScope\n",
    "FROM (\n",
    "SELECT DISTINCT id, prompt, file_source, created_at, UNNEST(terms) as terms, terms.term, terms.definition, terms.isLocalScope\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTERSECTION(PRED, TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, prompt, term, definition, isLocalScope\n",
    "FROM (\n",
    "SELECT DISTINCT id, prompt, file_source, created_at, UNNEST(terms) as terms, terms.term, terms.definition, terms.isLocalScope\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    ")\n",
    "EXCEPT\n",
    "SELECT DISTINCT id, prompt, term, definition, isLocalScope\n",
    "FROM (\n",
    "SELECT DISTINCT id, prompt, file_source, created_at, UNNEST(terms) as terms, terms.term, terms.definition, terms.isLocalScope\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total terms and distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, COUNT(terms) AS total_terms, COUNT(DISTINCT terms) AS total_terms_distinct\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "GROUP BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distinct terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, terms.term as term, terms.definition as definition, if(terms.definition is NULL, 0, 1) as hasDefinition, terms.isLocalScope as isLocalScope\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "-- GROUP BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many terms has definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, count(terms) as term, count(if(terms.definition is NULL, NULL, 1)) as hasDefinition, count(if(terms.definition is NULL, NULL, 1)) / count(terms) as ratio\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "GROUP BY id\n",
    "ORDER BY id\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "isLocalScope per id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH TotalCounts AS (\n",
    "    SELECT\n",
    "        id,\n",
    "        COUNT(terms) AS total_terms\n",
    "    FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "    GROUP BY id\n",
    ")\n",
    "SELECT\n",
    "    e.id,\n",
    "    COUNT(e.terms) AS term,\n",
    "    e.terms.isLocalScope AS isLocalScope,\n",
    "    (COUNT(e.terms) * 100.0 / tc.total_terms) AS percentage\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN e\n",
    "JOIN TotalCounts tc ON e.id = tc.id\n",
    "GROUP BY e.id, e.terms.isLocalScope, tc.total_terms\n",
    "ORDER BY e.id, e.terms.isLocalScope;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, terms, terms.isLocalScope as isLocalScope\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "WHERE terms.isLocalScope is NULL\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset extraction tables to evaluation checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoints_extraction to evaluate the original extraction\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_extraction\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"_P1|llm_response\",\n",
    "        key_pattern2=\"§\",\n",
    "        table_name=\"RAW_SECTION_P1_EXTRACTED_ELEMENTS\",\n",
    "        source=file_path,\n",
    "        key_value=\"extract_p1\",\n",
    "        drop=drop,\n",
    "        content_key=\"content.elements\",\n",
    "        alias=\"elements\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT distinct file_source\n",
    "FROM RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "ORDER BY 1;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DESCRIBE RAW_SECTION_P1_EXTRACTED_ELEMENTS;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *,\n",
    "FROM CHECKPOINT_METADATA\n",
    "WHERE PROCESS='classification'\n",
    "ORDER BY DOC_ID, DOC_TYPE DESC;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify P1 - Operative Rules into top-level Witt(2012) taxonomy (P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_table_from_json(\n",
    "    conn,\n",
    "    key_pattern1=\"classify_P1|true_table\",\n",
    "    table_name=\"RAW_CLASSIFY_P1_OPERATIVE_RULES_TRUE\",\n",
    "    source=f\"{config['DEFAULT_DATA_DIR']}/documents_true_table.json\",\n",
    "    key_value=\"classify_p1\",\n",
    "    drop=True,\n",
    "    content_key=\"content\",\n",
    "    alias=\"content\",\n",
    "    doc_id_key=\"id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_classification\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"classify_P1|llm_response_classification\",\n",
    "        table_name=\"RAW_CLASSIFY_P1_OPERATIVE_RULES\",\n",
    "        source=file_path,\n",
    "        key_value=\"classify_p1\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    content.doc_id as id,\n",
    "    content.statement_id,\n",
    "    content.statement_title,\n",
    "    content.statement_text,\n",
    "    content.statement_sources,\n",
    "    content.classification\n",
    "FROM RAW_CLASSIFY_P1_OPERATIVE_RULES\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    content.doc_id as doc_id,\n",
    "    content.statement_id as statement_id,\n",
    "    content.statement_title as statement_title,\n",
    "    content.statement_sources as statement_sources,\n",
    "    content.file_source as file_source,\n",
    "    MAX(classification.type) as classification_type, \n",
    "    MAX(classification.explanation) as classification_explanation, \n",
    "    MAX(classification.confidence) as classification_confidence,\n",
    "    MAX(file_source) as file_source\n",
    "FROM (\n",
    "    SELECT\n",
    "        file_source,\n",
    "        content.doc_id,\n",
    "        content.statement_id,\n",
    "        content.statement_sources,\n",
    "        content.statement_title,\n",
    "        unnest(content.classification) as classification\n",
    "    FROM\n",
    "        RAW_CLASSIFY_P1_OPERATIVE_RULES\n",
    ") AS content\n",
    "GROUP BY doc_id, statement_id, statement_title, statement_sources, file_source\n",
    "ORDER BY file_source, doc_id, statement_id\n",
    "\"\"\"\n",
    "\n",
    "pred_classify_p1_operative_rules = conn.sql(query).fetchall()\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best classification of all checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH ExpandedClassifications AS (\n",
    "    SELECT\n",
    "        content.doc_id AS id,\n",
    "        content.statement_id,\n",
    "        content.statement_title,\n",
    "        content.statement_text,\n",
    "        content.statement_sources,\n",
    "        classification_item.value.type AS type,\n",
    "        classification_item.value.confidence AS confidence,\n",
    "        classification_item.value.explanation AS explanation,\n",
    "        content.file_source as file_source,\n",
    "        ROW_NUMBER() OVER (PARTITION BY content.doc_id, content.statement_id ORDER BY classification_item.value.confidence DESC) AS rn\n",
    "    FROM\n",
    "        RAW_CLASSIFY_P1_OPERATIVE_RULES AS content,\n",
    "        UNNEST(content.classification) AS classification_item(value)\n",
    ")\n",
    "SELECT\n",
    "    id,\n",
    "    statement_id,\n",
    "    statement_title,\n",
    "    statement_text,\n",
    "    statement_sources,\n",
    "    file_source,\n",
    "    type AS classification_type,\n",
    "    confidence AS classification_confidence,\n",
    "    explanation AS classification_explanation\n",
    "FROM\n",
    "    ExpandedClassifications\n",
    "WHERE\n",
    "    rn = 1\n",
    "ORDER BY statement_text;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join with terms P1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify - P2 Operative Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_table_from_json(\n",
    "    conn,\n",
    "    key_pattern1=\"classify_P2_Operative_rules|true_table\",\n",
    "    table_name=\"RAW_CLASSIFY_P2_OPERATIVE_RULES_TRUE\",\n",
    "    source=f\"{config['DEFAULT_DATA_DIR']}/documents_true_table.json\",\n",
    "    key_value=\"classify_P2_Operative_rules\",\n",
    "    drop=True,\n",
    "    content_key=\"content\",\n",
    "    alias=\"content\",\n",
    "    doc_id_key=\"id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM RAW_CLASSIFY_P2_OPERATIVE_RULES_TRUE;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join extract P1 with Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_classification\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"classify_P2_Operative_rules|llm_response_classification\",\n",
    "        table_name=\"RAW_CLASSIFY_P2_OPERATIVE_RULES\",\n",
    "        source=file_path,\n",
    "        key_value=\"classify_P2_Operative_rules\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distinct pred rules match true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT id, prompt, content\n",
    "FROM RAW_CLASSIFY_P2_OPERATIVE_RULES_TRUE;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify P2 - Definitional Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_table_from_json(\n",
    "    conn,\n",
    "    key_pattern1=\"classify_P2_Definitional_terms|true_table\",\n",
    "    table_name=\"RAW_CLASSIFY_P2_DEFINITIONAL_TERMS_TRUE\",\n",
    "    source=f\"{config['DEFAULT_DATA_DIR']}/documents_true_table.json\",\n",
    "    key_value=\"classify_P2_Definitional_terms\",\n",
    "    drop=True,\n",
    "    content_key=\"content\",\n",
    "    alias=\"content\",\n",
    "    doc_id_key=\"id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_CLASSIFY_P2_DEFINITIONAL_TERMS_TRUE\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_classification\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"classify_P2_Definitional_terms|llm_response_classification\",\n",
    "        table_name=\"RAW_CLASSIFY_P2_DEFINITIONAL_TERMS\",\n",
    "        source=file_path,\n",
    "        key_value=\"classify_P2_Definitional_terms\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_CLASSIFY_P2_DEFINITIONAL_TERMS\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify P2 - Definitional Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_table_from_json(\n",
    "    conn,\n",
    "    key_pattern1=\"classify_P2_Definitional_names|true_table\",\n",
    "    table_name=\"RAW_CLASSIFY_P2_DEFINITIONAL_NAMES_TRUE\",\n",
    "    source=f\"{config['DEFAULT_DATA_DIR']}/documents_true_table.json\",\n",
    "    key_value=\"classify_P2_Definitional_names\",\n",
    "    drop=True,\n",
    "    content_key=\"content\",\n",
    "    alias=\"content\",\n",
    "    doc_id_key=\"id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_CLASSIFY_P2_DEFINITIONAL_NAMES_TRUE\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_classification\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"classify_P2_Definitional_names|llm_response_classification\",\n",
    "        table_name=\"RAW_CLASSIFY_P2_DEFINITIONAL_NAMES\",\n",
    "        source=file_path,\n",
    "        key_value=\"classify_P2_Definitional_names\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_CLASSIFY_P2_DEFINITIONAL_NAMES\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify P2 - Definitional Facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_table_from_json(\n",
    "    conn,\n",
    "    key_pattern1=\"classify_P2_Definitional_facts|true_table\",\n",
    "    table_name=\"RAW_CLASSIFY_P2_DEFINITIONAL_FACTS_TRUE\",\n",
    "    source=f\"{config['DEFAULT_DATA_DIR']}/documents_true_table.json\",\n",
    "    key_value=\"classify_P2_Definitional_facts\",\n",
    "    drop=True,\n",
    "    content_key=\"content\",\n",
    "    alias=\"content\",\n",
    "    doc_id_key=\"id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_CLASSIFY_P2_DEFINITIONAL_FACTS_TRUE\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_classification\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"classify_P2_Definitional_facts|llm_response_classification\",\n",
    "        table_name=\"RAW_CLASSIFY_P2_DEFINITIONAL_FACTS\",\n",
    "        source=file_path,\n",
    "        key_value=\"classify_P2_Definitional_facts\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_CLASSIFY_P2_DEFINITIONAL_FACTS\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *,\n",
    "FROM CHECKPOINT_METADATA\n",
    "WHERE PROCESS='transformation'\n",
    "ORDER BY DOC_ID, DOC_TYPE DESC;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operative Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_transform\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"transform_Operative_Rules|llm_response_transform\",\n",
    "        table_name=\"RAW_TRANSFORM_OPERATIVE_RULES\",\n",
    "        source=file_path,\n",
    "        key_value=\"transform_Operative_Rules\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_TRANSFORM_OPERATIVE_RULES\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_transform\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"transform_Terms|llm_response_transform\",\n",
    "        table_name=\"RAW_TRANSFORM_TERMS\",\n",
    "        source=file_path,\n",
    "        key_value=\"transform_Terms\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_TRANSFORM_TERMS\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_transform\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"transform_Names|llm_response_transform\",\n",
    "        table_name=\"RAW_TRANSFORM_NAMES\",\n",
    "        source=file_path,\n",
    "        key_value=\"transform_Names\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_TRANSFORM_NAMES\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fact types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_transform\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"transform_Fact_Types|llm_response_transform\",\n",
    "        table_name=\"RAW_TRANSFORM_FACT_TYPES\",\n",
    "        source=file_path,\n",
    "        key_value=\"transform_Fact_Types\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_TRANSFORM_FACT_TYPES\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_evaluation\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"|llm_validation\",\n",
    "        table_name=\"RAW_LLM_VALIDATION\",\n",
    "        source=file_path,\n",
    "        key_value=\"llm_validation\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "*\n",
    "FROM RAW_LLM_VALIDATION\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id,\n",
    "    REPLACE(id, 'validation_judge_', '') as element_type,\n",
    "    file_source,\n",
    "    content.doc_id,\n",
    "    content.statement_id,\n",
    "    content.statement,\n",
    "    content.sources,\n",
    "    content.semscore,\n",
    "    content.similarity_score,\n",
    "    content.similarity_score_confidence,\n",
    "    content.transformation_accuracy,\n",
    "    content.grammar_syntax_accuracy,\n",
    "    content.findings,\n",
    "    created_at  \n",
    "FROM RAW_LLM_VALIDATION;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Scores per id, file_source, and doc_id\n",
    "\n",
    " - semscore\n",
    " - similarity_score\n",
    " - similarity_score_confidence\n",
    " - transformation_accuracy\n",
    " - grammar_syntax_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id,\n",
    "    REPLACE(id, 'validation_judge_', '') as element_type,\n",
    "    file_source,\n",
    "    content.doc_id,\n",
    "    count(content.doc_id) as count_doc_id,\n",
    "    avg(content.semscore) as avg_semscore,\n",
    "    avg(content.similarity_score) as avg_similarity_score,\n",
    "    avg(content.similarity_score_confidence) as avg_similarity_score_confidence,\n",
    "    avg(content.transformation_accuracy) as avg_transformation_accuracy,\n",
    "    avg(content.grammar_syntax_accuracy) as avg_grammar_syntax_accuracy,\n",
    "    max(created_at) as last_created_at\n",
    "FROM RAW_LLM_VALIDATION\n",
    "group by id, file_source, content.doc_id\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Scores per element_type\n",
    "\n",
    " - semscore\n",
    " - similarity_score\n",
    " - similarity_score_confidence\n",
    " - transformation_accuracy\n",
    " - grammar_syntax_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    REPLACE(id, 'validation_judge_', '') as element_type,\n",
    "    count(content.doc_id) as count_doc_id,\n",
    "    max(created_at) as last_created_at,\n",
    "    -- avg\n",
    "    round(avg(content.semscore), 3) as avg_semscore,\n",
    "    round(avg(content.similarity_score), 3) as avg_similarity_score,\n",
    "    round(avg(content.similarity_score_confidence), 3) as avg_similarity_score_confidence,\n",
    "    round(avg(content.transformation_accuracy), 3) as avg_transformation_accuracy,\n",
    "    round(avg(content.grammar_syntax_accuracy), 3) as avg_grammar_syntax_accuracy,\n",
    "    -- min\n",
    "    round(min(content.semscore), 3) as avg_semscore,\n",
    "    round(min(content.similarity_score), 3) as avg_similarity_score,\n",
    "    round(min(content.similarity_score_confidence), 3) as min_similarity_score_confidence,\n",
    "    round(min(content.transformation_accuracy), 3) as min_transformation_accuracy,\n",
    "    round(min(content.grammar_syntax_accuracy), 3) as min_grammar_syntax_accuracy,\n",
    "    -- max\n",
    "    round(max(content.semscore), 3) as avg_semscore,\n",
    "    round(max(content.similarity_score), 3) as avg_similarity_score,\n",
    "    round(max(content.similarity_score_confidence), 3) as max_similarity_score_confidence,\n",
    "    round(max(content.transformation_accuracy), 3) as max_transformation_accuracy,\n",
    "    round(max(content.grammar_syntax_accuracy), 3) as max_grammar_syntax_accuracy\n",
    "FROM RAW_LLM_VALIDATION\n",
    "GROUP BY \n",
    "    REPLACE(id, 'validation_judge_', '')\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Scores of all runs\n",
    "\n",
    " - semscore\n",
    " - similarity_score\n",
    " - similarity_score_confidence\n",
    " - transformation_accuracy\n",
    " - grammar_syntax_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    count(content.doc_id) as count_doc_id,\n",
    "    max(created_at) as last_created_at,\n",
    "    -- avg\n",
    "    round(avg(content.semscore), 3) as avg_semscore,\n",
    "    round(avg(content.similarity_score), 3) as avg_similarity_score,\n",
    "    round(avg(content.similarity_score_confidence), 3) as avg_similarity_score_confidence,\n",
    "    round(avg(content.transformation_accuracy), 3) as avg_transformation_accuracy,\n",
    "    round(avg(content.grammar_syntax_accuracy), 3) as avg_grammar_syntax_accuracy,\n",
    "    -- min\n",
    "    round(min(content.semscore), 3) as avg_semscore,\n",
    "    round(min(content.similarity_score), 3) as avg_similarity_score,\n",
    "    round(min(content.similarity_score_confidence), 3) as min_similarity_score_confidence,\n",
    "    round(min(content.transformation_accuracy), 3) as min_transformation_accuracy,\n",
    "    round(min(content.grammar_syntax_accuracy), 3) as min_grammar_syntax_accuracy,\n",
    "    -- max\n",
    "    round(max(content.semscore), 3) as avg_semscore,\n",
    "    round(max(content.similarity_score), 3) as avg_similarity_score,\n",
    "    round(max(content.similarity_score_confidence), 3) as max_similarity_score_confidence,\n",
    "    round(max(content.transformation_accuracy), 3) as max_transformation_accuracy,\n",
    "    round(max(content.grammar_syntax_accuracy), 3) as max_grammar_syntax_accuracy\n",
    "FROM RAW_LLM_VALIDATION\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_evaluation\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"|llm_\",\n",
    "        table_name=\"RAW_ELAPSED_TIME\",\n",
    "        source=file_path,\n",
    "        key_value=\"llm_response\",\n",
    "        drop=drop,\n",
    "        content_key=\"elapsed_times\",\n",
    "        alias=\"elapsed_times\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_ELAPSED_TIME\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed time by checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT file_source as checkpoint, count(elapsed_times) as count_elapsed_time, sum(elapsed_times) as total_elapsed_time, avg(elapsed_times) as avg_elapsed_time\n",
    "FROM RAW_ELAPSED_TIME\n",
    "GROUP BY id, file_source\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "et_file = conn.sql(query).fetchdf()\n",
    "\n",
    "et_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_file,\n",
    "    group_col=\"checkpoint\",\n",
    "    value_col=\"total_elapsed_time\",\n",
    "    title=\"Soma do Tempo Total Decorrido por Checkpoint\",\n",
    "    x_label=\"Tempo Total Decorrido\",\n",
    "    y_label=\"Checkpoint\",\n",
    "    big_numbers={\"factor\": 1, \"suffix\": \"s\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed time by doc_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id as doc_type, count(elapsed_times) as count_elapsed_time, sum(elapsed_times) as total_elapsed_time, avg(elapsed_times) as avg_elapsed_time\n",
    "FROM RAW_ELAPSED_TIME\n",
    "GROUP BY id\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "et_doc_type = conn.sql(query).fetchdf()\n",
    "\n",
    "et_doc_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_doc_type,\n",
    "    group_col=\"doc_type\",\n",
    "    value_col=\"total_elapsed_time\",\n",
    "    title=\"Soma do tempo total decorrido por tipo documento\",\n",
    "    x_label=\"Tempo total decorrido\",\n",
    "    y_label=\"doc_type\",\n",
    "    big_numbers={\"factor\": 1, \"suffix\": \"s\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed time by process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH CHECKPOINT AS (\n",
    "    SELECT \n",
    "        PROCESS, \n",
    "        DOC_ID, \n",
    "        DOC_TYPE,\n",
    "        DOC_SOURCE\n",
    "    FROM \n",
    "        CHECKPOINT_METADATA\n",
    "    GROUP BY \n",
    "        PROCESS, DOC_ID, DOC_TYPE, DOC_SOURCE\n",
    ")\n",
    "SELECT\n",
    "    cm.process,\n",
    "    cm.doc_id,\n",
    "    cm.doc_type,\n",
    "    cm.doc_source,\n",
    "    COUNT(et.elapsed_times) AS count_elapsed_time,\n",
    "    SUM(et.elapsed_times) AS total_elapsed_time,\n",
    "    AVG(et.elapsed_times) AS avg_elapsed_time\n",
    "FROM\n",
    "    RAW_ELAPSED_TIME AS et\n",
    "JOIN\n",
    "    CHECKPOINT AS cm ON et.id = cm.doc_id\n",
    "WHERE\n",
    "    cm.doc_source in ('pred', 'val')\n",
    "GROUP BY\n",
    "    cm.process, cm.doc_id, cm.doc_type, cm.doc_source\n",
    "ORDER BY\n",
    "    cm.process, cm.doc_id, cm.doc_type, cm.doc_source;\n",
    "\"\"\"\n",
    "\n",
    "et_process = conn.sql(query).fetchdf()\n",
    "\n",
    "et_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_process,\n",
    "    group_col=\"process\",\n",
    "    value_col=\"total_elapsed_time\",\n",
    "    title=\"Soma do tempo total decorrido por processo\",\n",
    "    x_label=\"Tempo total decorrido\",\n",
    "    y_label=\"processo\",\n",
    "    big_numbers={\"factor\": 1, \"suffix\": \"s\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_process.groupby(\"process\")[\"total_elapsed_time\"].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average elapsed time by process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_process,\n",
    "    group_col=\"process\",\n",
    "    value_col=\"avg_elapsed_time\",\n",
    "    title=\"Tempo médio decorrido por processo\",\n",
    "    x_label=\"Tempo total decorrido\",\n",
    "    y_label=\"processo\",\n",
    "    big_numbers={\"factor\": 1, \"suffix\": \"s\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed time all runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT count(elapsed_times) as count_elapsed_time, sum(elapsed_times) as total_elapsed_time, avg(elapsed_times) as avg_elapsed_time\n",
    "FROM RAW_ELAPSED_TIME\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "elapsed_time_big_numbers = conn.sql(query).fetchdf()\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"count_elapsed_time: {humanize.intword(elapsed_time_big_numbers['count_elapsed_time'])}\")\n",
    "print(f\"total_elapsed_time: {humanize.intword(elapsed_time_big_numbers['total_elapsed_time'])}\")\n",
    "print(f\"avg_elapsed_time: {humanize.intword(elapsed_time_big_numbers['avg_elapsed_time'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_evaluation\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"|llm_\",\n",
    "        table_name=\"RAW_LLM_COMPLETION\",\n",
    "        source=file_path,\n",
    "        key_value=\"llm_response\",\n",
    "        drop=drop,\n",
    "        content_key=\"completions\",\n",
    "        alias=\"completions\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DESCRIBE RAW_LLM_COMPLETION\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    id, \n",
    "    file_source, \n",
    "    created_at, \n",
    "    completions.id,\n",
    "    completions.model,\n",
    "    completions.object,\n",
    "    completions.system_fingerprint,\n",
    "    completions.usage.completion_tokens,\n",
    "    completions.usage.prompt_tokens,\n",
    "    completions.usage.total_tokens\n",
    "FROM RAW_LLM_COMPLETION\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH CHECKPOINT AS (\n",
    "    SELECT \n",
    "        PROCESS, \n",
    "        DOC_ID, \n",
    "        DOC_TYPE,\n",
    "        DOC_SOURCE\n",
    "    FROM \n",
    "        CHECKPOINT_METADATA\n",
    "    GROUP BY \n",
    "        PROCESS, DOC_ID, DOC_TYPE, DOC_SOURCE\n",
    ")\n",
    "SELECT \n",
    "    id as doc_id,\n",
    "    cm.process,\n",
    "    cm.doc_source as doc_source,\n",
    "    file_source as checkpoint, \n",
    "    created_at, \n",
    "    completions.id as completion_id,\n",
    "    completions.model,\n",
    "    completions.object,\n",
    "    --completions.service_tier,\n",
    "    completions.system_fingerprint,\n",
    "    completions.usage.completion_tokens,\n",
    "    completions.usage.prompt_tokens,\n",
    "    completions.usage.total_tokens\n",
    "    --completions.usage.completion_tokens_details,\n",
    "    --completions.usage.prompt_tokens_details\n",
    "FROM RAW_LLM_COMPLETION AS lc\n",
    "JOIN\n",
    "    CHECKPOINT AS cm ON lc.id = cm.doc_id\n",
    "WHERE\n",
    "    cm.doc_source in ('pred', 'val')\n",
    "--GROUP BY\n",
    "--    cm.process, cm.doc_id, cm.doc_type, cm.doc_source\n",
    "--ORDER BY\n",
    "--    cm.process, cm.doc_id, cm.doc_type, cm.doc_source;\n",
    "\"\"\"\n",
    "\n",
    "lc_process = conn.sql(query).fetchdf()\n",
    "\n",
    "lc_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens per process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH CHECKPOINT AS (\n",
    "    SELECT \n",
    "        PROCESS, \n",
    "        DOC_ID, \n",
    "        DOC_TYPE,\n",
    "        DOC_SOURCE\n",
    "    FROM \n",
    "        CHECKPOINT_METADATA\n",
    "    GROUP BY \n",
    "        PROCESS, DOC_ID, DOC_TYPE, DOC_SOURCE\n",
    ")\n",
    "SELECT \n",
    "    cm.process,\n",
    "    cm.doc_id,\n",
    "    cm.doc_type,\n",
    "    cm.doc_source,\n",
    "    lc.file_source as checkpoint,\n",
    "    COUNT(lc.id) AS count_completions,\n",
    "    SUM(lc.completions.usage.completion_tokens) AS total_completion_tokens,\n",
    "    AVG(lc.completions.usage.completion_tokens) AS avg_completion_tokens,\n",
    "\n",
    "    SUM(lc.completions.usage.prompt_tokens) AS total_prompt_tokens,\n",
    "    AVG(lc.completions.usage.prompt_tokens) AS avg_prompt_tokens,\n",
    "\n",
    "    SUM(lc.completions.usage.total_tokens) AS total_total_tokens,\n",
    "    AVG(lc.completions.usage.total_tokens) AS avg_total_tokens,\n",
    "    \n",
    "    MAX(lc.created_at) AS last_completion\n",
    "FROM RAW_LLM_COMPLETION AS lc\n",
    "JOIN\n",
    "    CHECKPOINT AS cm ON lc.id = cm.doc_id\n",
    "WHERE\n",
    "    cm.doc_source in ('pred', 'val')\n",
    "GROUP BY\n",
    "    cm.process, cm.doc_id, cm.doc_type, cm.doc_source, lc.file_source\n",
    "ORDER BY\n",
    "    cm.process, cm.doc_id, cm.doc_type, cm.doc_source, lc.file_source;\n",
    "\"\"\"\n",
    "\n",
    "et_llm_completion = conn.sql(query).fetchdf()\n",
    "\n",
    "et_llm_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_llm_completion,\n",
    "    group_col=\"process\",\n",
    "    value_col=\"total_prompt_tokens\",\n",
    "    title=\"Tempo médio decorrido por processo\",\n",
    "    x_label=\"Qde tokens\",\n",
    "    y_label=\"processo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avarege tokens per process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_llm_completion,\n",
    "    group_col=\"process\",\n",
    "    value_col=\"avg_total_tokens\",\n",
    "    title=\"Qde tokens médio por processo\",\n",
    "    x_label=\"Qde tokens médio\",\n",
    "    y_label=\"processo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens per document type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_llm_completion,\n",
    "    group_col=\"doc_id\",\n",
    "    value_col=\"total_prompt_tokens\",\n",
    "    title=\"Qde tokens por tipo documento\",\n",
    "    x_label=\"Qde tokens\",\n",
    "    y_label=\"doc_id\",\n",
    "    figsize=(12, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average tokens per checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_llm_completion,\n",
    "    group_col=\"doc_id\",\n",
    "    value_col=\"avg_prompt_tokens\",\n",
    "    title=\"Qde tokens por tipo documento\",\n",
    "    x_label=\"Qde tokens\",\n",
    "    y_label=\"doc_id\",\n",
    "    figsize=(12, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens per checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_llm_completion,\n",
    "    group_col=\"checkpoint\",\n",
    "    value_col=\"total_prompt_tokens\",\n",
    "    title=\"Qde tokens por checkpoint\",\n",
    "    x_label=\"Qde tokens\",\n",
    "    y_label=\"checkpoint\",\n",
    "    figsize=(12, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_llm_completion,\n",
    "    group_col=\"checkpoint\",\n",
    "    value_col=\"avg_total_tokens\",\n",
    "    title=\"Tokens médio por checkpoint\",\n",
    "    x_label=\"Qde tokens\",\n",
    "    y_label=\"checkpoint\",\n",
    "    figsize=(12, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH CHECKPOINT AS (\n",
    "    SELECT \n",
    "        PROCESS, \n",
    "        DOC_ID, \n",
    "        DOC_TYPE,\n",
    "        DOC_SOURCE\n",
    "    FROM \n",
    "        CHECKPOINT_METADATA\n",
    "    GROUP BY \n",
    "        PROCESS, DOC_ID, DOC_TYPE, DOC_SOURCE\n",
    ")\n",
    "SELECT\n",
    "    cm.process,\n",
    "    COUNT(lc.id) AS count_completions,\n",
    "    SUM(lc.completions.usage.completion_tokens) AS total_completion_tokens,\n",
    "    AVG(lc.completions.usage.completion_tokens) AS avg_completion_tokens,\n",
    "\n",
    "    SUM(lc.completions.usage.prompt_tokens) AS total_prompt_tokens,\n",
    "    AVG(lc.completions.usage.prompt_tokens) AS avg_prompt_tokens,\n",
    "\n",
    "    SUM(lc.completions.usage.total_tokens) AS total_total_tokens,\n",
    "    AVG(lc.completions.usage.total_tokens) AS avg_total_tokens,\n",
    "    \n",
    "    MAX(lc.created_at) AS last_completion\n",
    "FROM RAW_LLM_COMPLETION AS lc\n",
    "JOIN\n",
    "    CHECKPOINT AS cm ON lc.id = cm.doc_id\n",
    "WHERE\n",
    "    cm.doc_source in ('pred', 'val')\n",
    "GROUP BY\n",
    "    cm.process\n",
    "\"\"\"\n",
    "\n",
    "token_big_numbers = conn.sql(query).fetchdf()\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total_prompt_tokens: {humanize.intword(token_big_numbers['total_prompt_tokens'])}\")\n",
    "print(f\"total_completion_tokens: {humanize.intword(token_big_numbers['total_completion_tokens'])}\")\n",
    "print(f\"total_total_tokens: {humanize.intword(token_big_numbers['total_total_tokens'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create views\n",
    "\n",
    "The main use of the views is exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 11:47:30 - INFO - Processing file ../cfr2sbvr_inspect/data/db_objects_v4/10_RAW_SECTION_EXTRACTED_ELEMENTS_VW.sql\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 11:47:31 - INFO - Processing file ../cfr2sbvr_inspect/data/db_objects_v4/20_RAW_SECTION_P1_EXTRACTED_ELEMENTS_VW.sql\n",
      "2025-01-16 11:47:31 - INFO - Processing file ../cfr2sbvr_inspect/data/db_objects_v4/30_RAW_SECTION_P2_EXTRACTED_NOUN_VW.sql\n",
      "2025-01-16 11:47:31 - INFO - Processing file ../cfr2sbvr_inspect/data/db_objects_v4/40_RAW_CLASSIFY_P1_OPERATIVE_RULES_VW.sql\n",
      "2025-01-16 11:47:32 - INFO - Processing file ../cfr2sbvr_inspect/data/db_objects_v4/50_RAW_CLASSIFY_P2_OPERATIVE_RULES_VW.sql\n",
      "2025-01-16 11:47:32 - INFO - Processing file ../cfr2sbvr_inspect/data/db_objects_v4/60_RAW_CLASSIFY_P2_DEFINITIONAL_NAMES_VW.sql\n",
      "2025-01-16 11:47:32 - INFO - Processing file ../cfr2sbvr_inspect/data/db_objects_v4/70_RAW_CLASSIFY_P2_DEFINITIONAL_TERMS_VW.sql\n",
      "2025-01-16 11:47:33 - INFO - Processing file ../cfr2sbvr_inspect/data/db_objects_v4/80_RAW_CLASSIFY_P2_DEFINITIONAL_FACTS_VW.sql\n",
      "2025-01-16 11:47:33 - INFO - Processing file ../cfr2sbvr_inspect/data/db_objects_v4/90_RAW_CLASSIFY_VW.sql\n",
      "2025-01-16 11:47:33 - INFO - Processing file ../cfr2sbvr_inspect/data/db_objects_v4/100_RAW_TRANSFORM_OPERATIVE_RULES_VW.sql\n",
      "2025-01-16 11:47:34 - INFO - Processing file ../cfr2sbvr_inspect/data/db_objects_v4/110_RAW_TRANSFORM_NAMES_VW.sql\n",
      "2025-01-16 11:47:34 - INFO - Processing file ../cfr2sbvr_inspect/data/db_objects_v4/120_RAW_TRANSFORM_TERMS_VW.sql\n",
      "2025-01-16 11:47:34 - INFO - Processing file ../cfr2sbvr_inspect/data/db_objects_v4/130_RAW_TRANSFORM_FACT_TYPES_VW.sql\n",
      "2025-01-16 11:47:35 - INFO - Processing file ../cfr2sbvr_inspect/data/db_objects_v4/140_RAW_TRANSFORM_ELEMENTS_VW.sql\n",
      "2025-01-16 11:47:35 - INFO - Processing file ../cfr2sbvr_inspect/data/db_objects_v4/150_RAW_ELAPSED_TIME_VW.sql\n",
      "2025-01-16 11:47:36 - INFO - Processing file ../cfr2sbvr_inspect/data/db_objects_v4/160_RAW_LLM_COMPLETION_VW.sql\n",
      "2025-01-16 11:47:36 - INFO - Processing file ../cfr2sbvr_inspect/data/db_objects_v4/170_RAW_LLM_VALIDATION_VW.sql\n",
      "2025-01-16 11:47:36 - INFO - Processing file ../cfr2sbvr_inspect/data/db_objects_v4/180_RAW_LLM_VALIDATION_BEST_VW.sql\n"
     ]
    }
   ],
   "source": [
    "from natsort import natsorted\n",
    "\n",
    "DB_OBJECTS=\"db_objects_v4\"\n",
    "\n",
    "if CLEAN_UP:\n",
    "    directory = Path(f\"{config['DEFAULT_APP_DIR']}/data/{DB_OBJECTS}\")\n",
    "    pattern = \"*_VW.sql\"\n",
    "\n",
    "    # Use glob to find all files matching the pattern\n",
    "    files = directory.glob(pattern)\n",
    "\n",
    "    files_processed = 0\n",
    "    for file_path in natsorted(files):\n",
    "        logger.info(f\"Processing file {file_path}\")\n",
    "        files_processed += 1\n",
    "        with open(file_path, \"r\") as file:\n",
    "            ddl_query = file.read()\n",
    "            logger.debug(ddl_query)\n",
    "            conn.sql(ddl_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database_name</th>\n",
       "      <th>database_oid</th>\n",
       "      <th>schema_name</th>\n",
       "      <th>schema_oid</th>\n",
       "      <th>view_name</th>\n",
       "      <th>view_oid</th>\n",
       "      <th>comment</th>\n",
       "      <th>tags</th>\n",
       "      <th>internal</th>\n",
       "      <th>temporary</th>\n",
       "      <th>column_count</th>\n",
       "      <th>sql</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cfr2sbvr_v4</td>\n",
       "      <td>1315</td>\n",
       "      <td>main</td>\n",
       "      <td>3616</td>\n",
       "      <td>RAW_TRANSFORM_NAMES_VW</td>\n",
       "      <td>3655</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>CREATE VIEW RAW_TRANSFORM_NAMES_VW AS SELECT T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cfr2sbvr_v4</td>\n",
       "      <td>1315</td>\n",
       "      <td>main</td>\n",
       "      <td>3616</td>\n",
       "      <td>RAW_CLASSIFY_P2_DEFINITIONAL_FACTS_VW</td>\n",
       "      <td>3641</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>CREATE VIEW RAW_CLASSIFY_P2_DEFINITIONAL_FACTS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cfr2sbvr_v4</td>\n",
       "      <td>1315</td>\n",
       "      <td>main</td>\n",
       "      <td>3616</td>\n",
       "      <td>RAW_CLASSIFY_P2_DEFINITIONAL_NAMES_VW</td>\n",
       "      <td>3642</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>CREATE VIEW RAW_CLASSIFY_P2_DEFINITIONAL_NAMES...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cfr2sbvr_v4</td>\n",
       "      <td>1315</td>\n",
       "      <td>main</td>\n",
       "      <td>3616</td>\n",
       "      <td>RAW_CLASSIFY_P2_OPERATIVE_RULES_VW</td>\n",
       "      <td>3644</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>CREATE VIEW RAW_CLASSIFY_P2_OPERATIVE_RULES_VW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cfr2sbvr_v4</td>\n",
       "      <td>1315</td>\n",
       "      <td>main</td>\n",
       "      <td>3616</td>\n",
       "      <td>RAW_CLASSIFY_VW</td>\n",
       "      <td>3645</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>18</td>\n",
       "      <td>CREATE VIEW RAW_CLASSIFY_VW AS (((SELECT CLASS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cfr2sbvr_v4</td>\n",
       "      <td>1315</td>\n",
       "      <td>main</td>\n",
       "      <td>3616</td>\n",
       "      <td>RAW_ELAPSED_TIME_VW</td>\n",
       "      <td>3646</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>CREATE VIEW RAW_ELAPSED_TIME_VW AS SELECT id, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cfr2sbvr_v4</td>\n",
       "      <td>1315</td>\n",
       "      <td>main</td>\n",
       "      <td>3616</td>\n",
       "      <td>RAW_LLM_VALIDATION_BEST_VW</td>\n",
       "      <td>3648</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>26</td>\n",
       "      <td>CREATE VIEW RAW_LLM_VALIDATION_BEST_VW AS SELE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cfr2sbvr_v4</td>\n",
       "      <td>1315</td>\n",
       "      <td>main</td>\n",
       "      <td>3616</td>\n",
       "      <td>RAW_LLM_COMPLETION_VW</td>\n",
       "      <td>3647</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>CREATE VIEW RAW_LLM_COMPLETION_VW AS SELECT id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cfr2sbvr_v4</td>\n",
       "      <td>1315</td>\n",
       "      <td>main</td>\n",
       "      <td>3616</td>\n",
       "      <td>RAW_CLASSIFY_P2_DEFINITIONAL_TERMS_VW</td>\n",
       "      <td>3643</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>CREATE VIEW RAW_CLASSIFY_P2_DEFINITIONAL_TERMS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cfr2sbvr_v4</td>\n",
       "      <td>1315</td>\n",
       "      <td>main</td>\n",
       "      <td>3616</td>\n",
       "      <td>RAW_LLM_VALIDATION_VW</td>\n",
       "      <td>3649</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>28</td>\n",
       "      <td>CREATE VIEW RAW_LLM_VALIDATION_VW AS SELECT VA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cfr2sbvr_v4</td>\n",
       "      <td>1315</td>\n",
       "      <td>main</td>\n",
       "      <td>3616</td>\n",
       "      <td>RAW_TRANSFORM_TERMS_VW</td>\n",
       "      <td>3657</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>CREATE VIEW RAW_TRANSFORM_TERMS_VW AS SELECT T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cfr2sbvr_v4</td>\n",
       "      <td>1315</td>\n",
       "      <td>main</td>\n",
       "      <td>3616</td>\n",
       "      <td>RAW_SECTION_EXTRACTED_ELEMENTS_VW</td>\n",
       "      <td>3650</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>CREATE VIEW RAW_SECTION_EXTRACTED_ELEMENTS_VW ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cfr2sbvr_v4</td>\n",
       "      <td>1315</td>\n",
       "      <td>main</td>\n",
       "      <td>3616</td>\n",
       "      <td>RAW_SECTION_P1_EXTRACTED_ELEMENTS_VW</td>\n",
       "      <td>3651</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>CREATE VIEW RAW_SECTION_P1_EXTRACTED_ELEMENTS_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cfr2sbvr_v4</td>\n",
       "      <td>1315</td>\n",
       "      <td>main</td>\n",
       "      <td>3616</td>\n",
       "      <td>RAW_SECTION_P2_EXTRACTED_NOUN_VW</td>\n",
       "      <td>3652</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>CREATE VIEW RAW_SECTION_P2_EXTRACTED_NOUN_VW A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cfr2sbvr_v4</td>\n",
       "      <td>1315</td>\n",
       "      <td>main</td>\n",
       "      <td>3616</td>\n",
       "      <td>RAW_TRANSFORM_ELEMENTS_VW</td>\n",
       "      <td>3653</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>21</td>\n",
       "      <td>CREATE VIEW RAW_TRANSFORM_ELEMENTS_VW AS (((SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cfr2sbvr_v4</td>\n",
       "      <td>1315</td>\n",
       "      <td>main</td>\n",
       "      <td>3616</td>\n",
       "      <td>RAW_CLASSIFY_P1_OPERATIVE_RULES_VW</td>\n",
       "      <td>3640</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>13</td>\n",
       "      <td>CREATE VIEW RAW_CLASSIFY_P1_OPERATIVE_RULES_VW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cfr2sbvr_v4</td>\n",
       "      <td>1315</td>\n",
       "      <td>main</td>\n",
       "      <td>3616</td>\n",
       "      <td>RAW_TRANSFORM_OPERATIVE_RULES_VW</td>\n",
       "      <td>3656</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>CREATE VIEW RAW_TRANSFORM_OPERATIVE_RULES_VW A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cfr2sbvr_v4</td>\n",
       "      <td>1315</td>\n",
       "      <td>main</td>\n",
       "      <td>3616</td>\n",
       "      <td>RAW_TRANSFORM_FACT_TYPES_VW</td>\n",
       "      <td>3654</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>CREATE VIEW RAW_TRANSFORM_FACT_TYPES_VW AS SEL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   database_name  database_oid schema_name  schema_oid  \\\n",
       "0    cfr2sbvr_v4          1315        main        3616   \n",
       "1    cfr2sbvr_v4          1315        main        3616   \n",
       "2    cfr2sbvr_v4          1315        main        3616   \n",
       "3    cfr2sbvr_v4          1315        main        3616   \n",
       "4    cfr2sbvr_v4          1315        main        3616   \n",
       "5    cfr2sbvr_v4          1315        main        3616   \n",
       "6    cfr2sbvr_v4          1315        main        3616   \n",
       "7    cfr2sbvr_v4          1315        main        3616   \n",
       "8    cfr2sbvr_v4          1315        main        3616   \n",
       "9    cfr2sbvr_v4          1315        main        3616   \n",
       "10   cfr2sbvr_v4          1315        main        3616   \n",
       "11   cfr2sbvr_v4          1315        main        3616   \n",
       "12   cfr2sbvr_v4          1315        main        3616   \n",
       "13   cfr2sbvr_v4          1315        main        3616   \n",
       "14   cfr2sbvr_v4          1315        main        3616   \n",
       "15   cfr2sbvr_v4          1315        main        3616   \n",
       "16   cfr2sbvr_v4          1315        main        3616   \n",
       "17   cfr2sbvr_v4          1315        main        3616   \n",
       "\n",
       "                                view_name  view_oid comment tags  internal  \\\n",
       "0                  RAW_TRANSFORM_NAMES_VW      3655    None   {}     False   \n",
       "1   RAW_CLASSIFY_P2_DEFINITIONAL_FACTS_VW      3641    None   {}     False   \n",
       "2   RAW_CLASSIFY_P2_DEFINITIONAL_NAMES_VW      3642    None   {}     False   \n",
       "3      RAW_CLASSIFY_P2_OPERATIVE_RULES_VW      3644    None   {}     False   \n",
       "4                         RAW_CLASSIFY_VW      3645    None   {}     False   \n",
       "5                     RAW_ELAPSED_TIME_VW      3646    None   {}     False   \n",
       "6              RAW_LLM_VALIDATION_BEST_VW      3648    None   {}     False   \n",
       "7                   RAW_LLM_COMPLETION_VW      3647    None   {}     False   \n",
       "8   RAW_CLASSIFY_P2_DEFINITIONAL_TERMS_VW      3643    None   {}     False   \n",
       "9                   RAW_LLM_VALIDATION_VW      3649    None   {}     False   \n",
       "10                 RAW_TRANSFORM_TERMS_VW      3657    None   {}     False   \n",
       "11      RAW_SECTION_EXTRACTED_ELEMENTS_VW      3650    None   {}     False   \n",
       "12   RAW_SECTION_P1_EXTRACTED_ELEMENTS_VW      3651    None   {}     False   \n",
       "13       RAW_SECTION_P2_EXTRACTED_NOUN_VW      3652    None   {}     False   \n",
       "14              RAW_TRANSFORM_ELEMENTS_VW      3653    None   {}     False   \n",
       "15     RAW_CLASSIFY_P1_OPERATIVE_RULES_VW      3640    None   {}     False   \n",
       "16       RAW_TRANSFORM_OPERATIVE_RULES_VW      3656    None   {}     False   \n",
       "17            RAW_TRANSFORM_FACT_TYPES_VW      3654    None   {}     False   \n",
       "\n",
       "    temporary  column_count                                                sql  \n",
       "0       False            20  CREATE VIEW RAW_TRANSFORM_NAMES_VW AS SELECT T...  \n",
       "1       False            14  CREATE VIEW RAW_CLASSIFY_P2_DEFINITIONAL_FACTS...  \n",
       "2       False            14  CREATE VIEW RAW_CLASSIFY_P2_DEFINITIONAL_NAMES...  \n",
       "3       False            14  CREATE VIEW RAW_CLASSIFY_P2_OPERATIVE_RULES_VW...  \n",
       "4       False            18  CREATE VIEW RAW_CLASSIFY_VW AS (((SELECT CLASS...  \n",
       "5       False             4  CREATE VIEW RAW_ELAPSED_TIME_VW AS SELECT id, ...  \n",
       "6       False            26  CREATE VIEW RAW_LLM_VALIDATION_BEST_VW AS SELE...  \n",
       "7       False             7  CREATE VIEW RAW_LLM_COMPLETION_VW AS SELECT id...  \n",
       "8       False            14  CREATE VIEW RAW_CLASSIFY_P2_DEFINITIONAL_TERMS...  \n",
       "9       False            28  CREATE VIEW RAW_LLM_VALIDATION_VW AS SELECT VA...  \n",
       "10      False            20  CREATE VIEW RAW_TRANSFORM_TERMS_VW AS SELECT T...  \n",
       "11      False            14  CREATE VIEW RAW_SECTION_EXTRACTED_ELEMENTS_VW ...  \n",
       "12      False            11  CREATE VIEW RAW_SECTION_P1_EXTRACTED_ELEMENTS_...  \n",
       "13      False            11  CREATE VIEW RAW_SECTION_P2_EXTRACTED_NOUN_VW A...  \n",
       "14      False            21  CREATE VIEW RAW_TRANSFORM_ELEMENTS_VW AS (((SE...  \n",
       "15      False            13  CREATE VIEW RAW_CLASSIFY_P1_OPERATIVE_RULES_VW...  \n",
       "16      False            20  CREATE VIEW RAW_TRANSFORM_OPERATIVE_RULES_VW A...  \n",
       "17      False            20  CREATE VIEW RAW_TRANSFORM_FACT_TYPES_VW AS SEL...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "SELECT * FROM duckdb_views\n",
    "  WHERE database_name = '{DATABASE if LOCAL_DB else CLOUD_DATABASE}'\n",
    "  AND schema_name = 'main'\n",
    "  AND view_name LIKE '%_VW';\n",
    "\"\"\"\n",
    "\n",
    "views_created = len(conn.sql(query).fetchall())\n",
    "assert views_created == files_processed, f\"Number of views created {views_created} is different from expected {files_processed}\"\n",
    "\n",
    "conn.sql(query).fetchdf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 11:47:47 - INFO - RAW_TRANSFORM_TERMS - Record(s): 280\n",
      "2025-01-16 11:47:47 - INFO - RAW_TRANSFORM_FACT_TYPES - Record(s): 160\n",
      "2025-01-16 11:47:47 - INFO - RAW_SECTION_P2_EXTRACTED_NOUN_TRUE - Record(s): 84\n",
      "2025-01-16 11:47:47 - INFO - RAW_SECTION_P2_EXTRACTED_NOUN - Record(s): 705\n",
      "2025-01-16 11:47:47 - INFO - RAW_SECTION_P1_EXTRACTED_ELEMENTS - Record(s): 215\n",
      "2025-01-16 11:47:47 - INFO - RAW_SECTION - Record(s): 3\n",
      "2025-01-16 11:47:48 - INFO - RAW_SECTION_P1_EXTRACTED_ELEMENTS_TRUE - Record(s): 22\n",
      "2025-01-16 11:47:48 - INFO - RAW_LLM_COMPLETION - Record(s): 1210\n",
      "2025-01-16 11:47:48 - INFO - RAW_ELAPSED_TIME - Record(s): 1210\n",
      "2025-01-16 11:47:48 - INFO - RAW_CLASSIFY_P2_DEFINITIONAL_TERMS_TRUE - Record(s): 28\n",
      "2025-01-16 11:47:48 - INFO - RAW_CLASSIFY_P2_OPERATIVE_RULES - Record(s): 60\n",
      "2025-01-16 11:47:48 - INFO - RAW_CLASSIFY_P2_DEFINITIONAL_TERMS - Record(s): 280\n",
      "2025-01-16 11:47:49 - INFO - RAW_CLASSIFY_P2_DEFINITIONAL_NAMES_TRUE - Record(s): 5\n",
      "2025-01-16 11:47:49 - INFO - RAW_LLM_VALIDATION - Record(s): 550\n",
      "2025-01-16 11:47:49 - INFO - RAW_CLASSIFY_P2_OPERATIVE_RULES_TRUE - Record(s): 6\n",
      "2025-01-16 11:47:49 - INFO - RAW_CLASSIFY_P2_DEFINITIONAL_NAMES - Record(s): 50\n",
      "2025-01-16 11:47:49 - INFO - RAW_TRANSFORM_NAMES - Record(s): 50\n",
      "2025-01-16 11:47:49 - INFO - RAW_CLASSIFY_P2_DEFINITIONAL_FACTS_TRUE - Record(s): 16\n",
      "2025-01-16 11:47:50 - INFO - RAW_CLASSIFY_P2_DEFINITIONAL_FACTS - Record(s): 160\n",
      "2025-01-16 11:47:50 - INFO - RAW_TRANSFORM_OPERATIVE_RULES - Record(s): 60\n",
      "2025-01-16 11:47:50 - INFO - RAW_CLASSIFY_P1_OPERATIVE_RULES_TRUE - Record(s): 6\n",
      "2025-01-16 11:47:50 - INFO - RAW_CLASSIFY_P1_OPERATIVE_RULES - Record(s): 60\n",
      "2025-01-16 11:47:50 - INFO - CHECKPOINT_METADATA - Record(s): 37\n",
      "2025-01-16 11:47:51 - INFO - RAW_TRANSFORM_NAMES_VW - Record(s): 50\n",
      "2025-01-16 11:47:51 - INFO - RAW_CLASSIFY_P2_DEFINITIONAL_FACTS_VW - Record(s): 176\n",
      "2025-01-16 11:47:51 - INFO - RAW_CLASSIFY_P2_DEFINITIONAL_NAMES_VW - Record(s): 55\n",
      "2025-01-16 11:47:51 - INFO - RAW_CLASSIFY_P2_OPERATIVE_RULES_VW - Record(s): 66\n",
      "2025-01-16 11:47:52 - INFO - RAW_CLASSIFY_VW - Record(s): 605\n",
      "2025-01-16 11:47:52 - INFO - RAW_ELAPSED_TIME_VW - Record(s): 1210\n",
      "2025-01-16 11:47:52 - INFO - RAW_LLM_VALIDATION_BEST_VW - Record(s): 51\n",
      "2025-01-16 11:47:53 - INFO - RAW_LLM_COMPLETION_VW - Record(s): 1210\n",
      "2025-01-16 11:47:53 - INFO - RAW_CLASSIFY_P2_DEFINITIONAL_TERMS_VW - Record(s): 308\n",
      "2025-01-16 11:47:53 - INFO - RAW_LLM_VALIDATION_VW - Record(s): 550\n",
      "2025-01-16 11:47:54 - INFO - RAW_TRANSFORM_TERMS_VW - Record(s): 280\n",
      "2025-01-16 11:47:54 - INFO - RAW_SECTION_EXTRACTED_ELEMENTS_VW - Record(s): 237\n",
      "2025-01-16 11:47:54 - INFO - RAW_SECTION_P1_EXTRACTED_ELEMENTS_VW - Record(s): 237\n",
      "2025-01-16 11:47:54 - INFO - RAW_SECTION_P2_EXTRACTED_NOUN_VW - Record(s): 789\n",
      "2025-01-16 11:47:55 - INFO - RAW_TRANSFORM_ELEMENTS_VW - Record(s): 550\n",
      "2025-01-16 11:47:55 - INFO - RAW_CLASSIFY_P1_OPERATIVE_RULES_VW - Record(s): 66\n",
      "2025-01-16 11:47:55 - INFO - RAW_TRANSFORM_OPERATIVE_RULES_VW - Record(s): 60\n",
      "2025-01-16 11:47:55 - INFO - RAW_TRANSFORM_FACT_TYPES_VW - Record(s): 160\n"
     ]
    }
   ],
   "source": [
    "for table_or_view_name in tables_or_views:\n",
    "    object_schema = table_or_view_name[1]\n",
    "    object_name = table_or_view_name[2]\n",
    "    object_type = table_or_view_name[3]\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT COUNT(1) FROM {object_schema}.{object_name};\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(f\"{object_name} - Record(s): {conn.sql(query).fetchall()[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipt-cfr2sbvr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
