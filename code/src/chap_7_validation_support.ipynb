{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation support\n",
    "\n",
    "Evaluating of consistency of the checkpoints.\n",
    "\n",
    "Dependencies:\n",
    "- Copy checkpoints files from evaluation and extraction to `cfr2sbvr_db`\n",
    "- Python DuckDB: `pip install duckdb --upgrade`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  !rm -rf cfr2sbvr configuration checkpoint\n",
    "  !git clone https://github.com/asantos2000/master-degree-santos-anderson.git cfr2sbvr\n",
    "  %pip install -r cfr2sbvr/code/requirements.txt\n",
    "  !cp -r cfr2sbvr/code/src/configuration .\n",
    "  !cp -r cfr2sbvr/code/src/checkpoint .\n",
    "  !cp -r cfr2sbvr/code/config.colab.yaml config.yaml\n",
    "  DEFAULT_CONFIG_FILE=\"config.yaml\"\n",
    "else:\n",
    "  DEFAULT_CONFIG_FILE=\"../config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import json\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import humanize\n",
    "\n",
    "# Local modules\n",
    "import configuration.main as configuration\n",
    "import logging_setup.main as logging_setup\n",
    "\n",
    "DEV_MODE = True\n",
    "\n",
    "if DEV_MODE:\n",
    "    # Development mode\n",
    "    import importlib\n",
    "\n",
    "    importlib.reload(configuration)\n",
    "    importlib.reload(logging_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = configuration.load_config(DEFAULT_CONFIG_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging_setup.setting_logging(config[\"DEFAULT_LOG_DIR\"], config[\"LOG_LEVEL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_horizontal_bar_chart(data, group_col, value_col, title, x_label, y_label, figsize=(10, 4), legend_position=\"best\", big_numbers={\"factor\":1000, \"suffix\":\"mil\"}):\n",
    "    \"\"\"\n",
    "    Plota um gráfico de barras horizontais estilizado com base nos parâmetros fornecidos.\n",
    "    \n",
    "    :param data: DataFrame contendo os dados\n",
    "    :param group_col: Coluna para agrupar no eixo Y\n",
    "    :param value_col: Coluna de valores para o eixo X\n",
    "    :param title: Título do gráfico\n",
    "    :param x_label: Rótulo do eixo X\n",
    "    :param y_label: Rótulo do eixo Y\n",
    "    :param figsize: Tamanho da figura\n",
    "    :param legend_position: Posição da legenda ('upper right', 'lower right', etc.), ou None para posicionar automaticamente\n",
    "    \"\"\"\n",
    "    # Agrupando os dados e somando os valores\n",
    "    filterd = data.groupby(group_col)[value_col].sum().reset_index()\n",
    "\n",
    "    # Calculando a média para o destaque\n",
    "    avg_value = filterd[value_col].mean()\n",
    "\n",
    "    # Configurando o tamanho do gráfico\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Adicionando linhas horizontais para conectar os marcadores ao eixo Y\n",
    "    colors = [\"#E69F00\" if val > avg_value else \"#56B4E9\" for val in filterd[value_col]]\n",
    "    for i, (group, val, color) in enumerate(zip(filterd[group_col], filterd[value_col], colors)):\n",
    "        plt.plot(\n",
    "            [0, val],\n",
    "            [group, group],\n",
    "            color=color,\n",
    "            linewidth=2,\n",
    "            zorder=1\n",
    "        )\n",
    "\n",
    "    # Adicionando os marcadores com cores amigáveis para daltônicos\n",
    "    plt.scatter(\n",
    "        filterd[value_col],\n",
    "        filterd[group_col],\n",
    "        color=colors,\n",
    "        s=100,  # Tamanho dos marcadores\n",
    "        zorder=2\n",
    "    )\n",
    "\n",
    "    # Adicionando uma linha vertical para a média\n",
    "    plt.axvline(\n",
    "        avg_value,\n",
    "        color=\"grey\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1.5,\n",
    "        label=f'Média: {avg_value / big_numbers[\"factor\"]:,.2f} {big_numbers[\"suffix\"]}'.replace('.', ',')\n",
    "    )\n",
    "\n",
    "    # Adicionando valores diretamente ao lado dos marcadores\n",
    "    for group, val in zip(filterd[group_col], filterd[value_col]):\n",
    "        plt.text(\n",
    "            val + 0.05 * avg_value,  # Posicionando à direita\n",
    "            group,\n",
    "            f'{val / big_numbers[\"factor\"]:,.2f} {big_numbers[\"suffix\"]}'.replace('.', ','),\n",
    "            va=\"center\",\n",
    "            fontsize=10,\n",
    "            zorder=3\n",
    "        )\n",
    "\n",
    "    # Adicionando linhas verticais no eixo X com cor cinza claro\n",
    "    plt.gca().xaxis.grid(color=\"lightgrey\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "    # Função personalizada para formatar os ticks do eixo X\n",
    "    def format_ticks(x, pos):\n",
    "        return f'{x / big_numbers[\"factor\"]:,.1f} {big_numbers[\"suffix\"]}'.replace('.', ',')\n",
    "\n",
    "    # Aplicando a formatação ao eixo X\n",
    "    plt.gca().xaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
    "\n",
    "    # Ajustando o título e os eixos\n",
    "    plt.title(title, fontsize=16, loc=\"left\", pad=20)\n",
    "    plt.xlabel(x_label, fontsize=14)\n",
    "    plt.ylabel(y_label, fontsize=14)\n",
    "\n",
    "    # Ajustando os limites do eixo Y para compactar os valores\n",
    "    plt.gca().set_ylim(-0.2, len(filterd[group_col]) - 0.8)\n",
    "\n",
    "    # Removendo as linhas superior, direita e esquerda (spines)\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    # Configurando o layout e a legenda\n",
    "    plt.legend(loc=legend_position, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Mostrando o gráfico\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_table_from_json(\n",
    "    conn,\n",
    "    table_name,\n",
    "    source,\n",
    "    key_value,\n",
    "    content_key,\n",
    "    alias,\n",
    "    drop=False,\n",
    "    doc_id_key=\"id\",\n",
    "    key_pattern1=\"\",\n",
    "    key_pattern2=\"\",\n",
    "    nested=True\n",
    ") -> bool:\n",
    "    \n",
    "    print(f\"{table_name=}\")\n",
    "    print(f\"{source=}\")\n",
    "    print(f\"{key_value=}\")\n",
    "    print(f\"{content_key=}\")\n",
    "    print(f\"{alias=}\")\n",
    "    print(f\"{drop=}\")\n",
    "    \n",
    "    _data = []\n",
    "    _source_file = Path(source).name\n",
    "\n",
    "    print(f\"{_source_file=}\")\n",
    "\n",
    "    directory_path = Path(f\"{config['DEFAULT_DATA_DIR']}/temp\")\n",
    "\n",
    "    print(f\"{directory_path=}\")\n",
    "\n",
    "    # Combine the directory path and the filename\n",
    "    _temp_file = directory_path / _source_file\n",
    "\n",
    "    print(f\"{_temp_file=}\")\n",
    "\n",
    "    with open(source, \"r\") as f:\n",
    "        loaded_data = json.load(f)\n",
    "\n",
    "    keys = loaded_data.keys()\n",
    "\n",
    "    print(f\"{len(keys)=}\")\n",
    "\n",
    "    for key in keys:\n",
    "        if (\n",
    "            key_pattern1 in key and key_pattern2 in key\n",
    "        ):  # key key.startswith(prefix_key_pattern) and key.endswith(suffix_key_pattern):\n",
    "            print(key)\n",
    "            _data.append(loaded_data[key])\n",
    "\n",
    "    if not _data:\n",
    "        print(\"No data found. Check the key pattern.\")\n",
    "        return False\n",
    "\n",
    "    with open(_temp_file, \"w\") as f:\n",
    "        json.dump(_data, f, indent=4)\n",
    "\n",
    "    if drop:\n",
    "        _query_drop_table = f\"\"\"\n",
    "        DROP TABLE IF EXISTS {table_name};\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"{_query_drop_table=}\")\n",
    "        print(f\"Trying to drop table because drop parameter is {drop}\")\n",
    "\n",
    "        try:\n",
    "            conn.execute(_query_drop_table)\n",
    "            print(f\"Table {table_name} dropped\")\n",
    "        except duckdb.CatalogException as e:\n",
    "            print(e)\n",
    "\n",
    "    unnest_clause = f\"unnest({content_key}) as {alias}\" if nested else f\"{content_key} as {alias}\"\n",
    "\n",
    "    _query_insert_data = f\"\"\"\n",
    "    INSERT INTO {table_name} ({doc_id_key}, prompt, file_source, {alias}, created_at)\n",
    "    SELECT \n",
    "        {doc_id_key}, \n",
    "        '{key_value}' as prompt, \n",
    "        '{_source_file}' as file_source, \n",
    "        {unnest_clause},\n",
    "        now() as created_at\n",
    "    FROM \n",
    "        read_json_auto(\"{_temp_file}\");\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"{_query_insert_data=}\")\n",
    "    print(f\"Trying to insert into {table_name}\")\n",
    "\n",
    "    try:\n",
    "        conn.execute(_query_insert_data)\n",
    "        print(f\"Data inserted into {table_name}.\")\n",
    "    except duckdb.CatalogException as e:\n",
    "        print(e)\n",
    "        print(f\"Failed to insert, trying create {table_name}\")\n",
    "        _query_create_table = f\"\"\"\n",
    "        CREATE TABLE {table_name} AS\n",
    "        SELECT {doc_id_key}, \n",
    "        '{key_value}' as prompt, \n",
    "        '{_source_file}' as file_source, \n",
    "        {unnest_clause},\n",
    "        now() as 'created_at'\n",
    "        FROM read_json_auto(\"{_temp_file}\");\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"{_query_create_table=}\")\n",
    "            conn.execute(_query_create_table)\n",
    "            print(f\"Table {table_name} created and loaded.\")\n",
    "        except duckdb.CatalogException as e:\n",
    "            print(e)\n",
    "            print(f\"Failed to create {table_name}\")\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect local or cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DB=True\n",
    "DATABASE = \"database_v5.db\" # \"database_v5.db\" or \"database_v4.db\"\n",
    "CLOUD_DATABASE = \"md:cfr2sbvr_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOCAL_DB:\n",
    "    conn = duckdb.connect(f'{config[\"DEFAULT_DATA_DIR\"]}/cfr2sbvr_db/{DATABASE}')\n",
    "else:\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    mother_duck_token=os.getenv(\"MOTHER_DUCK_TOKEN\")\n",
    "    conn = duckdb.connect(f'{CLOUD_DATABASE}?motherduck_token={mother_duck_token}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_UP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SHOW TABLES;\n",
    "\"\"\"\n",
    "\n",
    "tables = conn.sql(query).fetchall()\n",
    "\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name in tables:\n",
    "    query = f\"\"\"\n",
    "    drop table {table_name[0]};\n",
    "    \"\"\"\n",
    "\n",
    "    # If True then drop the table\n",
    "    if CLEAN_UP:\n",
    "        print(query)\n",
    "        conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "DROP TABLE IF EXISTS CHECKPOINT_METADATA;\n",
    "\n",
    "CREATE TABLE CHECKPOINT_METADATA AS\n",
    "SELECT process,\n",
    "doc_source,\n",
    "doc_id,\n",
    "doc_type,\n",
    "table_name, \n",
    "now() as 'created_at'\n",
    "FROM read_csv_auto(\"{config['DEFAULT_DATA_DIR']}/cfr2sbvr_db/metadata/checkpoints_metadata.csv\");\n",
    "\"\"\"\n",
    "\n",
    "if CLEAN_UP:\n",
    "    conn.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *,\n",
    "FROM CHECKPOINT_METADATA;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT TABLE_NAME,\n",
    "FROM CHECKPOINT_METADATA\n",
    "WHERE DOC_SOURCE = 'both'\n",
    "ORDER BY 1;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "CFR sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_table_from_json(\n",
    "    conn,\n",
    "    key_pattern1=\"|section\",\n",
    "    key_pattern2=\"§\",\n",
    "    table_name=\"RAW_SECTION\",\n",
    "    source=f\"{config['DEFAULT_DATA_DIR']}/documents_true_table.json\",\n",
    "    key_value=\"section\",\n",
    "    drop=True,\n",
    "    content_key=\"content\",\n",
    "    alias=\"content\",\n",
    "    doc_id_key=\"id\",\n",
    "    nested=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *,\n",
    "FROM RAW_SECTION;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *,\n",
    "FROM CHECKPOINT_METADATA\n",
    "WHERE PROCESS='extraction'\n",
    "ORDER BY DOC_ID, DOC_TYPE DESC;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract elements from sections (P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_table_from_json(\n",
    "    conn,\n",
    "    key_pattern1=\"_P1|true_table\",\n",
    "    key_pattern2=\"§\",\n",
    "    table_name=\"RAW_SECTION_P1_EXTRACTED_ELEMENTS_TRUE\",\n",
    "    source=f\"{config['DEFAULT_DATA_DIR']}/documents_true_table.json\",\n",
    "    key_value=\"extract_p1\",\n",
    "    drop=True,\n",
    "    content_key=\"content.elements\",\n",
    "    alias=\"elements\",\n",
    "    doc_id_key=\"id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM RAW_SECTION_P1_EXTRACTED_ELEMENTS_TRUE;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoints_extraction to evaluate the original extraction\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_extraction\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"_P1|llm_response\",\n",
    "        key_pattern2=\"§\",\n",
    "        # suffix_key_pattern=\"_P1|llm_response\",\n",
    "        # prefix_key_pattern=\"§\",\n",
    "        table_name=\"RAW_SECTION_P1_EXTRACTED_ELEMENTS\",\n",
    "        source=file_path,\n",
    "        key_value=\"extract_p1\",\n",
    "        drop=drop,\n",
    "        content_key=\"content.elements\",\n",
    "        alias=\"elements\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)#.fetchdf().to_excel(\"elements_classification.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT count(distinct elements) as elements_count\n",
    "FROM RAW_SECTION_P1_EXTRACTED_ELEMENTS_TRUE\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)#.fetchdf().to_excel(\"elements_classification.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT file_source, elements.classification, count(*) as count\n",
    "FROM RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "GROUP BY file_source, elements.classification\n",
    "ORDER BY file_source, elements.classification\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)#.fetchdf().to_excel(\"elements_classification.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, elements.classification, count(*) as count\n",
    "FROM RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "GROUP BY id, elements.classification\n",
    "ORDER BY id, elements.classification\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)#.fetchdf().to_excel(\"elements_classification.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, count(*) FROM RAW_SECTION_P1_EXTRACTED_ELEMENTS_TRUE\n",
    "GROUP BY id\n",
    "ORDER BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, count(*) FROM RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "GROUP BY id\n",
    "ORDER BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verb symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total verb symbols extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    COUNT(verb) AS total_verbs\n",
    "FROM (\n",
    "    SELECT\n",
    "        UNNEST(CAST(json_extract(elements, '$.verb_symbols') AS VARCHAR[])) AS verb\n",
    "    FROM\n",
    "        RAW_SECTION_P1_EXTRACTED_ELEMENTS_TRUE\n",
    ") AS flattened_verbs;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distinct verb symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    COUNT(DISTINCT verb) AS distinct_verbs_count\n",
    "FROM (\n",
    "    SELECT\n",
    "        UNNEST(CAST(json_extract(elements, '$.verb_symbols') AS VARCHAR[])) AS verb\n",
    "    FROM\n",
    "        RAW_SECTION_P1_EXTRACTED_ELEMENTS_TRUE\n",
    ") AS flattened_verbs;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT\n",
    "    verb,\n",
    "    doc_id,\n",
    "    source\n",
    "FROM (\n",
    "    SELECT\n",
    "        UNNEST(CAST(json_extract(elements, '$.verb_symbols') AS VARCHAR[])) AS verb,\n",
    "        id AS doc_id,\n",
    "        CAST(json_extract(elements, '$.sources') AS VARCHAR) AS source\n",
    "    FROM\n",
    "        RAW_SECTION_P1_EXTRACTED_ELEMENTS_TRUE\n",
    ") AS distinct_combinations;\n",
    "\"\"\"\n",
    "\n",
    "section_p1_extracted_elements_true_values = conn.sql(query).fetchall()\n",
    "\n",
    "conn.sql(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total verb symbols extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, COUNT(verb) AS total_verbs\n",
    "FROM (\n",
    "    SELECT\n",
    "        id, UNNEST(CAST(json_extract(elements, '$.verb_symbols') AS VARCHAR[])) AS verb\n",
    "    FROM\n",
    "        RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    ") \n",
    "GROUP BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, terms\n",
    "FROM (\n",
    "    SELECT id, UNNEST(terms) AS terms\n",
    "    FROM (\n",
    "        SELECT\n",
    "            id, UNNEST(elements) AS element\n",
    "        FROM\n",
    "            RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "    )\n",
    ")\n",
    "ORDER BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, terms.classification AS term_classification, count(if(terms.term is NULL, NULL, 1)) as hasDefinition, COUNT(*) AS term_count\n",
    "FROM (\n",
    "    SELECT id, UNNEST(terms) AS terms\n",
    "    FROM (\n",
    "        SELECT\n",
    "            id, UNNEST(elements) AS element\n",
    "        FROM\n",
    "            RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "    )\n",
    ")\n",
    "GROUP BY id, terms.classification, if(terms.term is NULL, NULL, 1)\n",
    "ORDER BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, terms.classification AS term_classification, terms.term AS term, if(terms.term is NULL, NULL, 1) as hasDefinition\n",
    "FROM (\n",
    "    SELECT id, UNNEST(terms) AS terms\n",
    "    FROM (\n",
    "        SELECT\n",
    "            id, UNNEST(elements) AS element\n",
    "        FROM\n",
    "            RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "    )\n",
    ")\n",
    "ORDER BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distinct verb symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    COUNT(DISTINCT verb) AS distinct_verbs_count\n",
    "FROM (\n",
    "    SELECT\n",
    "        UNNEST(CAST(json_extract(elements, '$.verb_symbols') AS VARCHAR[])) AS verb\n",
    "    FROM\n",
    "        RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    ") AS flattened_verbs;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distinct verb symbols with doc_id and source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT\n",
    "    verb,\n",
    "    doc_id,\n",
    "    source\n",
    "FROM (\n",
    "    SELECT\n",
    "        UNNEST(CAST(json_extract(elements, '$.verb_symbols') AS VARCHAR[])) AS verb,\n",
    "        id AS doc_id,\n",
    "        CAST(json_extract(elements, '$.sources') AS VARCHAR) AS source\n",
    "    FROM\n",
    "        RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    ") AS distinct_combinations;\n",
    "\"\"\"\n",
    "\n",
    "# pred_values=duckdb.sql(query).fetchall()\n",
    "\n",
    "conn.sql(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verb symbols with doc_id and source for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    verb,\n",
    "    doc_id,\n",
    "    source\n",
    "FROM (\n",
    "    SELECT\n",
    "        UNNEST(CAST(json_extract(elements, '$.verb_symbols') AS VARCHAR[])) AS verb,\n",
    "        id AS doc_id,\n",
    "        CAST(json_extract(elements, '$.sources') AS VARCHAR) AS source\n",
    "    FROM\n",
    "        RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    ") AS distinct_combinations;\n",
    "\"\"\"\n",
    "\n",
    "section_p1_extracted_elements_pred_values = conn.sql(query).fetchall()\n",
    "\n",
    "conn.sql(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Terms and Names definitions (P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_table_from_json(\n",
    "    conn,\n",
    "    key_pattern1=\"_P2|true_table\",\n",
    "    key_pattern2=\"§\",\n",
    "    # suffix_key_pattern=\"_P2|true_table\",\n",
    "    # prefix_key_pattern=\"§\",\n",
    "    table_name=\"RAW_SECTION_P2_EXTRACTED_NOUN_TRUE\",\n",
    "    source=f\"{config['DEFAULT_DATA_DIR']}/documents_true_table.json\",\n",
    "    key_value=\"extract_p2\",\n",
    "    drop=True,\n",
    "    content_key=\"content.terms\",\n",
    "    alias=\"terms\",\n",
    "    doc_id_key=\"id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total terms per id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, COUNT(terms) AS total_terms, COUNT(DISTINCT terms) AS total_terms_distinct\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE\n",
    "GROUP BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, terms.term as term, terms.definition as definition, if(terms.definition is NULL, 0, 1) as hasDefinition, terms.isLocalScope as isLocalScope\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE\n",
    "-- GROUP BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many terms has definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, count(terms) as term, count(if(terms.definition is NULL, NULL, 1)) as hasDefinition, count(if(terms.definition is NULL, NULL, 1)) / count(terms) as ratio\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE\n",
    "GROUP BY id\n",
    "ORDER BY id\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "isLocalScope per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH TotalCounts AS (\n",
    "    SELECT\n",
    "        id,\n",
    "        COUNT(terms) AS total_terms\n",
    "    FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE\n",
    "    GROUP BY id\n",
    ")\n",
    "SELECT\n",
    "    e.id,\n",
    "    COUNT(e.terms) AS term,\n",
    "    e.terms.isLocalScope AS isLocalScope,\n",
    "    (COUNT(e.terms) * 100.0 / tc.total_terms) AS percentage\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE e\n",
    "JOIN TotalCounts tc ON e.id = tc.id\n",
    "GROUP BY e.id, e.terms.isLocalScope, tc.total_terms\n",
    "ORDER BY e.id, e.terms.isLocalScope;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, terms, terms.isLocalScope as isLocalScope\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE\n",
    "WHERE terms.isLocalScope is NULL\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_extraction\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"_P2|llm_response\",\n",
    "        key_pattern2=\"§\",\n",
    "        # suffix_key_pattern=\"_P2|llm_response\",\n",
    "        # prefix_key_pattern=\"§\",\n",
    "        table_name=\"RAW_SECTION_P2_EXTRACTED_NOUN\",\n",
    "        source=file_path,\n",
    "        key_value=\"extract_p2\",\n",
    "        drop=drop,\n",
    "        content_key=\"content.terms\",\n",
    "        alias=\"terms\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT * FROM RAW_SECTION_P2_EXTRACTED_NOUN;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT ID, COUNT(*)\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "GROUP BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DESCRIBE RAW_SECTION_P2_EXTRACTED_NOUN;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, terms.classification AS term_classification, terms.term AS term\n",
    "FROM (\n",
    "    SELECT id, UNNEST(terms) AS terms\n",
    "    FROM (\n",
    "        SELECT\n",
    "            id, UNNEST(elements) AS element\n",
    "        FROM\n",
    "            RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "    )\n",
    ")\n",
    "ORDER BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has definition ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, count(terms) as term, count(if(terms.definition is NULL, NULL, 1)) as hasDefinition, count(if(terms.definition is NULL, NULL, 1)) / count(terms) as ratio\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "GROUP BY id\n",
    "ORDER BY id\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terms and names classify by definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, terms.term as term, if(terms.definition is NULL, NULL, 1) as hasDefinition\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT \n",
    "        id,\n",
    "        terms.term AS term,\n",
    "        terms.classification AS term_classification,\n",
    "        count(terms.term) AS term_count\n",
    "    FROM (\n",
    "        SELECT \n",
    "            id, UNNEST(terms) AS terms\n",
    "        FROM (\n",
    "            SELECT \n",
    "                id, UNNEST(elements) AS element\n",
    "            FROM \n",
    "                RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "        )\n",
    "    )\n",
    "    GROUP BY id, terms.term, terms.classification\n",
    "    ORDER BY id, terms.term;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)#.fetchdf().to_excel(\"terms_classification.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT\n",
    "        id, \n",
    "        terms.term AS term, \n",
    "        count(IF(terms.definition IS NULL, NULL, 1)) AS hasDefinition\n",
    "    FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "    GROUP BY id, terms.term\n",
    "    ORDER BY id, terms.term;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)#.fetchdf().to_excel(\"terms_definition.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join results from P1 and P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH terms_classification AS (SELECT \n",
    "        REPLACE(id, '_P1', '') AS doc_id,\n",
    "        terms.term AS term,\n",
    "        terms.classification AS term_classification,\n",
    "        count(terms.term) AS term_count\n",
    "    FROM (\n",
    "        SELECT \n",
    "            id, UNNEST(terms) AS terms\n",
    "        FROM (\n",
    "            SELECT \n",
    "                id, UNNEST(elements) AS element\n",
    "            FROM \n",
    "                RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "        )\n",
    "    )\n",
    "    GROUP BY id, terms.term, terms.classification\n",
    "    ORDER BY id, terms.term),\n",
    "terms_definition AS (\n",
    "    SELECT\n",
    "        REPLACE(id, '_P2', '') AS doc_id,\n",
    "        terms.term AS term, \n",
    "        count(IF(terms.definition IS NULL, NULL, 1)) AS hasDefinition\n",
    "    FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "    GROUP BY id, terms.term\n",
    "    ORDER BY id, terms.term\n",
    ")\n",
    "SELECT\n",
    "    terms_classification.doc_id,\n",
    "    terms_classification.term,\n",
    "    terms_classification.term_classification,\n",
    "    terms_classification.term_count,\n",
    "    terms_definition.hasDefinition\n",
    "FROM terms_classification  \n",
    "LEFT JOIN terms_definition\n",
    "ON terms_classification.doc_id = terms_definition.doc_id AND terms_classification.term = terms_definition.term;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of terms extract P1 (Proper and Common), P2 (with and without definition)\n",
    "\n",
    "> Terms summary for dissertation's tables: \"Tabela 2  - Conjunto de dados ouro para validação (P1)\" and \"Tabela 3 – Termos e nomes com definição.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH terms_classification AS (SELECT \n",
    "        REPLACE(id, '_P1', '') AS doc_id,\n",
    "        terms.term AS term,\n",
    "        terms.classification AS term_classification,\n",
    "        count(terms.term) AS term_count\n",
    "    FROM (\n",
    "        SELECT \n",
    "            id, UNNEST(terms) AS terms\n",
    "        FROM (\n",
    "            SELECT \n",
    "                id, UNNEST(elements) AS element\n",
    "            FROM \n",
    "                RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "        )\n",
    "    )\n",
    "    GROUP BY id, terms.term, terms.classification\n",
    "    ORDER BY id, terms.term),\n",
    "terms_definition AS (\n",
    "    SELECT\n",
    "        REPLACE(id, '_P2', '') AS doc_id,\n",
    "        terms.term AS term, \n",
    "        if(count(IF(terms.definition IS NULL, NULL, 1)) > 0, 'Yes', 'No') AS hasDefinition\n",
    "    FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "    GROUP BY id, terms.term\n",
    "    ORDER BY id, terms.term\n",
    ")\n",
    "SELECT\n",
    "    terms_classification.doc_id,\n",
    "    -- terms_classification.term,\n",
    "    terms_classification.term_classification,\n",
    "    terms_definition.hasDefinition,\n",
    "    count(*) AS term_count\n",
    "FROM terms_classification  \n",
    "LEFT JOIN terms_definition\n",
    "ON terms_classification.doc_id = terms_definition.doc_id AND terms_classification.term = terms_definition.term\n",
    "GROUP BY terms_classification.doc_id, terms_classification.term_classification, terms_definition.hasDefinition\n",
    "ORDER BY terms_classification.doc_id, terms_classification.term_classification, terms_definition.hasDefinition;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)#.fetchdf().to_excel(\"terms_classification_definition.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTERSECTION(TRUE, PRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, prompt, term, definition, isLocalScope\n",
    "FROM (\n",
    "SELECT DISTINCT id, prompt, file_source, created_at, UNNEST(terms) as terms, terms.term, terms.definition, terms.isLocalScope\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE\n",
    ")\n",
    "EXCEPT\n",
    "SELECT DISTINCT id, prompt, term, definition, isLocalScope\n",
    "FROM (\n",
    "SELECT DISTINCT id, prompt, file_source, created_at, UNNEST(terms) as terms, terms.term, terms.definition, terms.isLocalScope\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTERSECTION(PRED, TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, prompt, term, definition, isLocalScope\n",
    "FROM (\n",
    "SELECT DISTINCT id, prompt, file_source, created_at, UNNEST(terms) as terms, terms.term, terms.definition, terms.isLocalScope\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    ")\n",
    "EXCEPT\n",
    "SELECT DISTINCT id, prompt, term, definition, isLocalScope\n",
    "FROM (\n",
    "SELECT DISTINCT id, prompt, file_source, created_at, UNNEST(terms) as terms, terms.term, terms.definition, terms.isLocalScope\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN_TRUE\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total terms and distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, COUNT(terms) AS total_terms, COUNT(DISTINCT terms) AS total_terms_distinct\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "GROUP BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distinct terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, terms.term as term, terms.definition as definition, if(terms.definition is NULL, 0, 1) as hasDefinition, terms.isLocalScope as isLocalScope\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "-- GROUP BY id;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many terms has definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, count(terms) as term, count(if(terms.definition is NULL, NULL, 1)) as hasDefinition, count(if(terms.definition is NULL, NULL, 1)) / count(terms) as ratio\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "GROUP BY id\n",
    "ORDER BY id\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "isLocalScope per id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH TotalCounts AS (\n",
    "    SELECT\n",
    "        id,\n",
    "        COUNT(terms) AS total_terms\n",
    "    FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "    GROUP BY id\n",
    ")\n",
    "SELECT\n",
    "    e.id,\n",
    "    COUNT(e.terms) AS term,\n",
    "    e.terms.isLocalScope AS isLocalScope,\n",
    "    (COUNT(e.terms) * 100.0 / tc.total_terms) AS percentage\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN e\n",
    "JOIN TotalCounts tc ON e.id = tc.id\n",
    "GROUP BY e.id, e.terms.isLocalScope, tc.total_terms\n",
    "ORDER BY e.id, e.terms.isLocalScope;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id, terms, terms.isLocalScope as isLocalScope\n",
    "FROM RAW_SECTION_P2_EXTRACTED_NOUN\n",
    "WHERE terms.isLocalScope is NULL\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset extraction tables to evaluation checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoints_extraction to evaluate the original extraction\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_extraction\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"_P1|llm_response\",\n",
    "        key_pattern2=\"§\",\n",
    "        # suffix_key_pattern=\"_P1|llm_response\",\n",
    "        # prefix_key_pattern=\"§\",\n",
    "        table_name=\"RAW_SECTION_P1_EXTRACTED_ELEMENTS\",\n",
    "        source=file_path,\n",
    "        key_value=\"extract_p1\",\n",
    "        drop=drop,\n",
    "        content_key=\"content.elements\",\n",
    "        alias=\"elements\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT distinct file_source\n",
    "FROM RAW_SECTION_P1_EXTRACTED_ELEMENTS\n",
    "ORDER BY 1;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DESCRIBE RAW_SECTION_P1_EXTRACTED_ELEMENTS;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *,\n",
    "FROM CHECKPOINT_METADATA\n",
    "WHERE PROCESS='classification'\n",
    "ORDER BY DOC_ID, DOC_TYPE DESC;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify P1 - Operative Rules into top-level Witt(2012) taxonomy (P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_table_from_json(\n",
    "    conn,\n",
    "    key_pattern1=\"classify_P1|true_table\",\n",
    "    # suffix_key_pattern=\"classify_P1|true_table\",\n",
    "    # prefix_key_pattern=\"\",\n",
    "    table_name=\"RAW_CLASSIFY_P1_OPERATIVE_RULES_TRUE\",\n",
    "    source=f\"{config['DEFAULT_DATA_DIR']}/documents_true_table.json\",\n",
    "    key_value=\"classify_p1\",\n",
    "    drop=True,\n",
    "    content_key=\"content\",\n",
    "    alias=\"content\",\n",
    "    doc_id_key=\"id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_classification\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"classify_P1|llm_response_classification\",\n",
    "        # suffix_key_pattern=\"classify_P1|llm_response_classification\",\n",
    "        # prefix_key_pattern=\"\",\n",
    "        table_name=\"RAW_CLASSIFY_P1_OPERATIVE_RULES\",\n",
    "        source=file_path,\n",
    "        key_value=\"classify_p1\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    content.doc_id as id,\n",
    "    content.statement_id,\n",
    "    content.statement_title,\n",
    "    content.statement_text,\n",
    "    content.statement_sources,\n",
    "    content.classification\n",
    "FROM RAW_CLASSIFY_P1_OPERATIVE_RULES\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    content.doc_id as doc_id,\n",
    "    content.statement_id as statement_id,\n",
    "    content.statement_title as statement_title,\n",
    "    content.statement_sources as statement_sources,\n",
    "    content.file_source as file_source,\n",
    "    --classification.type as classification_type, \n",
    "    --classification.explanation as classification_explanation, \n",
    "    --classification.confidence as classification_confidence\n",
    "    MAX(classification.type) as classification_type, \n",
    "    MAX(classification.explanation) as classification_explanation, \n",
    "    MAX(classification.confidence) as classification_confidence,\n",
    "    MAX(file_source) as file_source\n",
    "FROM (\n",
    "    SELECT\n",
    "        file_source,\n",
    "        content.doc_id,\n",
    "        content.statement_id,\n",
    "        content.statement_sources,\n",
    "        content.statement_title,\n",
    "        unnest(content.classification) as classification\n",
    "    FROM\n",
    "        RAW_CLASSIFY_P1_OPERATIVE_RULES\n",
    ") AS content\n",
    "GROUP BY doc_id, statement_id, statement_title, statement_sources, file_source\n",
    "ORDER BY file_source, doc_id, statement_id\n",
    "\"\"\"\n",
    "\n",
    "pred_classify_p1_operative_rules = conn.sql(query).fetchall()\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best classification of all checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH ExpandedClassifications AS (\n",
    "    SELECT\n",
    "        content.doc_id AS id,\n",
    "        content.statement_id,\n",
    "        content.statement_title,\n",
    "        content.statement_text,\n",
    "        content.statement_sources,\n",
    "        classification_item.value.type AS type,\n",
    "        classification_item.value.confidence AS confidence,\n",
    "        classification_item.value.explanation AS explanation,\n",
    "        content.file_source as file_source,\n",
    "        ROW_NUMBER() OVER (PARTITION BY content.doc_id, content.statement_id ORDER BY classification_item.value.confidence DESC) AS rn\n",
    "    FROM\n",
    "        RAW_CLASSIFY_P1_OPERATIVE_RULES AS content,\n",
    "        UNNEST(content.classification) AS classification_item(value)\n",
    ")\n",
    "SELECT\n",
    "    id,\n",
    "    statement_id,\n",
    "    statement_title,\n",
    "    statement_text,\n",
    "    statement_sources,\n",
    "    file_source,\n",
    "    type AS classification_type,\n",
    "    confidence AS classification_confidence,\n",
    "    explanation AS classification_explanation\n",
    "FROM\n",
    "    ExpandedClassifications\n",
    "WHERE\n",
    "    rn = 1\n",
    "ORDER BY statement_text;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join with terms P1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify - P2 Operative Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_table_from_json(\n",
    "    conn,\n",
    "    key_pattern1=\"classify_P2_Operative_rules|true_table\",\n",
    "    # suffix_key_pattern=\"classify_P2_Operative_rules|true_table\",\n",
    "    # prefix_key_pattern=\"\",\n",
    "    table_name=\"RAW_CLASSIFY_P2_OPERATIVE_RULES_TRUE\",\n",
    "    source=f\"{config['DEFAULT_DATA_DIR']}/documents_true_table.json\",\n",
    "    key_value=\"classify_P2_Operative_rules\",\n",
    "    drop=True,\n",
    "    content_key=\"content\",\n",
    "    alias=\"content\",\n",
    "    doc_id_key=\"id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM RAW_CLASSIFY_P2_OPERATIVE_RULES_TRUE;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join extract P1 with Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "#directory = Path(\"cfr2sbvr_db/evaluation\")\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_classification\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"classify_P2_Operative_rules|llm_response_classification\",\n",
    "        # suffix_key_pattern=\"classify_P2_Operative_rules|llm_response_classification\",\n",
    "        # prefix_key_pattern=\"\",\n",
    "        table_name=\"RAW_CLASSIFY_P2_OPERATIVE_RULES\",\n",
    "        source=file_path,\n",
    "        key_value=\"classify_P2_Operative_rules\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distinct pred rules match true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT id, prompt, content\n",
    "FROM RAW_CLASSIFY_P2_OPERATIVE_RULES_TRUE;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify P2 - Definitional Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_table_from_json(\n",
    "    conn,\n",
    "    key_pattern1=\"classify_P2_Definitional_terms|true_table\",\n",
    "    # suffix_key_pattern=\"classify_P2_Definitional_terms|true_table\",\n",
    "    # prefix_key_pattern=\"\",\n",
    "    table_name=\"RAW_CLASSIFY_P2_DEFINITIONAL_TERMS_TRUE\",\n",
    "    source=f\"{config['DEFAULT_DATA_DIR']}/documents_true_table.json\",\n",
    "    key_value=\"classify_P2_Definitional_terms\",\n",
    "    drop=True,\n",
    "    content_key=\"content\",\n",
    "    alias=\"content\",\n",
    "    doc_id_key=\"id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_CLASSIFY_P2_DEFINITIONAL_TERMS_TRUE\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_classification\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"classify_P2_Definitional_terms|llm_response_classification\",\n",
    "        # suffix_key_pattern=\"classify_P2_Definitional_terms|llm_response_classification\",\n",
    "        # prefix_key_pattern=\"\",\n",
    "        table_name=\"RAW_CLASSIFY_P2_DEFINITIONAL_TERMS\",\n",
    "        source=file_path,\n",
    "        key_value=\"classify_P2_Definitional_terms\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_CLASSIFY_P2_DEFINITIONAL_TERMS\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify P2 - Definitional Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_table_from_json(\n",
    "    conn,\n",
    "    key_pattern1=\"classify_P2_Definitional_names|true_table\",\n",
    "    # suffix_key_pattern=\"classify_P2_Definitional_names|true_table\",\n",
    "    # prefix_key_pattern=\"\",\n",
    "    table_name=\"RAW_CLASSIFY_P2_DEFINITIONAL_NAMES_TRUE\",\n",
    "    source=f\"{config['DEFAULT_DATA_DIR']}/documents_true_table.json\",\n",
    "    key_value=\"classify_P2_Definitional_names\",\n",
    "    drop=True,\n",
    "    content_key=\"content\",\n",
    "    alias=\"content\",\n",
    "    doc_id_key=\"id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_CLASSIFY_P2_DEFINITIONAL_NAMES_TRUE\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_classification\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"classify_P2_Definitional_names|llm_response_classification\",\n",
    "        # suffix_key_pattern=\"classify_P2_Definitional_names|llm_response_classification\",\n",
    "        # prefix_key_pattern=\"\",\n",
    "        table_name=\"RAW_CLASSIFY_P2_DEFINITIONAL_NAMES\",\n",
    "        source=file_path,\n",
    "        key_value=\"classify_P2_Definitional_names\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_CLASSIFY_P2_DEFINITIONAL_NAMES\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify P2 - Definitional Facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_table_from_json(\n",
    "    conn,\n",
    "    key_pattern1=\"classify_P2_Definitional_facts|true_table\",\n",
    "    # suffix_key_pattern=\"classify_P2_Definitional_facts|true_table\",\n",
    "    # prefix_key_pattern=\"\",\n",
    "    table_name=\"RAW_CLASSIFY_P2_DEFINITIONAL_FACTS_TRUE\",\n",
    "    source=f\"{config['DEFAULT_DATA_DIR']}/documents_true_table.json\",\n",
    "    key_value=\"classify_P2_Definitional_facts\",\n",
    "    drop=True,\n",
    "    content_key=\"content\",\n",
    "    alias=\"content\",\n",
    "    doc_id_key=\"id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_CLASSIFY_P2_DEFINITIONAL_FACTS_TRUE\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_classification\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"classify_P2_Definitional_facts|llm_response_classification\",\n",
    "        # suffix_key_pattern=\"classify_P2_Definitional_facts|llm_response_classification\",\n",
    "        # prefix_key_pattern=\"\",\n",
    "        table_name=\"RAW_CLASSIFY_P2_DEFINITIONAL_FACTS\",\n",
    "        source=file_path,\n",
    "        key_value=\"classify_P2_Definitional_facts\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_CLASSIFY_P2_DEFINITIONAL_FACTS\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *,\n",
    "FROM CHECKPOINT_METADATA\n",
    "WHERE PROCESS='transformation'\n",
    "ORDER BY DOC_ID, DOC_TYPE DESC;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operative Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_transform\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"transform_Operative_Rules|llm_response_transform\",\n",
    "        # suffix_key_pattern=\"transform_Operative_Rules|llm_response_transform\",\n",
    "        # prefix_key_pattern=\"\",\n",
    "        table_name=\"RAW_TRANSFORM_OPERATIVE_RULES\",\n",
    "        source=file_path,\n",
    "        key_value=\"transform_Operative_Rules\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_TRANSFORM_OPERATIVE_RULES\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_transform\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"transform_Terms|llm_response_transform\",\n",
    "        # suffix_key_pattern=\"transform_Terms|llm_response_transform\",\n",
    "        # prefix_key_pattern=\"\",\n",
    "        table_name=\"RAW_TRANSFORM_TERMS\",\n",
    "        source=file_path,\n",
    "        key_value=\"transform_Terms\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_TRANSFORM_TERMS\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_transform\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"transform_Names|llm_response_transform\",\n",
    "        # suffix_key_pattern=\"transform_Names|llm_response_transform\",\n",
    "        # prefix_key_pattern=\"\",\n",
    "        table_name=\"RAW_TRANSFORM_NAMES\",\n",
    "        source=file_path,\n",
    "        key_value=\"transform_Names\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_TRANSFORM_NAMES\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fact types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_transform\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"transform_Fact_Types|llm_response_transform\",\n",
    "        # suffix_key_pattern=\"transform_Fact_Types|llm_response_transform\",\n",
    "        # prefix_key_pattern=\"\",\n",
    "        table_name=\"RAW_TRANSFORM_FACT_TYPES\",\n",
    "        source=file_path,\n",
    "        key_value=\"transform_Fact_Types\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_TRANSFORM_FACT_TYPES\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_evaluation\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"|llm_validation\",\n",
    "        # suffix_key_pattern=\"|llm_\",\n",
    "        # prefix_key_pattern=\"\",\n",
    "        table_name=\"RAW_LLM_VALIDATION\",\n",
    "        source=file_path,\n",
    "        key_value=\"llm_validation\",\n",
    "        drop=drop,\n",
    "        content_key=\"content\",\n",
    "        alias=\"content\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "*\n",
    "FROM RAW_LLM_VALIDATION\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id,\n",
    "    REPLACE(id, 'validation_judge_', '') as element_type,\n",
    "    file_source,\n",
    "    content.doc_id,\n",
    "    content.statement_id,\n",
    "    content.statement,\n",
    "    content.sources,\n",
    "    content.semscore,\n",
    "    content.similarity_score,\n",
    "    content.similarity_score_confidence,\n",
    "    content.transformation_accuracy,\n",
    "    content.grammar_syntax_accuracy,\n",
    "    content.findings,\n",
    "    created_at  \n",
    "FROM RAW_LLM_VALIDATION;\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Scores per id, file_source, and doc_id\n",
    "\n",
    " - semscore\n",
    " - similarity_score\n",
    " - similarity_score_confidence\n",
    " - transformation_accuracy\n",
    " - grammar_syntax_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    id,\n",
    "    REPLACE(id, 'validation_judge_', '') as element_type,\n",
    "    file_source,\n",
    "    content.doc_id,\n",
    "    count(content.doc_id) as count_doc_id,\n",
    "    avg(content.semscore) as avg_semscore,\n",
    "    avg(content.similarity_score) as avg_similarity_score,\n",
    "    avg(content.similarity_score_confidence) as avg_similarity_score_confidence,\n",
    "    avg(content.transformation_accuracy) as avg_transformation_accuracy,\n",
    "    avg(content.grammar_syntax_accuracy) as avg_grammar_syntax_accuracy,\n",
    "    max(created_at) as last_created_at\n",
    "FROM RAW_LLM_VALIDATION\n",
    "group by id, file_source, content.doc_id\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Scores per element_type\n",
    "\n",
    " - semscore\n",
    " - similarity_score\n",
    " - similarity_score_confidence\n",
    " - transformation_accuracy\n",
    " - grammar_syntax_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    REPLACE(id, 'validation_judge_', '') as element_type,\n",
    "    count(content.doc_id) as count_doc_id,\n",
    "    max(created_at) as last_created_at,\n",
    "    -- avg\n",
    "    round(avg(content.semscore), 3) as avg_semscore,\n",
    "    round(avg(content.similarity_score), 3) as avg_similarity_score,\n",
    "    round(avg(content.similarity_score_confidence), 3) as avg_similarity_score_confidence,\n",
    "    round(avg(content.transformation_accuracy), 3) as avg_transformation_accuracy,\n",
    "    round(avg(content.grammar_syntax_accuracy), 3) as avg_grammar_syntax_accuracy,\n",
    "    -- min\n",
    "    round(min(content.semscore), 3) as avg_semscore,\n",
    "    round(min(content.similarity_score), 3) as avg_similarity_score,\n",
    "    round(min(content.similarity_score_confidence), 3) as min_similarity_score_confidence,\n",
    "    round(min(content.transformation_accuracy), 3) as min_transformation_accuracy,\n",
    "    round(min(content.grammar_syntax_accuracy), 3) as min_grammar_syntax_accuracy,\n",
    "    -- max\n",
    "    round(max(content.semscore), 3) as avg_semscore,\n",
    "    round(max(content.similarity_score), 3) as avg_similarity_score,\n",
    "    round(max(content.similarity_score_confidence), 3) as max_similarity_score_confidence,\n",
    "    round(max(content.transformation_accuracy), 3) as max_transformation_accuracy,\n",
    "    round(max(content.grammar_syntax_accuracy), 3) as max_grammar_syntax_accuracy\n",
    "FROM RAW_LLM_VALIDATION\n",
    "GROUP BY \n",
    "    REPLACE(id, 'validation_judge_', '')\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Scores of all runs\n",
    "\n",
    " - semscore\n",
    " - similarity_score\n",
    " - similarity_score_confidence\n",
    " - transformation_accuracy\n",
    " - grammar_syntax_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    count(content.doc_id) as count_doc_id,\n",
    "    max(created_at) as last_created_at,\n",
    "    -- avg\n",
    "    round(avg(content.semscore), 3) as avg_semscore,\n",
    "    round(avg(content.similarity_score), 3) as avg_similarity_score,\n",
    "    round(avg(content.similarity_score_confidence), 3) as avg_similarity_score_confidence,\n",
    "    round(avg(content.transformation_accuracy), 3) as avg_transformation_accuracy,\n",
    "    round(avg(content.grammar_syntax_accuracy), 3) as avg_grammar_syntax_accuracy,\n",
    "    -- min\n",
    "    round(min(content.semscore), 3) as avg_semscore,\n",
    "    round(min(content.similarity_score), 3) as avg_similarity_score,\n",
    "    round(min(content.similarity_score_confidence), 3) as min_similarity_score_confidence,\n",
    "    round(min(content.transformation_accuracy), 3) as min_transformation_accuracy,\n",
    "    round(min(content.grammar_syntax_accuracy), 3) as min_grammar_syntax_accuracy,\n",
    "    -- max\n",
    "    round(max(content.semscore), 3) as avg_semscore,\n",
    "    round(max(content.similarity_score), 3) as avg_similarity_score,\n",
    "    round(max(content.similarity_score_confidence), 3) as max_similarity_score_confidence,\n",
    "    round(max(content.transformation_accuracy), 3) as max_transformation_accuracy,\n",
    "    round(max(content.grammar_syntax_accuracy), 3) as max_grammar_syntax_accuracy\n",
    "FROM RAW_LLM_VALIDATION\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_evaluation\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"|llm_\",\n",
    "        # suffix_key_pattern=\"|llm_\",\n",
    "        # prefix_key_pattern=\"\",\n",
    "        table_name=\"RAW_ELAPSED_TIME\",\n",
    "        source=file_path,\n",
    "        key_value=\"llm_response\",\n",
    "        drop=drop,\n",
    "        content_key=\"elapsed_times\",\n",
    "        alias=\"elapsed_times\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM RAW_ELAPSED_TIME\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed time by checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT file_source as checkpoint, count(elapsed_times) as count_elapsed_time, sum(elapsed_times) as total_elapsed_time, avg(elapsed_times) as avg_elapsed_time\n",
    "FROM RAW_ELAPSED_TIME\n",
    "GROUP BY id, file_source\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "et_file = conn.sql(query).fetchdf()\n",
    "\n",
    "et_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_file,\n",
    "    group_col=\"checkpoint\",\n",
    "    value_col=\"total_elapsed_time\",\n",
    "    title=\"Soma do Tempo Total Decorrido por Checkpoint\",\n",
    "    x_label=\"Tempo Total Decorrido\",\n",
    "    y_label=\"Checkpoint\",\n",
    "    big_numbers={\"factor\": 1, \"suffix\": \"s\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed time by doc_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id as doc_type, count(elapsed_times) as count_elapsed_time, sum(elapsed_times) as total_elapsed_time, avg(elapsed_times) as avg_elapsed_time\n",
    "FROM RAW_ELAPSED_TIME\n",
    "GROUP BY id\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "et_doc_type = conn.sql(query).fetchdf()\n",
    "\n",
    "et_doc_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_doc_type,\n",
    "    group_col=\"doc_type\",\n",
    "    value_col=\"total_elapsed_time\",\n",
    "    title=\"Soma do tempo total decorrido por tipo documento\",\n",
    "    x_label=\"Tempo total decorrido\",\n",
    "    y_label=\"doc_type\",\n",
    "    big_numbers={\"factor\": 1, \"suffix\": \"s\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed time by process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH CHECKPOINT AS (\n",
    "    SELECT \n",
    "        PROCESS, \n",
    "        DOC_ID, \n",
    "        DOC_TYPE,\n",
    "        DOC_SOURCE\n",
    "    FROM \n",
    "        CHECKPOINT_METADATA\n",
    "    GROUP BY \n",
    "        PROCESS, DOC_ID, DOC_TYPE, DOC_SOURCE\n",
    ")\n",
    "SELECT\n",
    "    cm.process,\n",
    "    cm.doc_id,\n",
    "    cm.doc_type,\n",
    "    cm.doc_source,\n",
    "    COUNT(et.elapsed_times) AS count_elapsed_time,\n",
    "    SUM(et.elapsed_times) AS total_elapsed_time,\n",
    "    AVG(et.elapsed_times) AS avg_elapsed_time\n",
    "FROM\n",
    "    RAW_ELAPSED_TIME AS et\n",
    "JOIN\n",
    "    CHECKPOINT AS cm ON et.id = cm.doc_id\n",
    "WHERE\n",
    "    cm.doc_source in ('pred', 'val')\n",
    "GROUP BY\n",
    "    cm.process, cm.doc_id, cm.doc_type, cm.doc_source\n",
    "ORDER BY\n",
    "    cm.process, cm.doc_id, cm.doc_type, cm.doc_source;\n",
    "\"\"\"\n",
    "\n",
    "et_process = conn.sql(query).fetchdf()\n",
    "\n",
    "et_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_process,\n",
    "    group_col=\"process\",\n",
    "    value_col=\"total_elapsed_time\",\n",
    "    title=\"Soma do tempo total decorrido por processo\",\n",
    "    x_label=\"Tempo total decorrido\",\n",
    "    y_label=\"processo\",\n",
    "    big_numbers={\"factor\": 1, \"suffix\": \"s\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_process.groupby(\"process\")[\"total_elapsed_time\"].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average elapsed time by process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_process,\n",
    "    group_col=\"process\",\n",
    "    value_col=\"avg_elapsed_time\",\n",
    "    title=\"Tempo médio decorrido por processo\",\n",
    "    x_label=\"Tempo total decorrido\",\n",
    "    y_label=\"processo\",\n",
    "    big_numbers={\"factor\": 1, \"suffix\": \"s\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed time all runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT count(elapsed_times) as count_elapsed_time, sum(elapsed_times) as total_elapsed_time, avg(elapsed_times) as avg_elapsed_time\n",
    "FROM RAW_ELAPSED_TIME\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "elapsed_time_big_numbers = conn.sql(query).fetchdf()\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"count_elapsed_time: {humanize.intword(elapsed_time_big_numbers['count_elapsed_time'])}\")\n",
    "print(f\"total_elapsed_time: {humanize.intword(elapsed_time_big_numbers['total_elapsed_time'])}\")\n",
    "print(f\"avg_elapsed_time: {humanize.intword(elapsed_time_big_numbers['avg_elapsed_time'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and pattern\n",
    "directory = Path(f\"{config['DEFAULT_DATA_DIR']}/checkpoints_evaluation\")\n",
    "pattern = \"documents-*.json\"\n",
    "\n",
    "# Use glob to find all files matching the pattern\n",
    "files = directory.glob(pattern)\n",
    "\n",
    "drop = True  # Drop the table first time\n",
    "for file_path in files:\n",
    "    print(file_path)  # Output each file path\n",
    "    upsert_table_from_json(\n",
    "        conn,\n",
    "        key_pattern1=\"|llm_\",\n",
    "        table_name=\"RAW_LLM_COMPLETION\",\n",
    "        source=file_path,\n",
    "        key_value=\"llm_response\",\n",
    "        drop=drop,\n",
    "        content_key=\"completions\",\n",
    "        alias=\"completions\",\n",
    "        doc_id_key=\"id\",\n",
    "    )\n",
    "    drop = False  # Stop dropping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DESCRIBE RAW_LLM_COMPLETION\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    id, \n",
    "    file_source, \n",
    "    created_at, \n",
    "    completions.id,\n",
    "    completions.model,\n",
    "    completions.object,\n",
    "    --completions.service_tier,\n",
    "    completions.system_fingerprint,\n",
    "    completions.usage.completion_tokens,\n",
    "    completions.usage.prompt_tokens,\n",
    "    completions.usage.total_tokens\n",
    "    --completions.usage.completion_tokens_details,\n",
    "    --completions.usage.prompt_tokens_details\n",
    "FROM RAW_LLM_COMPLETION\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH CHECKPOINT AS (\n",
    "    SELECT \n",
    "        PROCESS, \n",
    "        DOC_ID, \n",
    "        DOC_TYPE,\n",
    "        DOC_SOURCE\n",
    "    FROM \n",
    "        CHECKPOINT_METADATA\n",
    "    GROUP BY \n",
    "        PROCESS, DOC_ID, DOC_TYPE, DOC_SOURCE\n",
    ")\n",
    "SELECT \n",
    "    id as doc_id,\n",
    "    cm.process,\n",
    "    cm.doc_source as doc_source,\n",
    "    file_source as checkpoint, \n",
    "    created_at, \n",
    "    completions.id as completion_id,\n",
    "    completions.model,\n",
    "    completions.object,\n",
    "    --completions.service_tier,\n",
    "    completions.system_fingerprint,\n",
    "    completions.usage.completion_tokens,\n",
    "    completions.usage.prompt_tokens,\n",
    "    completions.usage.total_tokens\n",
    "    --completions.usage.completion_tokens_details,\n",
    "    --completions.usage.prompt_tokens_details\n",
    "FROM RAW_LLM_COMPLETION AS lc\n",
    "JOIN\n",
    "    CHECKPOINT AS cm ON lc.id = cm.doc_id\n",
    "WHERE\n",
    "    cm.doc_source in ('pred', 'val')\n",
    "--GROUP BY\n",
    "--    cm.process, cm.doc_id, cm.doc_type, cm.doc_source\n",
    "--ORDER BY\n",
    "--    cm.process, cm.doc_id, cm.doc_type, cm.doc_source;\n",
    "\"\"\"\n",
    "\n",
    "lc_process = conn.sql(query).fetchdf()\n",
    "\n",
    "lc_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens per process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH CHECKPOINT AS (\n",
    "    SELECT \n",
    "        PROCESS, \n",
    "        DOC_ID, \n",
    "        DOC_TYPE,\n",
    "        DOC_SOURCE\n",
    "    FROM \n",
    "        CHECKPOINT_METADATA\n",
    "    GROUP BY \n",
    "        PROCESS, DOC_ID, DOC_TYPE, DOC_SOURCE\n",
    ")\n",
    "SELECT \n",
    "    cm.process,\n",
    "    cm.doc_id,\n",
    "    cm.doc_type,\n",
    "    cm.doc_source,\n",
    "    lc.file_source as checkpoint,\n",
    "    COUNT(lc.id) AS count_completions,\n",
    "    SUM(lc.completions.usage.completion_tokens) AS total_completion_tokens,\n",
    "    AVG(lc.completions.usage.completion_tokens) AS avg_completion_tokens,\n",
    "\n",
    "    SUM(lc.completions.usage.prompt_tokens) AS total_prompt_tokens,\n",
    "    AVG(lc.completions.usage.prompt_tokens) AS avg_prompt_tokens,\n",
    "\n",
    "    SUM(lc.completions.usage.total_tokens) AS total_total_tokens,\n",
    "    AVG(lc.completions.usage.total_tokens) AS avg_total_tokens,\n",
    "    \n",
    "    MAX(lc.created_at) AS last_completion\n",
    "FROM RAW_LLM_COMPLETION AS lc\n",
    "JOIN\n",
    "    CHECKPOINT AS cm ON lc.id = cm.doc_id\n",
    "WHERE\n",
    "    cm.doc_source in ('pred', 'val')\n",
    "GROUP BY\n",
    "    cm.process, cm.doc_id, cm.doc_type, cm.doc_source, lc.file_source\n",
    "ORDER BY\n",
    "    cm.process, cm.doc_id, cm.doc_type, cm.doc_source, lc.file_source;\n",
    "\"\"\"\n",
    "\n",
    "et_llm_completion = conn.sql(query).fetchdf()\n",
    "\n",
    "et_llm_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_llm_completion,\n",
    "    group_col=\"process\",\n",
    "    value_col=\"total_prompt_tokens\",\n",
    "    title=\"Tempo médio decorrido por processo\",\n",
    "    x_label=\"Qde tokens\",\n",
    "    y_label=\"processo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avarege tokens per process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_llm_completion,\n",
    "    group_col=\"process\",\n",
    "    value_col=\"avg_total_tokens\",\n",
    "    title=\"Qde tokens médio por processo\",\n",
    "    x_label=\"Qde tokens médio\",\n",
    "    y_label=\"processo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens per document type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_llm_completion,\n",
    "    group_col=\"doc_id\",\n",
    "    value_col=\"total_prompt_tokens\",\n",
    "    title=\"Qde tokens por tipo documento\",\n",
    "    x_label=\"Qde tokens\",\n",
    "    y_label=\"doc_id\",\n",
    "    figsize=(12, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average tokens per checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_llm_completion,\n",
    "    group_col=\"doc_id\",\n",
    "    value_col=\"avg_prompt_tokens\",\n",
    "    title=\"Qde tokens por tipo documento\",\n",
    "    x_label=\"Qde tokens\",\n",
    "    y_label=\"doc_id\",\n",
    "    figsize=(12, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens per checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_llm_completion,\n",
    "    group_col=\"checkpoint\",\n",
    "    value_col=\"total_prompt_tokens\",\n",
    "    title=\"Qde tokens por checkpoint\",\n",
    "    x_label=\"Qde tokens\",\n",
    "    y_label=\"checkpoint\",\n",
    "    figsize=(12, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_bar_chart(\n",
    "    et_llm_completion,\n",
    "    group_col=\"checkpoint\",\n",
    "    value_col=\"avg_total_tokens\",\n",
    "    title=\"Tokens médio por checkpoint\",\n",
    "    x_label=\"Qde tokens\",\n",
    "    y_label=\"checkpoint\",\n",
    "    figsize=(12, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH CHECKPOINT AS (\n",
    "    SELECT \n",
    "        PROCESS, \n",
    "        DOC_ID, \n",
    "        DOC_TYPE,\n",
    "        DOC_SOURCE\n",
    "    FROM \n",
    "        CHECKPOINT_METADATA\n",
    "    GROUP BY \n",
    "        PROCESS, DOC_ID, DOC_TYPE, DOC_SOURCE\n",
    ")\n",
    "SELECT\n",
    "    cm.process,\n",
    "    COUNT(lc.id) AS count_completions,\n",
    "    SUM(lc.completions.usage.completion_tokens) AS total_completion_tokens,\n",
    "    AVG(lc.completions.usage.completion_tokens) AS avg_completion_tokens,\n",
    "\n",
    "    SUM(lc.completions.usage.prompt_tokens) AS total_prompt_tokens,\n",
    "    AVG(lc.completions.usage.prompt_tokens) AS avg_prompt_tokens,\n",
    "\n",
    "    SUM(lc.completions.usage.total_tokens) AS total_total_tokens,\n",
    "    AVG(lc.completions.usage.total_tokens) AS avg_total_tokens,\n",
    "    \n",
    "    MAX(lc.created_at) AS last_completion\n",
    "FROM RAW_LLM_COMPLETION AS lc\n",
    "JOIN\n",
    "    CHECKPOINT AS cm ON lc.id = cm.doc_id\n",
    "WHERE\n",
    "    cm.doc_source in ('pred', 'val')\n",
    "GROUP BY\n",
    "    cm.process\n",
    "\"\"\"\n",
    "\n",
    "token_big_numbers = conn.sql(query).fetchdf()\n",
    "\n",
    "conn.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total_prompt_tokens: {humanize.intword(token_big_numbers['total_prompt_tokens'])}\")\n",
    "print(f\"total_completion_tokens: {humanize.intword(token_big_numbers['total_completion_tokens'])}\")\n",
    "print(f\"total_total_tokens: {humanize.intword(token_big_numbers['total_total_tokens'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create views\n",
    "\n",
    "The main use of the views is exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failsafe to avoid running the code in the wrong database if run all cells.\n",
    "if LOCAL_DB:\n",
    "    DB = DATABASE\n",
    "else:\n",
    "    DB = CLOUD_DATABASE\n",
    "user_input = input(f\"The code will create the database objects in the database '{DB}'. To continue type the database name: \")\n",
    "if user_input != DB:\n",
    "    raise Exception(f\"The database name '{user_input}' is different from current database '{DB}'. The code will not continue.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "\n",
    "# Check the version of the database objects\n",
    "DB_VERSION = \"v5\"\n",
    "\n",
    "if CLEAN_UP:\n",
    "    directory = Path(f\"{config['DEFAULT_DATA_DIR']}/cfr2sbvr_db/db_objects_{DB_VERSION}\")\n",
    "    pattern = \"*_VW.sql\"\n",
    "\n",
    "    # Use glob to find all files matching the pattern\n",
    "    files = directory.glob(pattern)\n",
    "\n",
    "    files_processed = 0\n",
    "    for file_path in natsorted(files):\n",
    "        logger.info(f\"Processing file {file_path}\")\n",
    "        files_processed += 1\n",
    "        with open(file_path, \"r\") as file:\n",
    "            ddl_query = file.read()\n",
    "            logger.debug(ddl_query)\n",
    "            conn.sql(ddl_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM duckdb_views\n",
    "  WHERE view_name LIKE '%_VW';\n",
    "\"\"\"\n",
    "\n",
    "views_created = len(conn.sql(query).fetchall())\n",
    "assert views_created == files_processed, f\"Number of views created {views_created} is different from expected {files_processed}\"\n",
    "\n",
    "conn.sql(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipt-cfr2sbvr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
