{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFR2SBVR Modules\n",
    "\n",
    "Supporting modules for the chapters 6, 7 of the dissertation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir configuration && touch configuration/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile configuration/main.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import glob\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "DEFAULT_CONFIG_DIR: str = '../config.yaml'  # Google drive: \"/content/drive/MyDrive/cfr2sbvr/config.yaml\"\n",
    "\n",
    "def _get_sorted_file_info(file_dir: str, file_prefix: str, extension: str):\n",
    "    \"\"\"\n",
    "    Helper function to retrieve and sort file information based on a specific prefix and extension.\n",
    "\n",
    "    Args:\n",
    "        file_dir (str): Directory to search for files.\n",
    "        file_prefix (str): Prefix for the filenames.\n",
    "        extension (str): File extension.\n",
    "\n",
    "    Returns:\n",
    "        list: Sorted list of file information dictionaries containing 'filename', 'date', and 'number' keys.\n",
    "    \"\"\"\n",
    "    path = Path(file_dir)\n",
<<<<<<< HEAD
    "    path.mkdir(parents=True, exist_ok=True)\n",
=======
    "    #path.mkdir(parents=True, exist_ok=True)\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "\n",
    "    files = list(path.glob(f\"{file_prefix}-*.{extension}\"))\n",
    "    file_info_list = []\n",
    "\n",
    "    pattern = re.compile(rf'^{file_prefix}-(\\d{{4}}-\\d{{2}}-\\d{{2}})-(\\d+)\\.{extension}$')\n",
    "    for filepath in files:\n",
    "        match = pattern.match(filepath.name)\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            number = int(match.group(2))\n",
    "            file_info_list.append({'filename': filepath.name, 'date': date_str, 'number': number})\n",
    "\n",
    "    return sorted(file_info_list, key=lambda x: (x['date'], x['number']), reverse=True)\n",
    "\n",
    "def get_next_filename(file_dir: str, file_prefix: str, extension: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates the next filename in a sequence based on existing files in a directory,\n",
    "    considering the file extension.\n",
    "\n",
    "    The filename format is: `{file_prefix}-{YYYY-MM-DD}-{N}.{extension}`,\n",
    "    where `N` is an incrementing integer for files with the same date.\n",
    "    \"\"\"\n",
    "    today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "    sorted_files = _get_sorted_file_info(file_dir, file_prefix, extension)\n",
    "\n",
    "    if sorted_files and sorted_files[0]['date'] == today_str:\n",
    "        new_number = sorted_files[0]['number'] + 1\n",
    "    else:\n",
    "        new_number = 1\n",
    "\n",
    "    new_filename = f'{file_prefix}-{today_str}-{new_number}.{extension}'\n",
    "    return str(Path(file_dir) / new_filename)\n",
    "\n",
    "def get_last_filename(file_dir: str, file_prefix: str, extension: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves the most recent filename based on the highest date and sequence number\n",
    "    for files with a specific prefix and extension in the specified directory.\n",
    "    \"\"\"\n",
    "    sorted_files = _get_sorted_file_info(file_dir, file_prefix, extension)\n",
    "    if sorted_files:\n",
    "        return str(Path(file_dir) / sorted_files[0]['filename'])\n",
    "    return None\n",
    "\n",
    "# Load the YAML config file\n",
    "def load_config(config_file: str = None):\n",
    "    if config_file is None:\n",
    "        config_file = DEFAULT_CONFIG_DIR\n",
    "    try:\n",
    "        with open(config_file, \"r\") as file:\n",
    "            config = yaml.safe_load(file)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Configuration file {config_file} not found.\")\n",
    "    except yaml.YAMLError as exc:\n",
    "        raise ValueError(f\"Error parsing YAML file {config_file}: {exc}\")\n",
    "\n",
    "    # Ensure config structure is correct\n",
    "    if \"LLM\" not in config or \"DEFAULT_CHECKPOINT_DIR\" not in config:\n",
    "        raise ValueError(\"Required configuration keys are missing in the config file.\")\n",
    "\n",
    "    # Set the OpenAI API key from environment variable if it's not set in config\n",
    "    config[\"LLM\"][\"OPENAI_API_KEY\"] = os.getenv(\n",
    "        \"OPENAI_API_KEY\", config[\"LLM\"].get(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "\n",
    "    # Dynamically set checkpoint and report files using the get_next_filename function\n",
    "    config[\"DEFAULT_CHECKPOINT_FILE\"] = get_next_filename(\n",
    "        config[\"DEFAULT_CHECKPOINT_DIR\"], \"documents\", \"json\"\n",
    "    )\n",
    "    config[\"DEFAULT_EXTRACTION_REPORT_FILE\"] = get_next_filename(\n",
    "        config[\"DEFAULT_OUTPUT_DIR\"], \"extraction_report\", \"html\"\n",
    "    )\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  # backup on Google Drive\n",
    "  !cp -r configuration /content/drive/MyDrive/cfr2sbvr/modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configuration.main as configuration\n",
    "\n",
    "# Development mode\n",
    "import importlib\n",
    "importlib.reload(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configuration.load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "Checkpoints are stored / retrieved at the directory `DEFAULT_CHECKPOINT_FILE` in the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir checkpoint && touch checkpoint/__init__.py"
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v5"
   ]
  },
  {
>>>>>>> 3ee09f8 (add chap 7 elements validation)
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile checkpoint/main.py\n",
    "\n",
    "from typing import List, Dict, Optional, Any, Tuple, Set\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "import json\n",
    "from json import JSONDecodeError\n",
<<<<<<< HEAD
=======
    "from pathlib import Path\n",
    "import re\n",
    "import unicodedata\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "\n",
    "# Set up basic logging configuration for the checkpoint module\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to INFO or another level as needed\n",
<<<<<<< HEAD
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Log format\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
=======
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Log format\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "def convert_set_to_list(data: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Recursively converts sets to lists in the data structure.\n",
    "\n",
    "    Args:\n",
    "        data (Any): The data structure to process, which can be a dict, list, set, or other types.\n",
    "\n",
    "    Returns:\n",
    "        Any: The data structure with all sets converted to lists.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {key: convert_set_to_list(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_set_to_list(item) for item in data]\n",
    "    elif isinstance(data, set):\n",
    "        return list(data)\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n",
<<<<<<< HEAD
=======
    "def normalize_str(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a string using Unicode normalization to ensure consistent representation.\n",
    "\n",
    "    Args:\n",
    "        s (str): The string to normalize.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized string.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize(\"NFKD\", s).strip()\n",
    "\n",
    "\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "# Define a model for the Document\n",
    "class Document(BaseModel):\n",
    "    id: str\n",
    "    type: str  # New field to represent the type of the document\n",
    "    content: Any  # Content can be any data type: list, dict, string, etc.\n",
    "\n",
<<<<<<< HEAD
    "# Define the DocumentManager class\n",
    "class DocumentManager(BaseModel):\n",
    "    documents: Dict[Tuple[str, str], Document] = Field(default_factory=dict)  # Keys are tuples (id, type)\n",
=======
    "\n",
    "# Define the DocumentManager class\n",
    "class DocumentManager(BaseModel):\n",
    "    documents: Dict[Tuple[str, str], Document] = Field(\n",
    "        default_factory=dict\n",
    "    )  # Keys are tuples (id, type)\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "\n",
    "    def add_document(self, doc: Document) -> None:\n",
    "        \"\"\"\n",
    "        Adds a document to the manager.\n",
    "\n",
    "        Args:\n",
    "            doc (Document): The document to add.\n",
    "        \"\"\"\n",
<<<<<<< HEAD
    "        key = (doc.id, doc.type)\n",
=======
    "        # Normalize the document ID and type to avoid inconsistencies\n",
    "        normalized_id = normalize_str(doc.id)\n",
    "        normalized_type = normalize_str(doc.type)\n",
    "\n",
    "        key = (normalized_id, normalized_type)\n",
    "\n",
    "        # Debug logging for added document keys\n",
    "        logger.debug(f\"Adding document with normalized key: {key}\")\n",
    "\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "        self.documents[key] = doc\n",
    "\n",
    "    def retrieve_document(self, doc_id: str, doc_type: str) -> Optional[Document]:\n",
    "        \"\"\"\n",
    "        Retrieves a document by its id and type.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): The ID of the document.\n",
    "            doc_type (str): The type of the document.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Document]: The retrieved document, or None if not found.\n",
    "        \"\"\"\n",
<<<<<<< HEAD
    "        key = (doc_id, doc_type)\n",
=======
    "        # Normalize the identifiers before using them to access the dictionary\n",
    "        normalized_id = normalize_str(doc_id)\n",
    "        normalized_type = normalize_str(doc_type)\n",
    "\n",
    "        key = (normalized_id, normalized_type)\n",
    "\n",
    "        # Debug logging for retrieval attempt\n",
    "        logger.debug(f\"Retrieving document with key: {key}\")\n",
    "\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "        return self.documents.get(key)\n",
    "\n",
    "    def list_document_ids(self, doc_type: Optional[str] = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Lists all document ids, optionally filtered by type.\n",
    "\n",
    "        Args:\n",
    "            doc_type (Optional[str], optional): The type of documents to list. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of document ids.\n",
    "        \"\"\"\n",
    "        if doc_type:\n",
<<<<<<< HEAD
    "            return [doc_id for (doc_id, d_type) in self.documents.keys() if d_type == doc_type]\n",
    "        else:\n",
=======
    "            normalized_type = normalize_str(doc_type)\n",
    "            logger.debug(f\"Listing documents filtered by type: {normalized_type}\")\n",
    "            return [\n",
    "                doc_id\n",
    "                for (doc_id, d_type) in self.documents.keys()\n",
    "                if d_type == normalized_type\n",
    "            ]\n",
    "        else:\n",
    "            logger.debug(\"Listing all documents without type filter.\")\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "            return [doc_id for (doc_id, _) in self.documents.keys()]\n",
    "\n",
    "    def exclude_document(self, doc_id: str, doc_type: str) -> None:\n",
    "        \"\"\"\n",
    "        Excludes a document by its id and type.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): The ID of the document to exclude.\n",
    "            doc_type (str): The type of the document.\n",
    "        \"\"\"\n",
<<<<<<< HEAD
    "        key = (doc_id, doc_type)\n",
    "        if key in self.documents:\n",
=======
    "        # Normalize identifiers before attempting exclusion\n",
    "        normalized_id = normalize_str(doc_id)\n",
    "        normalized_type = normalize_str(doc_type)\n",
    "\n",
    "        key = (normalized_id, normalized_type)\n",
    "\n",
    "        if key in self.documents:\n",
    "            logger.debug(f\"Excluding document with key: {key}\")\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "            del self.documents[key]\n",
    "\n",
    "    def persist_to_file(self, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Persists the current state to a file, converting tuple keys to strings and sets to lists.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The filename to save the documents.\n",
    "        \"\"\"\n",
<<<<<<< HEAD
    "        serializable_documents = {f\"{doc_id}|{doc_type}\": convert_set_to_list(doc.dict()) for (doc_id, doc_type), doc in self.documents.items()}\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(serializable_documents, file, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def restore_from_file(cls, filename: str) -> 'DocumentManager':\n",
=======
    "        serializable_documents = {\n",
    "            f\"{doc_id}|{doc_type}\": convert_set_to_list(doc.dict())\n",
    "            for (doc_id, doc_type), doc in self.documents.items()\n",
    "        }\n",
    "        with open(filename, \"w\") as file:\n",
    "            json.dump(serializable_documents, file, indent=4)\n",
    "        logger.info(f\"DocumentManager state persisted to file: {filename}\")\n",
    "\n",
    "    @classmethod\n",
    "    def restore_from_file(cls, filename: str) -> \"DocumentManager\":\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "        \"\"\"\n",
    "        Restores the state from a file, converting string keys back to tuples.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The filename to restore the documents from.\n",
    "\n",
    "        Returns:\n",
    "            DocumentManager: The restored DocumentManager instance.\n",
    "        \"\"\"\n",
<<<<<<< HEAD
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            documents = {(doc_id.split('|')[0], doc_id.split('|')[1]): Document(**doc_data) for doc_id, doc_data in data.items()}\n",
    "            return cls(documents=documents)\n",
    "\n",
=======
    "        with open(filename, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "            documents = {\n",
    "                (doc_id.split(\"|\")[0], doc_id.split(\"|\")[1]): Document(**doc_data)\n",
    "                for doc_id, doc_data in data.items()\n",
    "            }\n",
    "            logger.info(f\"DocumentManager restored from file: {filename}\")\n",
    "            return cls(documents=documents)\n",
    "\n",
    "\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "def restore_checkpoint(filename: Optional[str]) -> DocumentManager:\n",
    "    \"\"\"\n",
    "    Restores the document manager from a checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        filename (str, optional): The path to the checkpoint file. Defaults to DEFAULT_CHECKPOINT_FILE.\n",
    "\n",
    "    Returns:\n",
    "        DocumentManager: The restored DocumentManager instance.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the checkpoint file does not exist.\n",
    "\n",
    "    See Also:\n",
    "        - Reset the values delete the documents.json file and run: manager = DocumentManager()\n",
    "        - Restore the state from the documents.json file, run: DocumentManager.restore_from_file(\"documents.json\")\n",
    "        - Exclue a document: manager.exclude_document(doc_id=\"ยง 275.0-2\", doc_type=\"section\")\n",
    "        - List documents: manager.list_document_ids(doc_type=\"section\")\n",
    "        - Get a document: manager.retrieve_document(doc_id=doc, doc_type=\"section\")\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        restored_docs = DocumentManager.restore_from_file(filename)\n",
    "        logger.info(f\"Checkpoint restored from {filename}.\")\n",
    "    except (FileNotFoundError, JSONDecodeError):\n",
    "        restored_docs = DocumentManager()\n",
<<<<<<< HEAD
    "        logger.error(f\"Checkpoint file '{filename}' not found or is empty, initializing new checkpoint.\")\n",
    "    return restored_docs\n",
    "\n",
=======
    "        logger.error(\n",
    "            f\"Checkpoint file '{filename}' not found or is empty, initializing new checkpoint.\"\n",
    "        )\n",
    "    return restored_docs\n",
    "\n",
    "\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "def save_checkpoint(filename: Optional[str], manager: DocumentManager) -> None:\n",
    "    \"\"\"\n",
    "    Saves the current state of the DocumentManager to a checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        manager (DocumentManager): The DocumentManager instance to save.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error saving the checkpoint.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        manager.persist_to_file(filename=filename)\n",
    "        logger.info(\"Checkpoint saved.\")\n",
    "    except FileNotFoundError:\n",
<<<<<<< HEAD
    "        logger.error(\"Error saving checkpoint. Check the directory path and permissions.\")\n",
=======
    "        logger.error(\n",
    "            \"Error saving checkpoint. Check the directory path and permissions.\"\n",
    "        )\n",
    "\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "\n",
    "def get_all_checkpoints(checkpoint_dir, prefix=\"documents\", extension=\"json\"):\n",
    "    managers = []\n",
    "\n",
    "    path = Path(checkpoint_dir)\n",
    "\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
<<<<<<< HEAD
    "    files = list(path.glob(f\"{file_prefix}-*.{extension}\"))\n",
    "    file_info_list = []\n",
    "\n",
    "    pattern = re.compile(rf'^{file_prefix}-(\\d{{4}}-\\d{{2}}-\\d{{2}})-(\\d+)\\.{extension}$')\n",
=======
    "    files = list(path.glob(f\"{prefix}-*.{extension}\"))\n",
    "    file_info_list = []\n",
    "\n",
    "    pattern = re.compile(rf\"^{prefix}-(\\d{{4}}-\\d{{2}}-\\d{{2}})-(\\d+)\\.{extension}$\")\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "    for filepath in files:\n",
    "        match = pattern.match(filepath.name)\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            number = int(match.group(2))\n",
<<<<<<< HEAD
    "            file_info_list.append({'filename': filepath.name, 'date': date_str, 'number': number})\n",
    "            \n",
    "            print(filepath)\n",
    "            managers.append(manager.restore_from_file(filepath))\n",
    "    \n",
    "    return managers, file_info_list\n",
    "\n",
=======
    "            file_info_list.append(\n",
    "                {\"filename\": filepath.name, \"date\": date_str, \"number\": number}\n",
    "            )\n",
    "\n",
    "            logger.debug(f\"filepath: {filepath}\")\n",
    "            managers.append(DocumentManager.restore_from_file(filepath))\n",
    "\n",
    "    return managers, file_info_list\n",
    "\n",
    "\n",
    "def get_elements_from_checkpoints(checkpoint_dir):\n",
    "    managers, file_info_list = get_all_checkpoints(checkpoint_dir)\n",
    "\n",
    "    pred_operative_rules = []\n",
    "    pred_facts = []\n",
    "    pred_terms = []\n",
    "    pred_names = []\n",
    "    pred_files = []\n",
    "\n",
    "    for manager, file_info in zip(managers, file_info_list):\n",
    "        # Process documents\n",
    "        processor = DocumentProcessor(manager)\n",
    "\n",
    "        # Access processed data\n",
    "        # unique_terms = processor.get_unique_terms()\n",
    "        # unique_names = processor.get_unique_names()\n",
    "        pred_operative_rules += processor.get_rules()\n",
    "        pred_facts += processor.get_facts()\n",
    "        pred_terms += processor.get_terms(definition_filter=\"non_null\")\n",
    "        pred_names += processor.get_names(definition_filter=\"non_null\")\n",
    "        pred_files.append(file_info)\n",
    "\n",
    "    logger.debug(f\"Rules: {pred_operative_rules}\")\n",
    "    logger.debug(f\"Facts: {pred_facts}\")\n",
    "    logger.debug(f\"Terms: {pred_terms}\")\n",
    "    logger.debug(f\"Names: {pred_names}\")\n",
    "    logger.info(f\"Rules to evaluate: {len(pred_operative_rules)}\")\n",
    "    logger.info(f\"Facts to evaluate: {len(pred_facts)}\")\n",
    "    logger.info(f\"Terms to evaluate: {len(pred_terms)}\")\n",
    "    logger.info(f\"Names to evaluate: {len(pred_names)}\")\n",
    "\n",
    "    return pred_operative_rules, pred_facts, pred_terms, pred_names, pred_files\n",
    "\n",
    "\n",
    "def get_true_table_files(data_dir):\n",
    "    true_table_files = [\n",
    "        {\n",
    "            \"path\": f\"{data_dir}/classify_p1_operative_rules_true_table.json\",\n",
    "            \"id\": \"classify_P1|true_table\",\n",
    "        },\n",
    "        {\n",
    "            \"path\": f\"{data_dir}/classify_p2_definitional_facts_true_table.json\",\n",
    "            \"id\": \"classify_P2_Definitional_facts|true_table\",\n",
    "        },\n",
    "        {\n",
    "            \"path\": f\"{data_dir}/classify_p2_definitional_names_true_table.json\",\n",
    "            \"id\": \"classify_P2_Definitional_names|true_table\",\n",
    "        },\n",
    "        {\n",
    "            \"path\": f\"{data_dir}/classify_p2_definitional_terms_true_table.json\",\n",
    "            \"id\": \"classify_P2_Definitional_terms|true_table\",\n",
    "        },\n",
    "        {\n",
    "            \"path\": f\"{data_dir}/classify_p2_operative_rules_true_table.json\",\n",
    "            \"id\": \"classify_P2_Operative_rules|true_table\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return true_table_files\n",
    "\n",
    "\n",
    "def get_elements_from_true_tables(data_dir):\n",
    "    true_table_files = get_true_table_files(data_dir)\n",
    "\n",
    "    true_operative_rules_p1 = []\n",
    "    true_facts_p2 = []\n",
    "    true_names_p2 = []\n",
    "    true_terms_p2 = []\n",
    "    true_operative_rules_p2 = []\n",
    "\n",
    "    for table in true_table_files:\n",
    "        manager_true_elements = restore_checkpoint(table[\"path\"])\n",
    "        match table[\"id\"]:\n",
    "            case \"classify_P1|true_table\":\n",
    "                true_operative_rules_p1 = manager_true_elements.retrieve_document(\n",
    "                    \"classify_P1\", \"true_table\"\n",
    "                ).content\n",
    "                logger.debug(f\"P1: True Operative Rules: {true_operative_rules_p1}\")\n",
    "                logger.info(\n",
    "                    f\"P1: Operative Rules to evaluate: {len(true_operative_rules_p1)}\"\n",
    "                )\n",
    "            case \"classify_P2_Definitional_facts|true_table\":\n",
    "                true_facts_p2 = manager_true_elements.retrieve_document(\n",
    "                    \"classify_P2_Definitional_facts\", \"true_table\"\n",
    "                ).content\n",
    "                logger.debug(f\"P2: True Facts: {true_facts_p2}\")\n",
    "                logger.info(f\"P2: Facts to evaluate: {len(true_facts_p2)}\")\n",
    "            case \"classify_P2_Definitional_names|true_table\":\n",
    "                true_names_p2 = manager_true_elements.retrieve_document(\n",
    "                    \"classify_P2_Definitional_names\", \"true_table\"\n",
    "                ).content\n",
    "                logger.debug(f\"P2: True Names: {true_names_p2}\")\n",
    "                logger.info(f\"P2: Names to evaluate: {len(true_names_p2)}\")\n",
    "            case \"classify_P2_Definitional_terms|true_table\":\n",
    "                true_terms_p2 = manager_true_elements.retrieve_document(\n",
    "                    \"classify_P2_Definitional_terms\", \"true_table\"\n",
    "                ).content\n",
    "                logger.debug(f\"P2: True Terms: {true_terms_p2}\")\n",
    "                logger.info(f\"P2: Terms to evaluate: {len(true_terms_p2)}\")\n",
    "            case \"classify_P2_Operative_rules|true_table\":\n",
    "                true_operative_rules_p2 = manager_true_elements.retrieve_document(\n",
    "                    \"classify_P2_Operative_rules\", \"true_table\"\n",
    "                ).content\n",
    "                logger.debug(f\"P2: True Operative Rules: {true_operative_rules_p2}\")\n",
    "                logger.info(\n",
    "                    f\"P2: Operative Rules to evaluate: {len(true_operative_rules_p2)}\"\n",
    "                )\n",
    "\n",
    "    return (\n",
    "        true_operative_rules_p1,\n",
    "        true_facts_p2,\n",
    "        true_names_p2,\n",
    "        true_terms_p2,\n",
    "        true_operative_rules_p2,\n",
    "    )\n",
    "\n",
    "\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    DocumentProcessor is responsible for processing documents and categorizing elements such as terms, names, facts, and rules.\n",
    "\n",
    "    Attributes:\n",
    "        manager: Object used to manage document retrieval.\n",
    "        elements_terms_set (set): Set of unique terms found in the documents.\n",
    "        elements_names_set (set): Set of unique names found in the documents.\n",
    "        elements_terms (list): List of detailed information about terms.\n",
    "        elements_names (list): List of detailed information about names.\n",
    "        elements_facts (list): List of facts extracted from documents.\n",
    "        elements_rules (list): List of rules extracted from documents.\n",
    "        elements_terms_definition (dict): Dictionary to store terms definitions by document ID.\n",
    "    \"\"\"\n",
    "\n",
<<<<<<< HEAD
    "    def __init__(self, manager):\n",
    "        \"\"\"\n",
    "        Initializes the DocumentProcessor instance and processes the documents.\n",
    "\n",
    "        Args:\n",
    "            manager: Object used to manage document retrieval.\n",
    "        \"\"\"\n",
=======
    "    def __init__(self, manager: DocumentManager):\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "        self.manager = manager\n",
    "        self.elements_terms_set = set()\n",
    "        self.elements_names_set = set()\n",
    "        self.elements_terms = []\n",
    "        self.elements_names = []\n",
    "        self.elements_facts = []\n",
    "        self.elements_rules = []\n",
    "        self.elements_terms_definition = {}\n",
<<<<<<< HEAD
    "        \n",
    "        # Automatically process definitions and elements when instantiated\n",
    "        self.process_definitions()\n",
    "        self.process_elements()\n",
=======
    "        self.operative_rules_classifications = (\n",
    "            []\n",
    "        )  # To store classifications with type and subtype for operative rules\n",
    "        self.facts_classifications = (\n",
    "            []\n",
    "        )  # To store classifications with type and subtype for facts\n",
    "        self.terms_classifications = (\n",
    "            []\n",
    "        )  # To store classifications with type, subtype, and confidence for definitional terms\n",
    "        self.names_classifications = (\n",
    "            []\n",
    "        )  # To store classifications with type, subtype, and confidence for definitional names\n",
    "\n",
    "        # Automatically process definitions, classifications, and elements when instantiated\n",
    "        try:\n",
    "            self.process_definitions()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing definitions: {e}\")\n",
    "\n",
    "        try:\n",
    "            self.process_operative_rules_classifications()\n",
    "        except Exception as e:\n",
    "            logger.info(\n",
    "                f\"Document did not have operative rules classifications to process: {e}\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            self.process_facts_classifications()\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Document did not have facts classifications to process: {e}\")\n",
    "\n",
    "        try:\n",
    "            self.process_terms_classifications()\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Document did not have terms classifications to process: {e}\")\n",
    "\n",
    "        try:\n",
    "            self.process_names_classifications()\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Document did not have names classifications to process: {e}\")\n",
    "\n",
    "        try:\n",
    "            self.process_elements()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing elements: {e}\")\n",
    "            raise e\n",
    "\n",
    "        try:\n",
    "            self.process_transformed_elements()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing transformed elements: {e}\")\n",
    "            raise e\n",
    "\n",
    "        try:\n",
    "            self.process_validations()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing validations: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def process_validations(self):\n",
    "        \"\"\"\n",
    "        Processes validation documents and updates elements with validation scores and findings.\n",
    "        \"\"\"\n",
    "        validation_docs = {\n",
    "            \"validation_judge_Operative_Rules\": self.elements_rules,\n",
    "            \"validation_judge_Fact_Types\": self.elements_facts,\n",
    "            \"validation_judge_Terms\": self.elements_terms,\n",
    "            \"validation_judge_Names\": self.elements_names,\n",
    "        }\n",
    "\n",
    "        for doc_name, elements_list in validation_docs.items():\n",
    "            try:\n",
    "                # Retrieve the validation document\n",
    "                validation_doc = self.manager.retrieve_document(doc_name, \"llm_validation\")\n",
    "                if not validation_doc or not validation_doc.content:\n",
    "                    logger.warning(f\"Validation document '{doc_name}' not found or empty.\")\n",
    "                    continue\n",
    "\n",
    "                # Iterate over items in the validation document\n",
    "                for item in validation_doc.content:\n",
    "                    doc_id = normalize_str(item.get(\"doc_id\"))\n",
    "                    statement_id = normalize_str(str(item.get(\"statement_id\")))\n",
    "                    source = item.get(\"source\")\n",
    "\n",
    "                    # Fields to add\n",
    "                    semscore = item.get(\"semscore\")\n",
    "                    similarity_score = item.get(\"similarity_score\")\n",
    "                    similarity_score_confidence = item.get(\"similarity_score_confidence\")\n",
    "                    transformation_accuracy = item.get(\"transformation_accuracy\")\n",
    "                    grammar_syntax_accuracy = item.get(\"grammar_syntax_accuracy\")\n",
    "                    findings = item.get(\"findings\")\n",
    "\n",
    "                    # Find matching element in elements_list\n",
    "                    for element in elements_list:\n",
    "                        if (\n",
    "                            normalize_str(element.get(\"doc_id\")) == doc_id\n",
    "                            and normalize_str(str(element.get(\"statement_id\"))) == statement_id\n",
    "                            and element.get(\"source\") == source\n",
    "                        ):\n",
    "                            # Update element with new fields\n",
    "                            element.update({\n",
    "                                \"semscore\": semscore,\n",
    "                                \"similarity_score\": similarity_score,\n",
    "                                \"similarity_score_confidence\": similarity_score_confidence,\n",
    "                                \"transformation_accuracy\": transformation_accuracy,\n",
    "                                \"grammar_syntax_accuracy\": grammar_syntax_accuracy,\n",
    "                                \"findings\": findings,\n",
    "                            })\n",
    "                            break  # Exit the loop after finding the matching element\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing validation document '{doc_name}': {e}\")\n",
    "\n",
    "\n",
    "    def process_transformed_elements(self):\n",
    "        \"\"\"\n",
    "        Processes the transformed elements from the llm_response_transform document types and updates the transformed attribute in each relevant list.\n",
    "        \"\"\"\n",
    "        transform_docs = [\n",
    "            \"transform_Fact_Types\",\n",
    "            \"transform_Terms\",\n",
    "            \"transform_Names\",\n",
    "            \"transform_Operative_Rules\",\n",
    "        ]\n",
    "\n",
    "        for transform_doc_id in transform_docs:\n",
    "            transform_doc_content = self.manager.retrieve_document(\n",
    "                transform_doc_id, \"llm_response_transform\"\n",
    "            ).content\n",
    "\n",
    "            for item in transform_doc_content:\n",
    "                logger.debug(f\"{item=}\")\n",
    "                doc_id = normalize_str(item.get(\"doc_id\"))\n",
    "                statement_id = normalize_str(str(item.get(\"statement_id\")))\n",
    "                source = item.get(\"statement_source\")\n",
    "                transformed = item.get(\"transformed\")\n",
    "\n",
    "                logger.debug(\n",
    "                    f\"doc_id: {doc_id} - statement_id: {statement_id} - transformed: {transformed}\"\n",
    "                )\n",
    "\n",
    "                # Update the transformed attribute in each relevant list based on the type of document\n",
    "                logger.debug(f\"{transform_doc_id=}\")\n",
    "                if \"Fact_Types\" in transform_doc_id:\n",
    "                    for element in self.elements_facts:\n",
    "                        if (\n",
    "                            normalize_str(element[\"doc_id\"]) == doc_id\n",
    "                            and normalize_str(str(element[\"statement_id\"]))\n",
    "                            == statement_id\n",
    "                            and element[\"source\"] == source\n",
    "                        ):\n",
    "                            element[\"transformed\"] = transformed\n",
    "                            logger.debug(f\"{element=}\")\n",
    "                elif \"Terms\" in transform_doc_id:\n",
    "                    for element in self.elements_terms:\n",
    "                        logger.debug(f\"{element=}\")\n",
    "                        if (\n",
    "                            normalize_str(element[\"doc_id\"]) == doc_id\n",
    "                            and normalize_str(str(element[\"statement_id\"]))\n",
    "                            == statement_id\n",
    "                            and element[\"source\"] == source\n",
    "                        ):\n",
    "                            element[\"transformed\"] = transformed\n",
    "                elif \"Names\" in transform_doc_id:\n",
    "                    for element in self.elements_names:\n",
    "                        if (\n",
    "                            normalize_str(element[\"doc_id\"]) == doc_id\n",
    "                            and normalize_str(str(element[\"statement_id\"]))\n",
    "                            == statement_id\n",
    "                            and element[\"source\"] == source\n",
    "                        ):\n",
    "                            element[\"transformed\"] = transformed\n",
    "                elif \"Operative_Rules\" in transform_doc_id:\n",
    "                    for element in self.elements_rules:\n",
    "                        if (\n",
    "                            normalize_str(element[\"doc_id\"]) == doc_id\n",
    "                            and normalize_str(str(element[\"statement_id\"]))\n",
    "                            == statement_id\n",
    "                            and element[\"source\"] == source\n",
    "                        ):\n",
    "                            element[\"transformed\"] = transformed\n",
    "\n",
    "    def process_names_classifications(self):\n",
    "        \"\"\"\n",
    "        Processes classification information specifically for names from 'classify_P2_Definitional_names'\n",
    "        document and stores the type, subtype, subtype confidence, and subtype explanation.\n",
    "        The type is always set to 'Definitional'.\n",
    "        \"\"\"\n",
    "        # Document identifier we are interested in\n",
    "        doc_classification = \"classify_P2_Definitional_names\"\n",
    "\n",
    "        # Retrieve document content\n",
    "        doc = self.manager.retrieve_document(\n",
    "            doc_classification, \"llm_response_classification\"\n",
    "        )\n",
    "        if not doc or not doc.content:\n",
    "            logger.warning(\n",
    "                f\"Document '{doc_classification}' not found or has empty content.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        doc_content = doc.content\n",
    "\n",
    "        # Iterate over each item in the document content\n",
    "        for item in doc_content:\n",
    "            doc_id = normalize_str(item.get(\"doc_id\"))\n",
    "            statement_id = normalize_str(str(item.get(\"statement_id\")))\n",
    "            classifications = item.get(\"classification\", [])\n",
    "\n",
    "            # Iterate over each classification to extract the highest confidence one for names\n",
    "            for classification in classifications:\n",
    "                confidence = classification.get(\"confidence\", 0)\n",
    "\n",
    "                # Check if this classification already exists in names_classifications\n",
    "                existing_name = next(\n",
    "                    (\n",
    "                        name\n",
    "                        for name in self.names_classifications\n",
    "                        if name[\"doc_id\"] == doc_id\n",
    "                        and name[\"statement_id\"] == statement_id\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                # Initialize if not found\n",
    "                if existing_name is None:\n",
    "                    existing_name = {\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"statement_id\": statement_id,\n",
    "                        \"type\": \"Definitional\",  # Set type as \"Definitional\"\n",
    "                        \"subtype\": None,\n",
    "                        \"confidence\": -1,\n",
    "                        \"explanation\": \"\",\n",
    "                        \"templates_ids\": [],\n",
    "                    }\n",
    "                    self.names_classifications.append(existing_name)\n",
    "\n",
    "                # Update subtype information if the confidence is higher than the existing one\n",
    "                if confidence > existing_name[\"confidence\"]:\n",
    "                    existing_name[\"subtype\"] = classification.get(\"subtype\")\n",
    "                    existing_name[\"confidence\"] = confidence\n",
    "                    existing_name[\"explanation\"] = classification.get(\"explanation\", \"\")\n",
    "                    existing_name[\"templates_ids\"] = classification.get(\n",
    "                        \"templates_ids\", []\n",
    "                    )\n",
    "\n",
    "        # Log the final classification for debugging purposes\n",
    "        logger.debug(f\"{self.names_classifications=}\")\n",
    "\n",
    "    def process_facts_classifications(self):\n",
    "        \"\"\"\n",
    "        Processes classification information specifically for facts from 'classify_P2_Definitional_facts'\n",
    "        document and stores the type, subtype, subtype confidence, and subtype explanation.\n",
    "        The type is always set to 'Definitional'.\n",
    "        \"\"\"\n",
    "        # Document identifier we are interested in\n",
    "        doc_classification = \"classify_P2_Definitional_facts\"\n",
    "\n",
    "        # Retrieve document content\n",
    "        doc_content = self.manager.retrieve_document(\n",
    "            doc_classification, \"llm_response_classification\"\n",
    "        ).content\n",
    "\n",
    "        # Iterate over each item in the document content\n",
    "        for item in doc_content:\n",
    "            doc_id = normalize_str(item.get(\"doc_id\"))\n",
    "            statement_id = normalize_str(str(item.get(\"statement_id\")))\n",
    "            classifications = item.get(\"classification\", [])\n",
    "\n",
    "            # Iterate over each classification to extract the highest confidence one for facts\n",
    "            for classification in classifications:\n",
    "                confidence = classification.get(\"confidence\", 0)\n",
    "\n",
    "                # Check if this classification already exists in facts_classifications\n",
    "                existing_fact = next(\n",
    "                    (\n",
    "                        fact\n",
    "                        for fact in self.facts_classifications\n",
    "                        if fact[\"doc_id\"] == doc_id\n",
    "                        and fact[\"statement_id\"] == statement_id\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                # Initialize if not found\n",
    "                if existing_fact is None:\n",
    "                    existing_fact = {\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"statement_id\": statement_id,\n",
    "                        \"type\": \"Definitional\",  # Set type as \"Definitional\"\n",
    "                        \"subtype\": None,\n",
    "                        \"subtype_confidence\": -1,\n",
    "                        \"subtype_explanation\": \"\",\n",
    "                        \"templates_ids\": [],\n",
    "                    }\n",
    "                    self.facts_classifications.append(existing_fact)\n",
    "\n",
    "                # Update subtype information if the confidence is higher than the existing one\n",
    "                if confidence > existing_fact[\"subtype_confidence\"]:\n",
    "                    existing_fact[\"subtype\"] = classification.get(\"subtype\")\n",
    "                    existing_fact[\"subtype_confidence\"] = confidence\n",
    "                    existing_fact[\"subtype_explanation\"] = classification.get(\n",
    "                        \"explanation\", \"\"\n",
    "                    )\n",
    "                    existing_fact[\"templates_ids\"] = classification.get(\n",
    "                        \"templates_ids\", []\n",
    "                    )\n",
    "\n",
    "        # Log the final classification for debugging purposes\n",
    "        logger.debug(f\"{self.facts_classifications=}\")\n",
    "\n",
    "    def process_terms_classifications(self):\n",
    "        \"\"\"\n",
    "        Processes classification information specifically for terms from 'classify_P2_Definitional_terms'\n",
    "        document and stores the type, subtype, subtype confidence, and subtype explanation.\n",
    "        The type is always set to 'Definitional'.\n",
    "        \"\"\"\n",
    "        # Document identifier we are interested in\n",
    "        doc_classification = \"classify_P2_Definitional_terms\"\n",
    "\n",
    "        # Retrieve document content\n",
    "        doc = self.manager.retrieve_document(\n",
    "            doc_classification, \"llm_response_classification\"\n",
    "        )\n",
    "        if not doc or not doc.content:\n",
    "            logger.warning(\n",
    "                f\"Document '{doc_classification}' not found or has empty content.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        doc_content = doc.content\n",
    "\n",
    "        # Iterate over each item in the document content\n",
    "        for item in doc_content:\n",
    "            doc_id = normalize_str(item.get(\"doc_id\"))\n",
    "            statement_id = normalize_str(str(item.get(\"statement_id\")))\n",
    "            classifications = item.get(\"classification\", [])\n",
    "\n",
    "            # Iterate over each classification to extract the highest confidence one for terms\n",
    "            for classification in classifications:\n",
    "                confidence = classification.get(\"confidence\", 0)\n",
    "\n",
    "                # Check if this classification already exists in terms_classifications\n",
    "                existing_term = next(\n",
    "                    (\n",
    "                        term\n",
    "                        for term in self.terms_classifications\n",
    "                        if term[\"doc_id\"] == doc_id\n",
    "                        and term[\"statement_id\"] == statement_id\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                # Initialize if not found\n",
    "                if existing_term is None:\n",
    "                    existing_term = {\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"statement_id\": statement_id,\n",
    "                        \"type\": \"Definitional\",  # Set type as \"Definitional\"\n",
    "                        \"subtype\": None,\n",
    "                        \"confidence\": -1,\n",
    "                        \"explanation\": \"\",\n",
    "                        \"templates_ids\": [],\n",
    "                    }\n",
    "                    self.terms_classifications.append(existing_term)\n",
    "\n",
    "                # Update subtype information if the confidence is higher than the existing one\n",
    "                if confidence > existing_term[\"confidence\"]:\n",
    "                    existing_term[\"subtype\"] = classification.get(\"subtype\")\n",
    "                    existing_term[\"confidence\"] = confidence\n",
    "                    existing_term[\"explanation\"] = classification.get(\"explanation\", \"\")\n",
    "                    existing_term[\"templates_ids\"] = classification.get(\n",
    "                        \"templates_ids\", []\n",
    "                    )\n",
    "\n",
    "        # Log the final classification for debugging purposes\n",
    "        logger.debug(f\"{self.terms_classifications=}\")\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "\n",
    "    def add_definition(self, doc_id, term, definition):\n",
    "        \"\"\"\n",
    "        Adds a term definition to the elements_terms_definition dictionary.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): Identifier of the document.\n",
    "            term (str): The term to be defined.\n",
    "            definition (str): The definition of the term.\n",
    "        \"\"\"\n",
    "        self.elements_terms_definition.setdefault(doc_id, {})[term] = definition\n",
    "\n",
    "    def process_definitions(self):\n",
    "        \"\"\"\n",
    "        Processes document terms definitions and stores them in elements_terms_definition.\n",
    "        \"\"\"\n",
<<<<<<< HEAD
    "        docs_p2 = [s for s in self.manager.list_document_ids(doc_type=\"llm_response\") if s.endswith(\"_P2\")]\n",
    "\n",
    "        for doc in docs_p2:\n",
    "            doc_id = doc.replace(\"_P2\", \"\")\n",
    "            doc_content = self.manager.retrieve_document(doc, doc_type=\"llm_response\").content\n",
=======
    "        docs_p2 = [\n",
    "            s\n",
    "            for s in self.manager.list_document_ids(doc_type=\"llm_response\")\n",
    "            if s.endswith(\"_P2\")\n",
    "        ]\n",
    "\n",
    "        for doc in docs_p2:\n",
    "            doc_id = doc.replace(\"_P2\", \"\")\n",
    "            doc_content = self.manager.retrieve_document(\n",
    "                doc, doc_type=\"llm_response\"\n",
    "            ).content\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "            doc_terms = doc_content.get(\"terms\", [])\n",
    "            for term in doc_terms:\n",
    "                self.add_definition(doc_id, term.get(\"term\"), term.get(\"definition\"))\n",
    "\n",
<<<<<<< HEAD
=======
    "    def process_operative_rules_classifications(self):\n",
    "        \"\"\"\n",
    "        Processes classification information specifically for operative rules from\n",
    "        'classify_P1' and 'classify_P2_Operative_rules' documents, and stores the type, subtype,\n",
    "        confidence, and explanation.\n",
    "        \"\"\"\n",
    "        # Get only the specific documents we are interested in\n",
    "        docs_classification = [\n",
    "            s\n",
    "            for s in self.manager.list_document_ids(\n",
    "                doc_type=\"llm_response_classification\"\n",
    "            )\n",
    "            if s in [\"classify_P1\", \"classify_P2_Operative_rules\"]\n",
    "        ]\n",
    "\n",
    "        # A temporary dictionary to group classifications by (doc_id, statement_id)\n",
    "        classification_dict = {}\n",
    "\n",
    "        # Iterate over each document\n",
    "        for doc in docs_classification:\n",
    "            # Retrieve document content for each classification document\n",
    "            doc_content = self.manager.retrieve_document(\n",
    "                doc, \"llm_response_classification\"\n",
    "            ).content\n",
    "\n",
    "            # Iterate over each item in the document content\n",
    "            for item in doc_content:\n",
    "                doc_id = normalize_str(item.get(\"doc_id\"))\n",
    "                statement_id = normalize_str(str(item.get(\"statement_id\")))\n",
    "                classifications = item.get(\"classification\", [])\n",
    "\n",
    "                # Create a key for grouping classifications\n",
    "                key = (doc_id, statement_id)\n",
    "\n",
    "                # Initialize the classification entry if it doesn't exist\n",
    "                if key not in classification_dict:\n",
    "                    classification_dict[key] = {\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"statement_id\": statement_id,\n",
    "                        \"type\": None,\n",
    "                        \"type_confidence\": -1,  # Initialize with a negative value to ensure first confidence is updated\n",
    "                        \"type_explanation\": \"\",\n",
    "                        \"subtype\": None,\n",
    "                        \"subtype_confidence\": -1,  # Initialize with a negative value to ensure first confidence is updated\n",
    "                        \"subtype_explanation\": \"\",\n",
    "                        \"templates_ids\": [],\n",
    "                    }\n",
    "\n",
    "                # Iterate over each classification to extract the highest confidence one\n",
    "                for classification in classifications:\n",
    "                    # Extract confidence and other classification details\n",
    "                    confidence = classification.get(\"confidence\", 0)\n",
    "                    current_classification = classification_dict[key]\n",
    "\n",
    "                    # Update based on document type and ensure we retain both type and subtype\n",
    "                    if doc == \"classify_P1\":\n",
    "                        # Update type if this document is from classify_P1 and has higher confidence\n",
    "                        if confidence > current_classification[\"type_confidence\"]:\n",
    "                            current_classification[\"type\"] = classification.get(\"type\")\n",
    "                            current_classification[\"type_confidence\"] = confidence\n",
    "                            current_classification[\"type_explanation\"] = (\n",
    "                                classification.get(\"explanation\", \"\")\n",
    "                            )\n",
    "\n",
    "                    elif doc == \"classify_P2_Operative_rules\":\n",
    "                        # Update subtype if this document is from classify_P2_Operative_rules and has higher confidence\n",
    "                        if confidence > current_classification[\"subtype_confidence\"]:\n",
    "                            current_classification[\"subtype\"] = classification.get(\n",
    "                                \"subtype\"\n",
    "                            )\n",
    "                            current_classification[\"subtype_confidence\"] = confidence\n",
    "                            current_classification[\"subtype_explanation\"] = (\n",
    "                                classification.get(\"explanation\", \"\")\n",
    "                            )\n",
    "                            current_classification[\"templates_ids\"] = (\n",
    "                                classification.get(\"templates_ids\", [])\n",
    "                            )\n",
    "\n",
    "        # Convert the classification_dict to a list and assign to operative_rules_classifications\n",
    "        self.operative_rules_classifications = list(classification_dict.values())\n",
    "        logger.debug(f\"{self.operative_rules_classifications=}\")\n",
    "\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "    def process_elements(self):\n",
    "        \"\"\"\n",
    "        Processes elements from documents and categorizes them into terms, names, facts, and rules.\n",
    "        \"\"\"\n",
<<<<<<< HEAD
    "        docs_p1 = [s for s in self.manager.list_document_ids(doc_type=\"llm_response\") if s.endswith(\"_P1\")]\n",
    "\n",
    "        for doc in docs_p1:\n",
    "            doc_content = self.manager.retrieve_document(doc, doc_type=\"llm_response\").content\n",
=======
    "        # Get the list of documents that end with '_P1'\n",
    "        docs_p1 = [\n",
    "            s\n",
    "            for s in self.manager.list_document_ids(doc_type=\"llm_response\")\n",
    "            if s.endswith(\"_P1\")\n",
    "        ]\n",
    "\n",
    "        for doc in docs_p1:\n",
    "            doc_content = self.manager.retrieve_document(\n",
    "                doc, doc_type=\"llm_response\"\n",
    "            ).content\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "            doc_id = doc_content.get(\"section\")\n",
    "            doc_elements = doc_content.get(\"elements\", [])\n",
    "            for element in doc_elements:\n",
    "                element_classification = element.get(\"classification\")\n",
    "                element_id = element.get(\"id\")\n",
    "                verb_symbols = element.get(\"verb_symbols\") or element.get(\"verb_symbol\")\n",
    "                if isinstance(verb_symbols, str):\n",
    "                    verb_symbols = [verb_symbols]\n",
    "                elif verb_symbols is None:\n",
    "                    verb_symbols = []\n",
    "                element_dict = {\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"statement_id\": element_id,\n",
    "                    \"statement\": element.get(\"statement\"),\n",
    "                    \"source\": element.get(\"source\"),\n",
    "                    \"terms\": element.get(\"terms\", []),\n",
<<<<<<< HEAD
    "                    \"verb_symbols\": verb_symbols\n",
=======
    "                    \"verb_symbols\": verb_symbols,\n",
    "                    \"element_name\": element_classification,\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "                }\n",
    "\n",
    "                match element_classification:\n",
    "                    case \"Fact\" | \"Fact Type\":\n",
    "                        self.elements_facts.append(element_dict)\n",
    "                    case \"Operative Rule\":\n",
    "                        self.elements_rules.append(element_dict)\n",
    "\n",
    "                element_terms = element.get(\"terms\", [])\n",
    "                if element_terms:\n",
    "                    for term in element_terms:\n",
    "                        signifier = term.get(\"term\")\n",
    "                        term_dict = {\n",
    "                            \"doc_id\": doc_id,\n",
<<<<<<< HEAD
    "                            \"signifier\": signifier,\n",
    "                            \"statement_id\": element_id,\n",
    "                            \"definition\": self.elements_terms_definition.get(doc_id, {}).get(signifier),\n",
    "                            \"source\": element.get(\"source\")\n",
    "                        }\n",
    "                        if term.get(\"classification\") == \"Common Noun\":\n",
    "                            self.elements_terms.append(term_dict)\n",
    "                            self.elements_terms_set.add(signifier)\n",
    "                        else:\n",
=======
    "                            \"statement_id\": signifier,\n",
    "                            # \"statement_id\": element_id,\n",
    "                            \"definition\": self.elements_terms_definition.get(\n",
    "                                doc_id, {}\n",
    "                            ).get(signifier),\n",
    "                            \"source\": element.get(\"source\"),\n",
    "                        }\n",
    "                        if term.get(\"classification\") == \"Common Noun\":\n",
    "                            term_dict[\"element_name\"] = \"Term\"\n",
    "                            self.elements_terms.append(term_dict)\n",
    "                            self.elements_terms_set.add(signifier)\n",
    "                        else:\n",
    "                            term_dict[\"element_name\"] = \"Name\"\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "                            self.elements_names.append(term_dict)\n",
    "                            self.elements_names_set.add(signifier)\n",
    "\n",
    "    def get_unique_terms(self, doc_id=None):\n",
    "        \"\"\"\n",
    "        Returns the set of unique terms found in the documents. If doc_id is provided,\n",
    "        returns only the unique terms for that specific document.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str, optional): Identifier of the document. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            set: Set of unique terms.\n",
    "        \"\"\"\n",
    "        if doc_id:\n",
<<<<<<< HEAD
    "            return {term[\"signifier\"] for term in self.elements_terms if term[\"doc_id\"] == doc_id}\n",
=======
    "            return {\n",
    "                term[\"signifier\"]\n",
    "                for term in self.elements_terms\n",
    "                if term[\"doc_id\"] == doc_id\n",
    "            }\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "        return self.elements_terms_set\n",
    "\n",
    "    def get_unique_names(self, doc_id=None):\n",
    "        \"\"\"\n",
    "        Returns the set of unique names found in the documents. If doc_id is provided,\n",
    "        returns only the unique names for that specific document.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str, optional): Identifier of the document. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            set: Set of unique names.\n",
    "        \"\"\"\n",
    "        if doc_id:\n",
<<<<<<< HEAD
    "            return {name[\"signifier\"] for name in self.elements_names if name[\"doc_id\"] == doc_id}\n",
    "        return self.elements_names_set\n",
    "\n",
    "    # def get_terms(self):\n",
    "    #     \"\"\"\n",
    "    #     Returns the list of terms with detailed information.\n",
    "\n",
    "    #     Returns:\n",
    "    #         list: List of terms.\n",
    "    #     \"\"\"\n",
    "    #     return self.elements_terms\n",
    "\n",
    "    # def get_names(self):\n",
    "    #     \"\"\"\n",
    "    #     Returns the list of names with detailed information.\n",
    "\n",
    "    #     Returns:\n",
    "    #         list: List of names.\n",
    "    #     \"\"\"\n",
    "    #     return self.elements_names\n",
    "\n",
    "    def get_terms(self, definition_filter=\"all\"):\n",
    "        \"\"\"\n",
    "        Returns the list of terms with detailed information, filtered by the presence of a definition.\n",
    "\n",
    "        Args:\n",
    "            definition_filter (str): Filter for terms based on definition presence. \n",
=======
    "            return {\n",
    "                name[\"signifier\"]\n",
    "                for name in self.elements_names\n",
    "                if name[\"doc_id\"] == doc_id\n",
    "            }\n",
    "        return self.elements_names_set\n",
    "\n",
    "    def get_terms(self, doc_id=None, term_id=None, definition_filter=\"all\"):\n",
    "        \"\"\"\n",
    "        Returns the list of terms extracted from documents, enriched with type, subtype, confidence, and explanation.\n",
    "        If doc_id and term_id are provided, returns a specific term.\n",
    "        If only doc_id is provided, returns all terms for that specific document.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str, optional): Document identifier to filter a specific term or all terms in the document.\n",
    "            term_id (str, optional): Term identifier to filter a specific term.\n",
    "            definition_filter (str): Filter for terms based on definition presence.\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "                                    \"non_null\" returns terms with definitions,\n",
    "                                    \"null\" returns terms without definitions,\n",
    "                                    \"all\" returns all terms regardless of definition.\n",
    "\n",
    "        Returns:\n",
<<<<<<< HEAD
    "            list: List of terms.\n",
    "        \"\"\"\n",
    "        if definition_filter == \"non_null\":\n",
    "            return [term for term in self.elements_terms if term.get(\"definition\") is not None]\n",
    "        elif definition_filter == \"null\":\n",
    "            return [term for term in self.elements_terms if term.get(\"definition\") is None]\n",
    "        return self.elements_terms\n",
    "\n",
    "    def get_names(self, definition_filter=\"all\"):\n",
    "        \"\"\"\n",
    "        Returns the list of names with detailed information, filtered by the presence of a definition.\n",
    "\n",
    "        Args:\n",
    "            definition_filter (str): Filter for names based on definition presence. \n",
    "                                    \"non_null\" returns names with definitions,\n",
    "                                    \"null\" returns names without definitions,\n",
    "                                    \"all\" returns all names regardless of definition.\n",
    "\n",
    "        Returns:\n",
    "            list: List of names.\n",
    "        \"\"\"\n",
    "        if definition_filter == \"non_null\":\n",
    "            return [name for name in self.elements_names if name.get(\"definition\") is not None]\n",
    "        elif definition_filter == \"null\":\n",
    "            return [name for name in self.elements_names if name.get(\"definition\") is None]\n",
    "        return self.elements_names\n",
    "\n",
    "\n",
    "    def get_facts(self):\n",
    "        \"\"\"\n",
    "        Returns the list of facts extracted from documents.\n",
    "\n",
    "        Returns:\n",
    "            list: List of facts.\n",
    "        \"\"\"\n",
    "        return self.elements_facts\n",
    "\n",
    "    def get_rules(self):\n",
    "        \"\"\"\n",
    "        Returns the list of rules extracted from documents.\n",
    "\n",
    "        Returns:\n",
    "            list: List of rules.\n",
    "        \"\"\"\n",
    "        return self.elements_rules\n",
    "\n",
    "    def get_term_info(self, doc_id, term):\n",
    "        \"\"\"\n",
    "        Retrieves information about a specific term from elements.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): Document identifier.\n",
    "            term (str): Term to retrieve information for.\n",
    "\n",
    "        Returns:\n",
    "            dict or None: A dictionary containing term information if found, otherwise None.\n",
    "        \"\"\"\n",
    "        definition = self.elements_terms_definition.get(doc_id, {}).get(term)\n",
    "        if definition:\n",
    "            for term_dict in self.elements_terms + self.elements_names:\n",
    "                if term_dict[\"doc_id\"] == doc_id and term_dict[\"signifier\"] == term:\n",
    "                    return {\n",
    "                        \"definition\": definition,\n",
    "                        \"source\": term_dict[\"source\"],\n",
    "                        \"statement_id\": term_dict[\"statement_id\"]\n",
    "                    }\n",
    "        return None\n",
    "\n",
    "    def get_name_info(self, doc_id, name):\n",
    "        \"\"\"\n",
    "        Retrieves information about a specific name from elements.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): Document identifier.\n",
    "            name (str): Name to retrieve information for.\n",
    "\n",
    "        Returns:\n",
    "            dict or None: A dictionary containing name information if found, otherwise None.\n",
    "        \"\"\"\n",
    "        for name_dict in self.elements_names:\n",
    "            if name_dict[\"doc_id\"] == doc_id and name_dict[\"signifier\"] == name:\n",
    "                return {\n",
    "                    \"definition\": name_dict.get(\"definition\"),\n",
    "                    \"source\": name_dict[\"source\"],\n",
    "                    \"statement_id\": name_dict[\"statement_id\"]\n",
    "                }\n",
    "        return None\n",
    "\n",
    "    def get_fact_info(self, doc_id, statement_id):\n",
    "        \"\"\"\n",
    "        Retrieves information about a specific fact from elements.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): Document identifier.\n",
    "            statement_id (str): statement identifier of the fact.\n",
    "\n",
    "        Returns:\n",
    "            dict or None: A dictionary containing fact information if found, otherwise None.\n",
    "        \"\"\"\n",
    "        for fact_dict in self.elements_facts:\n",
    "            if fact_dict[\"doc_id\"] == doc_id and fact_dict[\"statement_id\"] == statement_id:\n",
    "                terms = [term.get(\"term\") for term in fact_dict.get(\"terms\", []) if term.get(\"classification\") == \"Common Noun\"]\n",
    "                names = [term.get(\"term\") for term in fact_dict.get(\"terms\", []) if term.get(\"classification\") == \"Proper Noun\"]\n",
    "                return {\n",
    "                    \"statement\": fact_dict[\"statement\"],\n",
    "                    \"source\": fact_dict[\"source\"],\n",
    "                    \"terms\": terms,\n",
    "                    \"names\": names,\n",
    "                    \"verb_symbols\": fact_dict.get(\"verb_symbols\", [])\n",
    "                }\n",
    "        return None\n",
    "\n",
    "    def get_rule_info(self, doc_id, statement_id):\n",
    "        \"\"\"\n",
    "        Retrieves information about a specific rule from elements.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): Document identifier.\n",
    "            statement_id (str): statement identifier of the rule.\n",
    "\n",
    "        Returns:\n",
    "            dict or None: A dictionary containing rule information if found, otherwise None.\n",
    "        \"\"\"\n",
    "        for rule_dict in self.elements_rules:\n",
    "            if rule_dict[\"doc_id\"] == doc_id and rule_dict[\"statement_id\"] == statement_id:\n",
    "                terms = [term.get(\"term\") for term in rule_dict.get(\"terms\", []) if term.get(\"classification\") == \"Common Noun\"]\n",
    "                names = [term.get(\"term\") for term in rule_dict.get(\"terms\", []) if term.get(\"classification\") == \"Proper Noun\"]\n",
    "                return {\n",
    "                    \"statement\": rule_dict.get(\"statement\"),\n",
    "                    \"source\": rule_dict.get(\"source\"),\n",
    "                    \"terms\": terms,\n",
    "                    \"names\": names,\n",
    "                    \"verb_symbols\": rule_dict.get(\"verb_symbols\", [])\n",
    "                }\n",
    "        return None\n",
    "\n",
    "# # Example usage\n",
    "# processor = DocumentProcessor(manager)\n",
    "\n",
    "# # Access processed data\n",
    "# unique_terms = processor.get_unique_terms()\n",
    "# unique_names = processor.get_unique_names()\n",
    "# terms = processor.get_terms()\n",
    "# names = processor.get_names()\n",
    "# facts = processor.get_facts()\n",
    "# rules = processor.get_rules()\n",
    "\n",
    "# print(f\"Unique terms: {len(unique_terms)}\")\n",
    "# print(f\"Unique names: {len(unique_names)}\")\n",
    "\n",
    "# print(f'Rules from ยง 275.0-2: {processor.get_rule_info(\"ยง 275.0-2\", 3)}')\n",
    "# print(f'Facts from ยง 275.0-2: {processor.get_fact_info(\"ยง 275.0-2\", 2)}')"
=======
    "            list or dict: List of enriched terms, or a dictionary with a specific term if both doc_id and term_id are provided.\n",
    "        \"\"\"\n",
    "        # Create a lookup dictionary for terms classifications for efficient access\n",
    "        term_classification_lookup = {\n",
    "            (\n",
    "                normalize_str(classification[\"doc_id\"]),\n",
    "                normalize_str(str(classification[\"statement_id\"])),\n",
    "            ): classification\n",
    "            for classification in self.terms_classifications\n",
    "        }\n",
    "\n",
    "        enriched_terms = []\n",
    "\n",
    "        # Enrich terms with classification information\n",
    "        for term in self.elements_terms:\n",
    "            term_key = (\n",
    "                normalize_str(term[\"doc_id\"]),\n",
    "                normalize_str(term.get(\"statement_id\", \"\")),\n",
    "            )\n",
    "            classification = term_classification_lookup.get(term_key)\n",
    "\n",
    "            if classification:\n",
    "                term.update(\n",
    "                    {\n",
    "                        \"type\": classification.get(\"type\"),\n",
    "                        \"subtype\": classification.get(\"subtype\"),\n",
    "                        \"confidence\": classification.get(\"confidence\"),\n",
    "                        \"explanation\": classification.get(\"explanation\"),\n",
    "                        \"templates_ids\": classification.get(\"templates_ids\", []),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            enriched_terms.append(term)\n",
    "\n",
    "        # Apply filtering based on definition presence\n",
    "        if definition_filter == \"non_null\":\n",
    "            enriched_terms = [term for term in enriched_terms if term.get(\"definition\")]\n",
    "        elif definition_filter == \"null\":\n",
    "            enriched_terms = [\n",
    "                term for term in enriched_terms if not term.get(\"definition\")\n",
    "            ]\n",
    "\n",
    "        # If both doc_id and term_id are provided, return the specific term\n",
    "        if doc_id and term_id:\n",
    "            for term in enriched_terms:\n",
    "                if normalize_str(term[\"doc_id\"]) == normalize_str(\n",
    "                    doc_id\n",
    "                ) and normalize_str(term.get(\"statement_id\", \"\")) == normalize_str(\n",
    "                    term_id\n",
    "                ):\n",
    "                    return term\n",
    "\n",
    "            # Return None if no matching term is found\n",
    "            logger.debug(f\"No term found for doc_id='{doc_id}', term_id='{term_id}'\")\n",
    "            return None\n",
    "\n",
    "        # If only doc_id is provided, return all terms that match the given doc_id\n",
    "        if doc_id:\n",
    "            filtered_terms = [\n",
    "                term\n",
    "                for term in enriched_terms\n",
    "                if normalize_str(term[\"doc_id\"]) == normalize_str(doc_id)\n",
    "            ]\n",
    "\n",
    "            if not filtered_terms:\n",
    "                logger.debug(f\"No terms found for doc_id='{doc_id}'\")\n",
    "                return []\n",
    "\n",
    "            return filtered_terms\n",
    "\n",
    "        # If neither doc_id nor term_id is provided, return all enriched terms\n",
    "        return enriched_terms\n",
    "\n",
    "    def get_names(self, doc_id=None, name_id=None, definition_filter=\"all\"):\n",
    "        \"\"\"\n",
    "        Returns the list of names extracted from documents, enriched with type, subtype, confidence, and explanation.\n",
    "        If doc_id and name_id are provided, returns a specific name.\n",
    "        If only doc_id is provided, returns all names for that specific document.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str, optional): Document identifier to filter a specific name or all names in the document.\n",
    "            name_id (str, optional): Name identifier to filter a specific name.\n",
    "            definition_filter (str, optional): Filter for names based on definition presence.\n",
    "                                            \"non_null\" returns names with definitions,\n",
    "                                            \"null\" returns names without definitions,\n",
    "                                            \"all\" returns all names regardless of definition.\n",
    "\n",
    "        Returns:\n",
    "            list or dict: List of enriched names, or a dictionary with a specific name if both doc_id and name_id are provided.\n",
    "        \"\"\"\n",
    "        # Create a lookup dictionary for names classifications for efficient access\n",
    "        name_classification_lookup = {\n",
    "            (\n",
    "                normalize_str(classification[\"doc_id\"]),\n",
    "                normalize_str(str(classification[\"statement_id\"])),\n",
    "            ): classification\n",
    "            for classification in self.names_classifications\n",
    "        }\n",
    "\n",
    "        enriched_names = []\n",
    "\n",
    "        # Enrich names with classification information\n",
    "        for name in self.elements_names:\n",
    "            name_key = (\n",
    "                normalize_str(name[\"doc_id\"]),\n",
    "                normalize_str(name.get(\"statement_id\", \"\")),\n",
    "            )\n",
    "            classification = name_classification_lookup.get(name_key)\n",
    "\n",
    "            if classification:\n",
    "                name.update(\n",
    "                    {\n",
    "                        \"type\": classification.get(\"type\"),\n",
    "                        \"subtype\": classification.get(\"subtype\"),\n",
    "                        \"confidence\": classification.get(\"confidence\"),\n",
    "                        \"explanation\": classification.get(\"explanation\"),\n",
    "                        \"templates_ids\": classification.get(\"templates_ids\", []),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            enriched_names.append(name)\n",
    "\n",
    "        # Apply filtering based on definition presence\n",
    "        if definition_filter == \"non_null\":\n",
    "            enriched_names = [name for name in enriched_names if name.get(\"definition\")]\n",
    "        elif definition_filter == \"null\":\n",
    "            enriched_names = [\n",
    "                name for name in enriched_names if not name.get(\"definition\")\n",
    "            ]\n",
    "\n",
    "        # If both doc_id and name_id are provided, return the specific name\n",
    "        if doc_id and name_id:\n",
    "            for name in enriched_names:\n",
    "                if normalize_str(name[\"doc_id\"]) == normalize_str(\n",
    "                    doc_id\n",
    "                ) and normalize_str(name.get(\"statement_id\", \"\")) == normalize_str(\n",
    "                    name_id\n",
    "                ):\n",
    "                    return name\n",
    "\n",
    "            # Return None if no matching name is found\n",
    "            logger.debug(f\"No name found for doc_id='{doc_id}', name_id='{name_id}'\")\n",
    "            return None\n",
    "\n",
    "        # If only doc_id is provided, return all names that match the given doc_id\n",
    "        if doc_id:\n",
    "            filtered_names = [\n",
    "                name\n",
    "                for name in enriched_names\n",
    "                if normalize_str(name[\"doc_id\"]) == normalize_str(doc_id)\n",
    "            ]\n",
    "\n",
    "            if not filtered_names:\n",
    "                logger.debug(f\"No names found for doc_id='{doc_id}'\")\n",
    "                return []\n",
    "\n",
    "            return filtered_names\n",
    "\n",
    "        # If neither doc_id nor name_id is provided, return all enriched names\n",
    "        return enriched_names\n",
    "\n",
    "    def get_facts(self, doc_id=None, statement_id=None):\n",
    "        \"\"\"\n",
    "        Returns the list of facts extracted from documents, enriched with type, subtype, confidence, and explanation.\n",
    "        If doc_id and statement_id are provided, returns a specific fact.\n",
    "        If only doc_id is provided, returns all facts for that specific document.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str, optional): Document identifier to filter a specific fact or all facts in the document.\n",
    "            statement_id (str, optional): Statement identifier to filter a specific fact.\n",
    "\n",
    "        Returns:\n",
    "            list or dict: List of enriched facts, or a dictionary with a specific fact if both doc_id and statement_id are provided.\n",
    "        \"\"\"\n",
    "        # Create a lookup dictionary for facts classifications for efficient access\n",
    "        fact_classification_lookup = {\n",
    "            (\n",
    "                normalize_str(classification[\"doc_id\"]),\n",
    "                normalize_str(str(classification[\"statement_id\"])),\n",
    "            ): classification\n",
    "            for classification in self.facts_classifications\n",
    "        }\n",
    "\n",
    "        enriched_facts = []\n",
    "\n",
    "        # Enrich facts with classification information\n",
    "        for fact in self.elements_facts:\n",
    "            fact_key = (\n",
    "                normalize_str(fact[\"doc_id\"]),\n",
    "                normalize_str(str(fact[\"statement_id\"])),\n",
    "            )\n",
    "            classification = fact_classification_lookup.get(fact_key)\n",
    "\n",
    "            if classification:\n",
    "                fact.update(\n",
    "                    {\n",
    "                        \"type\": classification.get(\"type\"),\n",
    "                        \"subtype\": classification.get(\"subtype\"),\n",
    "                        \"subtype_confidence\": classification.get(\"subtype_confidence\"),\n",
    "                        \"subtype_explanation\": classification.get(\n",
    "                            \"subtype_explanation\"\n",
    "                        ),\n",
    "                        \"templates_ids\": classification.get(\"templates_ids\", []),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            enriched_facts.append(fact)\n",
    "\n",
    "        # If both doc_id and statement_id are provided, return the specific fact\n",
    "        if doc_id and statement_id:\n",
    "            for fact in enriched_facts:\n",
    "                if normalize_str(fact[\"doc_id\"]) == normalize_str(\n",
    "                    doc_id\n",
    "                ) and normalize_str(str(fact[\"statement_id\"])) == normalize_str(\n",
    "                    str(statement_id)\n",
    "                ):\n",
    "                    return fact\n",
    "\n",
    "            # Return None if no matching fact is found\n",
    "            logger.debug(\n",
    "                f\"No fact found for doc_id='{doc_id}', statement_id='{statement_id}'\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        # If only doc_id is provided, return all facts that match the given doc_id\n",
    "        if doc_id:\n",
    "            filtered_facts = [\n",
    "                fact\n",
    "                for fact in enriched_facts\n",
    "                if normalize_str(fact[\"doc_id\"]) == normalize_str(doc_id)\n",
    "            ]\n",
    "\n",
    "            if not filtered_facts:\n",
    "                logger.debug(f\"No facts found for doc_id='{doc_id}'\")\n",
    "                return []\n",
    "\n",
    "            return filtered_facts\n",
    "\n",
    "        # If neither doc_id nor statement_id is provided, return all enriched facts\n",
    "        return enriched_facts\n",
    "\n",
    "    def get_rules(self, doc_id=None, statement_id=None):\n",
    "        \"\"\"\n",
    "        Returns the list of rules extracted from documents, enriched with type, subtype, confidence, and explanation.\n",
    "        If doc_id and statement_id are provided, returns a specific rule.\n",
    "        If only doc_id is provided, returns all rules for that specific document.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str, optional): Document identifier to filter a specific rule or all rules in the document.\n",
    "            statement_id (str, optional): Statement identifier to filter a specific rule.\n",
    "\n",
    "        Returns:\n",
    "            list or dict: List of enriched rules, or a dictionary with a specific rule if both doc_id and statement_id are provided.\n",
    "        \"\"\"\n",
    "        # Create a lookup dictionary for rules classifications for efficient access\n",
    "        rule_classification_lookup = {\n",
    "            (\n",
    "                normalize_str(classification[\"doc_id\"]),\n",
    "                normalize_str(str(classification[\"statement_id\"])),\n",
    "            ): classification\n",
    "            for classification in self.operative_rules_classifications\n",
    "        }\n",
    "\n",
    "        enriched_rules = []\n",
    "\n",
    "        # Enrich rules with classification information\n",
    "        for rule in self.elements_rules:\n",
    "            rule_key = (\n",
    "                normalize_str(rule[\"doc_id\"]),\n",
    "                normalize_str(str(rule[\"statement_id\"])),\n",
    "            )\n",
    "            classification = rule_classification_lookup.get(rule_key)\n",
    "\n",
    "            if classification:\n",
    "                rule.update(\n",
    "                    {\n",
    "                        \"type\": classification.get(\"type\"),\n",
    "                        \"type_confidence\": classification.get(\"type_confidence\"),\n",
    "                        \"type_explanation\": classification.get(\"type_explanation\"),\n",
    "                        \"subtype\": classification.get(\"subtype\"),\n",
    "                        \"subtype_confidence\": classification.get(\"subtype_confidence\"),\n",
    "                        \"subtype_explanation\": classification.get(\n",
    "                            \"subtype_explanation\"\n",
    "                        ),\n",
    "                        \"templates_ids\": classification.get(\"templates_ids\"),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            enriched_rules.append(rule)\n",
    "\n",
    "        # If both doc_id and statement_id are provided, return the specific rule\n",
    "        if doc_id and statement_id:\n",
    "            for rule in enriched_rules:\n",
    "                if normalize_str(rule[\"doc_id\"]) == normalize_str(\n",
    "                    doc_id\n",
    "                ) and normalize_str(str(rule[\"statement_id\"])) == normalize_str(\n",
    "                    str(statement_id)\n",
    "                ):\n",
    "                    return rule\n",
    "\n",
    "            # Return None if no matching rule is found\n",
    "            logger.debug(\n",
    "                f\"No rule found for doc_id='{doc_id}', statement_id='{statement_id}'\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        # If only doc_id is provided, return all rules that match the given doc_id\n",
    "        if doc_id:\n",
    "            filtered_rules = [\n",
    "                rule\n",
    "                for rule in enriched_rules\n",
    "                if normalize_str(rule[\"doc_id\"]) == normalize_str(doc_id)\n",
    "            ]\n",
    "\n",
    "            if not filtered_rules:\n",
    "                logger.debug(f\"No rules found for doc_id='{doc_id}'\")\n",
    "                return []\n",
    "\n",
    "            return filtered_rules\n",
    "\n",
    "        # If neither doc_id nor statement_id is provided, return all enriched rules\n",
    "        return enriched_rules"
>>>>>>> 3ee09f8 (add chap 7 elements validation)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir logging_setup && touch logging_setup/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile logging_setup/main.py\n",
    "\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "\n",
    "\n",
    "def setting_logging(log_path: str, log_level: str):\n",
    "    # Ensure the ../logs directory exists\n",
    "    log_directory = Path.cwd() / log_path\n",
    "    log_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Path for the log file\n",
    "    log_file_path = log_directory / \"application.log\"\n",
    "\n",
    "    # Set up TimedRotatingFileHandler to rotate logs every day\n",
    "    file_handler = TimedRotatingFileHandler(\n",
    "        log_file_path,\n",
    "        when=\"midnight\",\n",
    "        interval=1,\n",
    "        backupCount=0,  # Rotate every midnight, keep all backups\n",
    "    )\n",
    "\n",
    "    # Set the file handler's log format\n",
    "    file_handler.setFormatter(\n",
    "        logging.Formatter(\n",
    "            \"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Set up logging configuration\n",
    "    logging.basicConfig(\n",
    "        level=log_level,  # Set to the desired log level\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Console log format\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",  # Custom date format\n",
    "        handlers=[\n",
    "            file_handler,  # Log to the rotating file in ../logs\n",
    "            logging.StreamHandler(),  # Log to console\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Example logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Log a test message to verify\n",
    "    logger.info(\"Logging is set up with daily rotation.\")\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir token_estimator && touch token_estimator/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile token_estimator/main.py\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def estimate_tokens(text, model=\"gpt-4o\"):\n",
    "    \"\"\"\n",
    "    Estimates the number of tokens in a given text using the OpenAI `tiktoken` library, \n",
    "    which closely approximates the tokenization method used by OpenAI language models.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to be tokenized and counted.\n",
    "        model (str): The model to use for tokenization. Defaults to \"gpt-4o\".\n",
    "                     Supported models include \"gpt-3.5-turbo\" and \"gpt-4o\".\n",
    "\n",
    "    Returns:\n",
    "        int: The estimated number of tokens in the text.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the specified model is not supported by `tiktoken`.\n",
    "\n",
    "    Example:\n",
    "        >>> text = \"This is a sample text.\"\n",
    "        >>> estimate_tokens_tiktoken(text)\n",
    "        6\n",
    "    \"\"\"\n",
    "    # Load the appropriate tokenizer\n",
    "    try:\n",
    "        tokenizer = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Model '{model}' is not supported by tiktoken.\")\n",
    "    \n",
    "    # Tokenize the text and return the token count\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir rules_taxonomy_provider && touch rules_taxonomy_provider/__init__.py"
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v1"
   ]
  },
  {
>>>>>>> 3ee09f8 (add chap 7 elements validation)
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rules_taxonomy_provider/main.py\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "class RuleInformationProvider:\n",
    "    \"\"\"\n",
    "    A class to provide information about rule classifications and templates based on YAML data.\n",
    "\n",
    "    This class loads and processes rule classification data, template data, and example data from specified YAML files.\n",
    "    It is used to generate markdown documentation for a given rule type, including details such as templates and examples.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    data_path : str\n",
    "        Path to the directory containing the YAML files.\n",
    "    template_dict : dict\n",
    "        Dictionary containing template information loaded from the templates YAML file.\n",
    "    examples_dict : dict\n",
    "        Dictionary containing example information loaded from the examples YAML file.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Initializes the RuleInformationProvider with the specified data path.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_path : str\n",
    "            Path to the directory containing the YAML files with rules, templates, and examples.\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.template_dict = self._load_yaml(f'{data_path}/witt_templates.yaml', 'template_list')\n",
    "        self.examples_dict = self._load_yaml(f'{data_path}/witt_examples.yaml', 'example_list')\n",
    "\n",
    "    def _load_yaml(self, file_path, list_key=None):\n",
    "        \"\"\"\n",
    "        Loads data from a YAML file.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        file_path : str\n",
    "            Path to the YAML file to be loaded.\n",
    "        list_key : str, optional\n",
    "            Key used to extract a specific list from the YAML data. If provided, returns a dictionary indexed by 'id'.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            If list_key is provided, returns a dictionary with items indexed by 'id'.\n",
    "        Any type\n",
    "            If list_key is not provided, returns the entire data structure from the YAML file.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = yaml.safe_load(file)\n",
    "            if list_key:\n",
    "                return {item['id']: item for item in data[list_key]}\n",
    "            return data\n",
    "\n",
    "    def get_classification_and_templates(self, section_title):\n",
    "        \"\"\"\n",
    "        Retrieves classification information and templates for a specified rule section.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        section_title : str\n",
    "            Title of the section for which to retrieve information.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A markdown formatted string containing the classification details, templates, and examples for the given section.\n",
    "        \"\"\"\n",
    "        data = self._load_yaml(f'{self.data_path}/classify_subtypes.yaml')\n",
    "        filtered_data = self._filter_sections_by_title(data, section_title)\n",
    "        return self._convert_to_markdown(filtered_data)\n",
    "\n",
    "    def _filter_sections_by_title(self, data, title):\n",
    "        \"\"\"\n",
    "        Filters sections based on the given title.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : list\n",
    "            List of sections to filter from.\n",
    "        title : str\n",
    "            Title to filter sections by.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            A list of sections that match the given title.\n",
    "        \"\"\"\n",
    "        return [section for section in data if section['section_title'] == title]\n",
    "\n",
    "    def _convert_to_markdown(self, filtered_data):\n",
    "        \"\"\"\n",
    "        Converts filtered rule classification data to markdown format.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        filtered_data : list\n",
    "            List of filtered sections to convert into markdown.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A markdown formatted string representing the filtered sections.\n",
    "        \"\"\"\n",
    "        def process_section(section, level=2):\n",
    "            \"\"\"\n",
    "            Processes a section recursively and converts it to markdown format.\n",
    "\n",
    "            Parameters:\n",
    "            -----------\n",
    "            section : dict\n",
    "                The section to process.\n",
    "            level : int, optional\n",
    "                The heading level for the section title in markdown (default is 1).\n",
    "\n",
    "            Returns:\n",
    "            --------\n",
    "            str\n",
    "                A markdown formatted string for the section and its subsections.\n",
    "            \"\"\"\n",
    "            markdown = f\"{'#' * level} {section['section_title']}\\n\\n\"\n",
    "            markdown += f\"**ID**: {section['section_id']}\\n\\n\"\n",
    "            markdown += f\"**Definition**: {section['section_definition']}\\n\\n\"\n",
    "\n",
    "            if 'templates' in section and section['templates']:\n",
    "                for template_id in section['templates']:\n",
    "                    if template_id in self.template_dict:\n",
    "                        template = self.template_dict[template_id]\n",
    "                        markdown += f\"**Template ID**: {template_id}\\n\\n\"\n",
    "                        markdown += f\"**Template Explanation**: {template['explanation']}\\n\\n\"\n",
    "                        markdown += f\"**Template Text**:\\n\\n```template\\n{template['text']}```\\n\\n\"\n",
    "                    else:\n",
    "                        markdown += f\"**Template ID**: {template_id} - No details found.\\n\\n\"\n",
    "            else:\n",
    "                markdown += \"**Templates**: Look in the subsection(s).\\n\\n\"\n",
    "\n",
    "            if 'examples' in section and section['examples']:\n",
    "                for example_id in section['examples']:\n",
    "                    if example_id in self.examples_dict:\n",
    "                        example = self.examples_dict[example_id]\n",
    "                        markdown += f\"**Example ID**: {example_id}\\n\\n\"\n",
    "                        markdown += f\"**Example Text**:\\n\\n```example\\n{example['text']}```\\n\\n\"\n",
    "                    else:\n",
    "                        markdown += f\"**Example ID**: {example_id} - No details found.\\n\\n\"\n",
    "\n",
    "            if 'subsections' in section:\n",
    "                for subsection in section['subsections']:\n",
    "                    markdown += process_section(subsection, level + 1)\n",
    "\n",
    "            return markdown\n",
    "\n",
    "        markdown = \"\"\n",
    "        for section in filtered_data:\n",
    "            markdown += process_section(section)\n",
    "        return markdown\n",
    "\n",
    "class RulesTemplateProvider:\n",
    "    \"\"\"\n",
    "    A class to provide information about rules templates and their relationships from YAML data.\n",
    "\n",
    "    This class loads and processes template data, subtemplate data, and their relationships from specified YAML files.\n",
    "    It is used to extract information about templates and format them into readable output.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    data_directory : Path\n",
    "        Path to the directory containing the YAML files.\n",
    "    data_dicts : dict\n",
    "        Dictionary containing data loaded from YAML files, including templates, subtemplates, and relationships.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_directory):\n",
    "        \"\"\"\n",
    "        Initializes the RulesTemplateProvider with the specified data directory.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_directory : str or Path\n",
    "            Path to the directory containing the YAML files with templates, subtemplates, and relationships.\n",
    "        \"\"\"\n",
    "        self.data_directory = Path(data_directory)\n",
    "        self.data_dicts = self._load_data()\n",
    "\n",
    "    def _load_yaml(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads data from a YAML file.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        file_path : Path\n",
    "            Path to the YAML file to be loaded.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            A dictionary containing the data from the YAML file.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r') as file:\n",
    "            return yaml.safe_load(file) or {}\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "        Loads data from multiple YAML files required for template processing.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            A dictionary containing data from templates, subtemplates, and template relationships YAML files.\n",
    "        \"\"\"\n",
    "        witt_template_relationship_file = self.data_directory / 'witt_template_subtemplate_relationship.yaml'\n",
    "        witt_templates_file = self.data_directory / 'witt_templates.yaml'\n",
    "        witt_subtemplates_file = self.data_directory / 'witt_subtemplates.yaml'\n",
    "\n",
    "        witt_template_relationship_data = self._load_yaml(witt_template_relationship_file).get('template_subtemplate_relationship', {})\n",
    "        witt_templates_data = self._load_yaml(witt_templates_file).get('template_list', [])\n",
    "        witt_subtemplates_data = self._load_yaml(witt_subtemplates_file).get('subtemplate_list', [])\n",
    "\n",
    "        return {\n",
    "            'witt_template_relationship_data': witt_template_relationship_data,\n",
    "            'witt_templates_data': witt_templates_data,\n",
    "            'witt_subtemplates_data': witt_subtemplates_data\n",
    "        }\n",
    "\n",
    "    def _get_template_data(self, template_key, data):\n",
    "        \"\"\"\n",
    "        Retrieves data for a specific template or subtemplate based on its key.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        template_key : str\n",
    "            The key of the template or subtemplate to be retrieved.\n",
    "        data : list or dict\n",
    "            The data to search in, which can be a list of templates or a dictionary of relationships.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict or None\n",
    "            The data corresponding to the specified template key, or None if not found.\n",
    "        \"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            return data.get(template_key, None)\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                if isinstance(item, dict) and item.get('id', '') == template_key:\n",
    "                    return item\n",
    "        return None\n",
    "\n",
    "    def _format_template_output(self, template_key, template_data):\n",
    "        \"\"\"\n",
    "        Formats the output for a given template or subtemplate.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        template_key : str\n",
    "            The key of the template or subtemplate.\n",
    "        template_data : dict\n",
    "            The data of the template or subtemplate to be formatted.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A formatted string representation of the template data.\n",
    "        \"\"\"\n",
<<<<<<< HEAD
    "        output = f\"# {template_key}\\n\\n\"\n",
    "        if not template_data:\n",
    "            output += \"Template data not found.\\n\\n\"\n",
    "            return output\n",
    "        if 'usesSubtemplate' in template_data:\n",
    "            uses_subtemplate = template_data['usesSubtemplate']\n",
    "            if isinstance(uses_subtemplate, list):\n",
    "                uses_subtemplate = ', '.join(uses_subtemplate)\n",
    "            output += f\"## usesSubtemplate\\n{uses_subtemplate}\\n\\n\"\n",
    "        if 'text' in template_data:\n",
    "            output += f\"## text\\n\\n{template_data['text']}\\n\\n\"\n",
    "        if 'explanation' in template_data:\n",
    "            output += f\"## explanation\\n\\n{template_data['explanation']}\\n\\n\"\n",
=======
    "        title = template_data.get('title', '')\n",
    "        output = f\"## {template_key}: {title}\\n\\n\" if title else f\"## {template_key}\\n\\n\"\n",
    "\n",
    "        if not template_data:\n",
    "            output += \"Template data not found.\\n\\n\"\n",
    "            return output\n",
    "\n",
    "        # Format subtemplates in use as a bullet list with titles if available\n",
    "        if 'usesSubtemplate' in template_data:\n",
    "            output += \"### Subtemplate(s) in use\\n\"\n",
    "            subtemplate_list = []\n",
    "            for sub_key in template_data['usesSubtemplate']:\n",
    "                sub_data = self._get_template_data(sub_key, self.data_dicts['witt_subtemplates_data'])\n",
    "                sub_title = sub_data.get('title', '') if sub_data else \"Unknown\"\n",
    "                subtemplate_list.append(f\"- {sub_key}: {sub_title}\")\n",
    "            output += \"\\n\".join(subtemplate_list) + \"\\n\\n\"\n",
    "        \n",
    "        if 'text' in template_data:\n",
    "            output += f\"### Text\\n\\n{template_data['text']}\\n\\n\"\n",
    "        if 'explanation' in template_data:\n",
    "            output += f\"### Explanation\\n\\n{template_data['explanation']}\\n\\n\"\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "        return output\n",
    "\n",
    "    def _process_template(self, template_key, processed_keys=None):\n",
    "        \"\"\"\n",
<<<<<<< HEAD
    "        Processes a template or subtemplate recursively, including any subtemplates used.\n",
=======
    "        Processes a template or subtemplate recursively, including any subtemplates used,\n",
    "        and avoids processing duplicates.\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        template_key : str\n",
    "            The key of the template or subtemplate to be processed.\n",
    "        processed_keys : set, optional\n",
<<<<<<< HEAD
    "            A set of keys that have already been processed to prevent circular references.\n",
=======
    "            A set of keys that have already been processed to prevent duplicate entries.\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
<<<<<<< HEAD
    "            A formatted string representation of the template and its subtemplates.\n",
=======
    "            A formatted string representation of the template and its subtemplates, without duplicates.\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "        \"\"\"\n",
    "        if processed_keys is None:\n",
    "            processed_keys = set()\n",
    "\n",
<<<<<<< HEAD
=======
    "        # If this template or subtemplate has already been processed, skip it\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "        if template_key in processed_keys:\n",
    "            return ''\n",
    "        processed_keys.add(template_key)\n",
    "\n",
    "        template_data = None\n",
    "\n",
<<<<<<< HEAD
=======
    "        # Determine whether it's a main template or subtemplate based on key prefix\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "        if template_key.startswith('T'):\n",
    "            template_data = self._get_template_data(template_key, self.data_dicts['witt_templates_data']) or {}\n",
    "            uses_subtemplate = self._get_template_data(template_key, self.data_dicts['witt_template_relationship_data'])\n",
    "            if uses_subtemplate:\n",
    "                template_data['usesSubtemplate'] = uses_subtemplate if isinstance(uses_subtemplate, list) else [uses_subtemplate]\n",
    "        elif template_key.startswith('S'):\n",
    "            template_data = self._get_template_data(template_key, self.data_dicts['witt_subtemplates_data']) or {}\n",
    "            uses_subtemplate = self._get_template_data(template_key, self.data_dicts['witt_template_relationship_data'])\n",
    "            if uses_subtemplate:\n",
    "                template_data['usesSubtemplate'] = uses_subtemplate if isinstance(uses_subtemplate, list) else [uses_subtemplate]\n",
    "\n",
    "        if not template_data:\n",
    "            return f\"# {template_key}\\n\\nTemplate data not found.\\n\\n\"\n",
    "\n",
    "        output = self._format_template_output(template_key, template_data)\n",
    "\n",
<<<<<<< HEAD
=======
    "        # Process each subtemplate recursively, avoiding duplicates\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "        if 'usesSubtemplate' in template_data:\n",
    "            subtemplate_keys = template_data['usesSubtemplate']\n",
    "            subtemplate_keys = [subtemplate_keys] if isinstance(subtemplate_keys, str) else subtemplate_keys\n",
    "            for sub_key in subtemplate_keys:\n",
    "                sub_key = sub_key.strip()\n",
    "                output += self._process_template(sub_key, processed_keys)\n",
    "\n",
    "        return output\n",
    "\n",
<<<<<<< HEAD
    "    def get_rules_template(self, template_key):\n",
    "        \"\"\"\n",
    "        Retrieves the formatted rules template for the specified template key.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        template_key : str\n",
    "            The key of the template to be retrieved.\n",
=======
    "    def get_rules_template(self, template_keys):\n",
    "        \"\"\"\n",
    "        Retrieves the formatted rules templates for the specified list of template keys, avoiding duplicate subtemplates.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        template_keys : str or list of str\n",
    "            The key(s) of the template(s) to be retrieved. Can be a single string key or a list of keys.\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
<<<<<<< HEAD
    "            A formatted string representation of the template and its associated subtemplates.\n",
    "        \"\"\"\n",
    "        return self._process_template(template_key)\n",
=======
    "            A formatted string representation of each template and its associated subtemplates, without duplicates.\n",
    "        \"\"\"\n",
    "        # If a single template key is provided as a string, convert it to a list\n",
    "        if isinstance(template_keys, str):\n",
    "            template_keys = [template_keys]\n",
    "\n",
    "        output = \"\"\n",
    "        processed_subtemplates = set()  # Track processed templates and subtemplates to avoid duplicates\n",
    "\n",
    "        for template_key in template_keys:\n",
    "            output += self._process_template(template_key, processed_keys=processed_subtemplates)\n",
    "            output += \"\\n\\n\"  # Separate each template with extra newlines for readability\n",
    "\n",
    "        return output\n",
>>>>>>> 3ee09f8 (add chap 7 elements validation)
    "\n",
    "# Example usage:\n",
    "# rule_information_provider = RuleInformationProvider(\"../data\")\n",
    "# markdown_data = rule_provider.get_classification_and_templates(\"Data rules\")\n",
    "# print(markdown_data)\n",
    "\n",
    "# rule_template_provider = RulesTemplateProvider(\"../data\")\n",
    "# markdown_data = processor.get_rules_template(\"T7\")\n",
    "# print(markdown_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llm_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir llm_query && touch llm_query/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llm_query/main.py\n",
    "\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import instructor\n",
    "import logging\n",
    "from typing import Any\n",
    "\n",
    "# Set up basic logging configuration for the checkpoint module\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to INFO or another level as needed\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Log format\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def measure_time(func):\n",
    "    \"\"\"\n",
    "    Decorator to measure the execution time of a function.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed_time = end_time - start_time\n",
    "        logger.info(f\"Execution time for {func.__name__}: {elapsed_time:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@measure_time\n",
    "def query_instruct_llm(system_prompt: str,\n",
    "                        user_prompt: str,\n",
    "                        llm_model: str,\n",
    "                        document_model: Any,\n",
    "                        temperature: float,\n",
    "                        max_tokens: int) -> Any:\n",
    "    \"\"\"\n",
    "    Queries the LLM with the given system and user prompts.\n",
    "\n",
    "    Args:\n",
    "        system_prompt (str): The system prompt to set the context for the LLM.\n",
    "        user_prompt (str): The user prompt containing the text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        Any: The response from the LLM, parsed into a document_model object.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the API call fails.\n",
    "    \"\"\"\n",
    "    client = instructor.from_openai(OpenAI())\n",
    "    resp = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        response_model=document_model,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "    )\n",
    "    return resp\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipt-cfr2sbvr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
