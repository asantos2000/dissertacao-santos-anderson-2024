{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFR2SBVR Modules\n",
    "\n",
    "Supporting modules for the chapters 6, 7 of the dissertation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir configuration && touch configuration/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile configuration/main.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import glob\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "DEFAULT_CONFIG_DIR: str = '../config.yaml'  # Google drive: \"/content/drive/MyDrive/cfr2sbvr/config.yaml\"\n",
    "\n",
    "def _get_sorted_file_info(file_dir: str, file_prefix: str, extension: str):\n",
    "    \"\"\"\n",
    "    Helper function to retrieve and sort file information based on a specific prefix and extension.\n",
    "\n",
    "    Args:\n",
    "        file_dir (str): Directory to search for files.\n",
    "        file_prefix (str): Prefix for the filenames.\n",
    "        extension (str): File extension.\n",
    "\n",
    "    Returns:\n",
    "        list: Sorted list of file information dictionaries containing 'filename', 'date', and 'number' keys.\n",
    "    \"\"\"\n",
    "    path = Path(file_dir)\n",
    "    #path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = list(path.glob(f\"{file_prefix}-*.{extension}\"))\n",
    "    file_info_list = []\n",
    "\n",
    "    pattern = re.compile(rf'^{file_prefix}-(\\d{{4}}-\\d{{2}}-\\d{{2}})-(\\d+)\\.{extension}$')\n",
    "    for filepath in files:\n",
    "        match = pattern.match(filepath.name)\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            number = int(match.group(2))\n",
    "            file_info_list.append({'filename': filepath.name, 'date': date_str, 'number': number})\n",
    "\n",
    "    return sorted(file_info_list, key=lambda x: (x['date'], x['number']), reverse=True)\n",
    "\n",
    "def get_next_filename(file_dir: str, file_prefix: str, extension: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates the next filename in a sequence based on existing files in a directory,\n",
    "    considering the file extension.\n",
    "\n",
    "    The filename format is: `{file_prefix}-{YYYY-MM-DD}-{N}.{extension}`,\n",
    "    where `N` is an incrementing integer for files with the same date.\n",
    "    \"\"\"\n",
    "    today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "    sorted_files = _get_sorted_file_info(file_dir, file_prefix, extension)\n",
    "\n",
    "    if sorted_files and sorted_files[0]['date'] == today_str:\n",
    "        new_number = sorted_files[0]['number'] + 1\n",
    "    else:\n",
    "        new_number = 1\n",
    "\n",
    "    new_filename = f'{file_prefix}-{today_str}-{new_number}.{extension}'\n",
    "    return str(Path(file_dir) / new_filename)\n",
    "\n",
    "def get_last_filename(file_dir: str, file_prefix: str, extension: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves the most recent filename based on the highest date and sequence number\n",
    "    for files with a specific prefix and extension in the specified directory.\n",
    "    \"\"\"\n",
    "    sorted_files = _get_sorted_file_info(file_dir, file_prefix, extension)\n",
    "    if sorted_files:\n",
    "        return str(Path(file_dir) / sorted_files[0]['filename'])\n",
    "    return None\n",
    "\n",
    "# Load the YAML config file\n",
    "def load_config(config_file: str = None):\n",
    "    if config_file is None:\n",
    "        config_file = DEFAULT_CONFIG_DIR\n",
    "    try:\n",
    "        with open(config_file, \"r\") as file:\n",
    "            config = yaml.safe_load(file)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Configuration file {config_file} not found.\")\n",
    "    except yaml.YAMLError as exc:\n",
    "        raise ValueError(f\"Error parsing YAML file {config_file}: {exc}\")\n",
    "\n",
    "    # Ensure config structure is correct\n",
    "    if \"LLM\" not in config or \"DEFAULT_CHECKPOINT_DIR\" not in config:\n",
    "        raise ValueError(\"Required configuration keys are missing in the config file.\")\n",
    "\n",
    "    # Set the OpenAI API key from environment variable if it's not set in config\n",
    "    config[\"LLM\"][\"OPENAI_API_KEY\"] = os.getenv(\n",
    "        \"OPENAI_API_KEY\", config[\"LLM\"].get(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "\n",
    "    # Dynamically set checkpoint and report files using the get_next_filename function\n",
    "    config[\"DEFAULT_CHECKPOINT_FILE\"] = get_next_filename(\n",
    "        config[\"DEFAULT_CHECKPOINT_DIR\"], \"documents\", \"json\"\n",
    "    )\n",
    "    config[\"DEFAULT_EXTRACTION_REPORT_FILE\"] = get_next_filename(\n",
    "        config[\"DEFAULT_OUTPUT_DIR\"], \"extraction_report\", \"html\"\n",
    "    )\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  # backup on Google Drive\n",
    "  !cp -r configuration /content/drive/MyDrive/cfr2sbvr/modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configuration.main as configuration\n",
    "\n",
    "# Development mode\n",
    "import importlib\n",
    "importlib.reload(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configuration.load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "Checkpoints are stored / retrieved at the directory `DEFAULT_CHECKPOINT_FILE` in the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir checkpoint && touch checkpoint/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting checkpoint/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile checkpoint/main.py\n",
    "\n",
    "from typing import List, Dict, Optional, Any, Tuple, Set\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "from pathlib import Path\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Set up basic logging configuration for the checkpoint module\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to INFO or another level as needed\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Log format\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def convert_set_to_list(data: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Recursively converts sets to lists in the data structure.\n",
    "\n",
    "    Args:\n",
    "        data (Any): The data structure to process, which can be a dict, list, set, or other types.\n",
    "\n",
    "    Returns:\n",
    "        Any: The data structure with all sets converted to lists.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {key: convert_set_to_list(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_set_to_list(item) for item in data]\n",
    "    elif isinstance(data, set):\n",
    "        return list(data)\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n",
    "def normalize_str(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a string using Unicode normalization to ensure consistent representation.\n",
    "\n",
    "    Args:\n",
    "        s (str): The string to normalize.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized string.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize(\"NFKD\", s).strip()\n",
    "\n",
    "\n",
    "# Define a model for the Document\n",
    "class Document(BaseModel):\n",
    "    id: str\n",
    "    type: str  # New field to represent the type of the document\n",
    "    content: Any  # Content can be any data type: list, dict, string, etc.\n",
    "    elapsed_times: Optional[list[float]] = None  # Optional field for elapsed time\n",
    "    completions: Optional[list[Dict]] = None  # Optional field for completion status\n",
    "\n",
    "# Define the DocumentManager class\n",
    "class DocumentManager(BaseModel):\n",
    "    documents: Dict[Tuple[str, str], Document] = Field(\n",
    "        default_factory=dict\n",
    "    )  # Keys are tuples (id, type)\n",
    "\n",
    "    def add_document(self, doc: Document) -> None:\n",
    "        \"\"\"\n",
    "        Adds a document to the manager.\n",
    "\n",
    "        Args:\n",
    "            doc (Document): The document to add.\n",
    "        \"\"\"\n",
    "        # Normalize the document ID and type to avoid inconsistencies\n",
    "        normalized_id = normalize_str(doc.id)\n",
    "        normalized_type = normalize_str(doc.type)\n",
    "\n",
    "        key = (normalized_id, normalized_type)\n",
    "\n",
    "        # Debug logging for added document keys\n",
    "        logger.debug(f\"Adding document with normalized key: {key}\")\n",
    "\n",
    "        self.documents[key] = doc\n",
    "\n",
    "    def retrieve_document(self, doc_id: str, doc_type: str) -> Optional[Document]:\n",
    "        \"\"\"\n",
    "        Retrieves a document by its id and type.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): The ID of the document.\n",
    "            doc_type (str): The type of the document.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Document]: The retrieved document, or None if not found.\n",
    "        \"\"\"\n",
    "        # Normalize the identifiers before using them to access the dictionary\n",
    "        normalized_id = normalize_str(doc_id)\n",
    "        normalized_type = normalize_str(doc_type)\n",
    "\n",
    "        key = (normalized_id, normalized_type)\n",
    "\n",
    "        # Debug logging for retrieval attempt\n",
    "        logger.debug(f\"Retrieving document with key: {key}\")\n",
    "\n",
    "        return self.documents.get(key)\n",
    "\n",
    "    def list_document_ids(self, doc_type: Optional[str] = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Lists all document ids, optionally filtered by type.\n",
    "\n",
    "        Args:\n",
    "            doc_type (Optional[str], optional): The type of documents to list. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of document ids.\n",
    "        \"\"\"\n",
    "        if doc_type:\n",
    "            normalized_type = normalize_str(doc_type)\n",
    "            logger.debug(f\"Listing documents filtered by type: {normalized_type}\")\n",
    "            return [\n",
    "                doc_id\n",
    "                for (doc_id, d_type) in self.documents.keys()\n",
    "                if d_type == normalized_type\n",
    "            ]\n",
    "        else:\n",
    "            logger.debug(\"Listing all documents without type filter.\")\n",
    "            return [doc_id for (doc_id, _) in self.documents.keys()]\n",
    "\n",
    "    def exclude_document(self, doc_id: str, doc_type: str) -> None:\n",
    "        \"\"\"\n",
    "        Excludes a document by its id and type.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): The ID of the document to exclude.\n",
    "            doc_type (str): The type of the document.\n",
    "        \"\"\"\n",
    "        # Normalize identifiers before attempting exclusion\n",
    "        normalized_id = normalize_str(doc_id)\n",
    "        normalized_type = normalize_str(doc_type)\n",
    "\n",
    "        key = (normalized_id, normalized_type)\n",
    "\n",
    "        if key in self.documents:\n",
    "            logger.debug(f\"Excluding document with key: {key}\")\n",
    "            del self.documents[key]\n",
    "\n",
    "    def persist_to_file(self, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Persists the current state to a file, converting tuple keys to strings and sets to lists.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The filename to save the documents.\n",
    "        \"\"\"\n",
    "        serializable_documents = {\n",
    "            f\"{doc_id}|{doc_type}\": convert_set_to_list(doc.dict())\n",
    "            for (doc_id, doc_type), doc in self.documents.items()\n",
    "        }\n",
    "        with open(filename, \"w\") as file:\n",
    "            json.dump(serializable_documents, file, indent=4)\n",
    "        logger.info(f\"DocumentManager state persisted to file: {filename}\")\n",
    "\n",
    "    @classmethod\n",
    "    def restore_from_file(cls, filename: str) -> \"DocumentManager\":\n",
    "        \"\"\"\n",
    "        Restores the state from a file, converting string keys back to tuples.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The filename to restore the documents from.\n",
    "\n",
    "        Returns:\n",
    "            DocumentManager: The restored DocumentManager instance.\n",
    "        \"\"\"\n",
    "        with open(filename, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "            documents = {\n",
    "                (doc_id.split(\"|\")[0], doc_id.split(\"|\")[1]): Document(**doc_data)\n",
    "                for doc_id, doc_data in data.items()\n",
    "            }\n",
    "            logger.info(f\"DocumentManager restored from file: {filename}\")\n",
    "            return cls(documents=documents)\n",
    "\n",
    "\n",
    "def restore_checkpoint(filename: Optional[str]) -> DocumentManager:\n",
    "    \"\"\"\n",
    "    Restores the document manager from a checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        filename (str, optional): The path to the checkpoint file. Defaults to DEFAULT_CHECKPOINT_FILE.\n",
    "\n",
    "    Returns:\n",
    "        DocumentManager: The restored DocumentManager instance.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the checkpoint file does not exist.\n",
    "\n",
    "    See Also:\n",
    "        - Reset the values delete the documents.json file and run: manager = DocumentManager()\n",
    "        - Restore the state from the documents.json file, run: DocumentManager.restore_from_file(\"documents.json\")\n",
    "        - Exclue a document: manager.exclude_document(doc_id=\"§ 275.0-2\", doc_type=\"section\")\n",
    "        - List documents: manager.list_document_ids(doc_type=\"section\")\n",
    "        - Get a document: manager.retrieve_document(doc_id=doc, doc_type=\"section\")\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        restored_docs = DocumentManager.restore_from_file(filename)\n",
    "        logger.info(f\"Checkpoint restored from {filename}.\")\n",
    "    except (FileNotFoundError, JSONDecodeError):\n",
    "        restored_docs = DocumentManager()\n",
    "        logger.error(\n",
    "            f\"Checkpoint file '{filename}' not found or is empty, initializing new checkpoint.\"\n",
    "        )\n",
    "    return restored_docs\n",
    "\n",
    "\n",
    "def save_checkpoint(filename: Optional[str], manager: DocumentManager) -> None:\n",
    "    \"\"\"\n",
    "    Saves the current state of the DocumentManager to a checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        manager (DocumentManager): The DocumentManager instance to save.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error saving the checkpoint.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        manager.persist_to_file(filename=filename)\n",
    "        logger.info(\"Checkpoint saved.\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(\n",
    "            \"Error saving checkpoint. Check the directory path and permissions.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_all_checkpoints(checkpoint_dir, prefix=\"documents\", extension=\"json\"):\n",
    "    managers = []\n",
    "\n",
    "    path = Path(checkpoint_dir)\n",
    "\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = list(path.glob(f\"{prefix}-*.{extension}\"))\n",
    "    file_info_list = []\n",
    "\n",
    "    pattern = re.compile(rf\"^{prefix}-(\\d{{4}}-\\d{{2}}-\\d{{2}})-(\\d+)\\.{extension}$\")\n",
    "    for filepath in files:\n",
    "        match = pattern.match(filepath.name)\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            number = int(match.group(2))\n",
    "            file_info_list.append(\n",
    "                {\"filename\": filepath.name, \"date\": date_str, \"number\": number}\n",
    "            )\n",
    "\n",
    "            logger.debug(f\"filepath: {filepath}\")\n",
    "            managers.append(DocumentManager.restore_from_file(filepath))\n",
    "\n",
    "    return managers, file_info_list\n",
    "\n",
    "\n",
    "def get_elements_from_checkpoints(checkpoint_dir, merge=True, filter=\"non_null\"):\n",
    "    managers, file_info_list = get_all_checkpoints(checkpoint_dir)\n",
    "\n",
    "    pred_operative_rules = []\n",
    "    pred_facts = []\n",
    "    pred_terms = []\n",
    "    pred_names = []\n",
    "    pred_files = []\n",
    "\n",
    "    for manager, file_info in zip(managers, file_info_list):\n",
    "        # Process documents\n",
    "        processor = DocumentProcessor(manager, merge=merge)\n",
    "\n",
    "        # Access processed data\n",
    "        # unique_terms = processor.get_unique_terms()\n",
    "        # unique_names = processor.get_unique_names()\n",
    "        pred_operative_rules += processor.get_rules()\n",
    "        pred_facts += processor.get_facts()\n",
    "        pred_terms += processor.get_terms(definition_filter=filter)\n",
    "        pred_names += processor.get_names(definition_filter=filter)\n",
    "        pred_files.append(file_info)\n",
    "\n",
    "    logger.debug(f\"Rules: {pred_operative_rules}\")\n",
    "    logger.debug(f\"Facts: {pred_facts}\")\n",
    "    logger.debug(f\"Terms: {pred_terms}\")\n",
    "    logger.debug(f\"Names: {pred_names}\")\n",
    "    logger.info(f\"Rules to evaluate: {len(pred_operative_rules)}\")\n",
    "    logger.info(f\"Facts to evaluate: {len(pred_facts)}\")\n",
    "    logger.info(f\"Terms to evaluate: {len(pred_terms)}\")\n",
    "    logger.info(f\"Names to evaluate: {len(pred_names)}\")\n",
    "\n",
    "    return pred_operative_rules, pred_facts, pred_terms, pred_names, pred_files\n",
    "\n",
    "\n",
    "def get_true_table_keys():\n",
    "    return [\n",
    "        \"classify_P1|true_table\",\n",
    "        \"classify_P2_Definitional_facts|true_table\",\n",
    "        \"classify_P2_Definitional_names|true_table\",\n",
    "        \"classify_P2_Definitional_terms|true_table\",\n",
    "        \"classify_P2_Operative_rules|true_table\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_elements_from_true_tables(data_dir):\n",
    "    true_table_file = f\"{data_dir}/documents_true_table.json\"\n",
    "    true_table_keys = get_true_table_keys()\n",
    "\n",
    "    true_operative_rules_p1 = []\n",
    "    true_facts_p2 = []\n",
    "    true_names_p2 = []\n",
    "    true_terms_p2 = []\n",
    "    true_operative_rules_p2 = []\n",
    "\n",
    "    manager_true_elements = restore_checkpoint(true_table_file)\n",
    "\n",
    "    for key in true_table_keys:\n",
    "        match key:\n",
    "            case \"classify_P1|true_table\":\n",
    "                true_operative_rules_p1 = manager_true_elements.retrieve_document(\n",
    "                    \"classify_P1\", \"true_table\"\n",
    "                ).content\n",
    "                logger.debug(f\"P1: True Operative Rules: {true_operative_rules_p1}\")\n",
    "                logger.info(\n",
    "                    f\"P1: Operative Rules to evaluate: {len(true_operative_rules_p1)}\"\n",
    "                )\n",
    "            case \"classify_P2_Definitional_facts|true_table\":\n",
    "                true_facts_p2 = manager_true_elements.retrieve_document(\n",
    "                    \"classify_P2_Definitional_facts\", \"true_table\"\n",
    "                ).content\n",
    "                logger.debug(f\"P2: True Facts: {true_facts_p2}\")\n",
    "                logger.info(f\"P2: Facts to evaluate: {len(true_facts_p2)}\")\n",
    "            case \"classify_P2_Definitional_names|true_table\":\n",
    "                true_names_p2 = manager_true_elements.retrieve_document(\n",
    "                    \"classify_P2_Definitional_names\", \"true_table\"\n",
    "                ).content\n",
    "                logger.debug(f\"P2: True Names: {true_names_p2}\")\n",
    "                logger.info(f\"P2: Names to evaluate: {len(true_names_p2)}\")\n",
    "            case \"classify_P2_Definitional_terms|true_table\":\n",
    "                true_terms_p2 = manager_true_elements.retrieve_document(\n",
    "                    \"classify_P2_Definitional_terms\", \"true_table\"\n",
    "                ).content\n",
    "                logger.debug(f\"P2: True Terms: {true_terms_p2}\")\n",
    "                logger.info(f\"P2: Terms to evaluate: {len(true_terms_p2)}\")\n",
    "            case \"classify_P2_Operative_rules|true_table\":\n",
    "                true_operative_rules_p2 = manager_true_elements.retrieve_document(\n",
    "                    \"classify_P2_Operative_rules\", \"true_table\"\n",
    "                ).content\n",
    "                logger.debug(f\"P2: True Operative Rules: {true_operative_rules_p2}\")\n",
    "                logger.info(\n",
    "                    f\"P2: Operative Rules to evaluate: {len(true_operative_rules_p2)}\"\n",
    "                )\n",
    "\n",
    "    return (\n",
    "        true_operative_rules_p1,\n",
    "        true_facts_p2,\n",
    "        true_names_p2,\n",
    "        true_terms_p2,\n",
    "        true_operative_rules_p2,\n",
    "    )\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    DocumentProcessor is responsible for processing documents and categorizing elements such as terms, names, facts, and rules.\n",
    "\n",
    "    Attributes:\n",
    "        manager: Object used to manage document retrieval.\n",
    "        elements_terms_set (set): Set of unique terms found in the documents.\n",
    "        elements_names_set (set): Set of unique names found in the documents.\n",
    "        elements_terms (list): List of detailed information about terms.\n",
    "        elements_names (list): List of detailed information about names.\n",
    "        elements_facts (list): List of facts extracted from documents.\n",
    "        elements_rules (list): List of rules extracted from documents.\n",
    "        elements_terms_definition (dict): Dictionary to store terms definitions by document ID.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, manager: DocumentManager, merge: bool = False):\n",
    "        self.manager = manager\n",
    "        self.elements_terms_set = set()\n",
    "        self.elements_names_set = set()\n",
    "        self.elements_terms = []\n",
    "        self.elements_names = []\n",
    "        self.elements_facts = []\n",
    "        self.elements_rules = []\n",
    "        self.elements_terms_definition = {}\n",
    "        self.operative_rules_classifications = (\n",
    "            []\n",
    "        )  # To store classifications with type and subtype for operative rules\n",
    "        self.facts_classifications = (\n",
    "            []\n",
    "        )  # To store classifications with type and subtype for facts\n",
    "        self.terms_classifications = (\n",
    "            []\n",
    "        )  # To store classifications with type, subtype, and confidence for definitional terms\n",
    "        self.names_classifications = (\n",
    "            []\n",
    "        )  # To store classifications with type, subtype, and confidence for definitional names\n",
    "\n",
    "        # Automatically process definitions, classifications, and elements when instantiated\n",
    "        try:\n",
    "            self.process_definitions()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing definitions: {e}\")\n",
    "\n",
    "        try:\n",
    "            self.process_operative_rules_classifications()\n",
    "        except Exception as e:\n",
    "            logger.info(\n",
    "                f\"Document did not have operative rules classifications to process: {e}\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            self.process_facts_classifications()\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Document did not have facts classifications to process: {e}\")\n",
    "\n",
    "        try:\n",
    "            self.process_terms_classifications()\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Document did not have terms classifications to process: {e}\")\n",
    "\n",
    "        try:\n",
    "            self.process_names_classifications()\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Document did not have names classifications to process: {e}\")\n",
    "\n",
    "        try:\n",
    "            self.process_elements()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing elements: {e}\")\n",
    "            raise e\n",
    "\n",
    "        # Conditionally merge elements if `merge` is True\n",
    "        if merge:\n",
    "            try:\n",
    "                self.merge_terms()\n",
    "                self.merge_names()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error merging elements: {e}\")\n",
    "                raise e\n",
    "            \n",
    "        try:\n",
    "            self.process_transformed_elements()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing transformed elements: {e}\")\n",
    "            raise e\n",
    "\n",
    "        try:\n",
    "            self.process_validations()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing validations: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def process_validations(self):\n",
    "        \"\"\"\n",
    "        Processes validation documents and updates elements with validation scores and findings.\n",
    "        \"\"\"\n",
    "        validation_docs = {\n",
    "            \"validation_judge_Operative_Rules\": self.elements_rules,\n",
    "            \"validation_judge_Fact_Types\": self.elements_facts,\n",
    "            \"validation_judge_Terms\": self.elements_terms,\n",
    "            \"validation_judge_Names\": self.elements_names,\n",
    "        }\n",
    "\n",
    "        for doc_name, elements_list in validation_docs.items():\n",
    "            try:\n",
    "                # Retrieve the validation document\n",
    "                validation_doc = self.manager.retrieve_document(doc_name, \"llm_validation\")\n",
    "                if not validation_doc or not validation_doc.content:\n",
    "                    logger.warning(f\"Validation document '{doc_name}' not found or empty.\")\n",
    "                    continue\n",
    "\n",
    "                # Iterate over items in the validation document\n",
    "                for item in validation_doc.content:\n",
    "                    doc_id = normalize_str(item.get(\"doc_id\"))\n",
    "                    statement_id = normalize_str(str(item.get(\"statement_id\")))\n",
    "                    sources = item.get(\"sources\")\n",
    "\n",
    "                    # Fields to add\n",
    "                    semscore = item.get(\"semscore\")\n",
    "                    similarity_score = item.get(\"similarity_score\")\n",
    "                    similarity_score_confidence = item.get(\"similarity_score_confidence\")\n",
    "                    transformation_accuracy = item.get(\"transformation_accuracy\")\n",
    "                    grammar_syntax_accuracy = item.get(\"grammar_syntax_accuracy\")\n",
    "                    findings = item.get(\"findings\")\n",
    "\n",
    "                    # Find matching element in elements_list\n",
    "                    for element in elements_list:\n",
    "                        if (\n",
    "                            normalize_str(element.get(\"doc_id\")) == doc_id\n",
    "                            and normalize_str(str(element.get(\"statement_id\"))) == statement_id\n",
    "                            and set(element[\"sources\"]) == set(sources) \n",
    "                        ):\n",
    "                            # Update element with new fields\n",
    "                            element.update({\n",
    "                                \"semscore\": semscore,\n",
    "                                \"similarity_score\": similarity_score,\n",
    "                                \"similarity_score_confidence\": similarity_score_confidence,\n",
    "                                \"transformation_accuracy\": transformation_accuracy,\n",
    "                                \"grammar_syntax_accuracy\": grammar_syntax_accuracy,\n",
    "                                \"findings\": findings,\n",
    "                            })\n",
    "                            break  # Exit the loop after finding the matching element\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing validation document '{doc_name}': {e}\")\n",
    "\n",
    "    def _update_element(self, elements_list, doc_id, statement_id, sources, transformed):\n",
    "        \"\"\"\n",
    "        Updates the transformed attribute for the matching element in the given list.\n",
    "        \"\"\"\n",
    "        for element in elements_list:\n",
    "            logger.debug(f'{doc_id}, {statement_id}, {sources} == {normalize_str(element[\"doc_id\"])}, {str(element[\"statement_id\"])}, {element[\"sources\"]}')\n",
    "            if (\n",
    "                normalize_str(element[\"doc_id\"]) == doc_id\n",
    "                and normalize_str(str(element[\"statement_id\"])) == statement_id\n",
    "                and set(element[\"sources\"]) == set(sources)  # Updated comparison for list\n",
    "            ):  \n",
    "                logger.debug('MATCH')\n",
    "                element[\"transformed\"] = transformed\n",
    "                logger.debug(f\"Updated element: {element}\")\n",
    "                break\n",
    "\n",
    "    def process_transformed_elements(self):\n",
    "        \"\"\"\n",
    "        Processes the transformed elements from the llm_response_transform document types and updates the transformed attribute in each relevant list.\n",
    "        \"\"\"\n",
    "        transform_docs = {\n",
    "            \"transform_Fact_Types\": self.elements_facts,\n",
    "            \"transform_Terms\": self.elements_terms,\n",
    "            \"transform_Names\": self.elements_names,\n",
    "            \"transform_Operative_Rules\": self.elements_rules,\n",
    "        }\n",
    "\n",
    "        for transform_doc_id, elements_list in transform_docs.items():\n",
    "            transform_doc = self.manager.retrieve_document(transform_doc_id, \"llm_response_transform\")\n",
    "\n",
    "            if transform_doc is None or not transform_doc.content:\n",
    "                logger.warning(f\"Document '{transform_doc_id}' of type 'llm_response_transform' not found or empty.\")\n",
    "                continue  # Skip this document and move to the next iteration\n",
    "\n",
    "            transform_doc_content = transform_doc.content\n",
    "\n",
    "            for item in transform_doc_content:\n",
    "                logger.debug(f\"{item=}\")\n",
    "                doc_id = normalize_str(item.get(\"doc_id\"))\n",
    "                statement_id = normalize_str(str(item.get(\"statement_id\")))\n",
    "                sources = item.get(\"statement_sources\")\n",
    "                transformed = item.get(\"transformed\")\n",
    "\n",
    "                logger.debug(f\"doc_id: {doc_id} - statement_id: {statement_id} - transformed: {transformed}\")\n",
    "                self._update_element(elements_list, doc_id, statement_id, sources, transformed)\n",
    "\n",
    "\n",
    "    def process_names_classifications(self):\n",
    "        \"\"\"\n",
    "        Processes classification information specifically for names from 'classify_P2_Definitional_names'\n",
    "        document and stores the type, subtype, subtype confidence, and subtype explanation.\n",
    "        The type is always set to 'Definitional'.\n",
    "        \"\"\"\n",
    "        # Document identifier we are interested in\n",
    "        doc_classification = \"classify_P2_Definitional_names\"\n",
    "\n",
    "        # Retrieve document content\n",
    "        doc = self.manager.retrieve_document(\n",
    "            doc_classification, \"llm_response_classification\"\n",
    "        )\n",
    "        if not doc or not doc.content:\n",
    "            logger.warning(\n",
    "                f\"Document '{doc_classification}' not found or has empty content.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        doc_content = doc.content\n",
    "\n",
    "        # Iterate over each item in the document content\n",
    "        for item in doc_content:\n",
    "            doc_id = normalize_str(item.get(\"doc_id\"))\n",
    "            statement_id = normalize_str(str(item.get(\"statement_id\")))\n",
    "            classifications = item.get(\"classification\", [])\n",
    "\n",
    "            # Iterate over each classification to extract the highest confidence one for names\n",
    "            for classification in classifications:\n",
    "                confidence = classification.get(\"confidence\", 0)\n",
    "\n",
    "                # Check if this classification already exists in names_classifications\n",
    "                existing_name = next(\n",
    "                    (\n",
    "                        name\n",
    "                        for name in self.names_classifications\n",
    "                        if name[\"doc_id\"] == doc_id\n",
    "                        and name[\"statement_id\"] == statement_id\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                # Initialize if not found\n",
    "                if existing_name is None:\n",
    "                    existing_name = {\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"statement_id\": statement_id,\n",
    "                        \"type\": \"Definitional\",  # Set type as \"Definitional\"\n",
    "                        \"subtype\": None,\n",
    "                        \"confidence\": -1,\n",
    "                        \"explanation\": \"\",\n",
    "                        \"templates_ids\": [],\n",
    "                    }\n",
    "                    self.names_classifications.append(existing_name)\n",
    "\n",
    "                # Update subtype information if the confidence is higher than the existing one\n",
    "                if confidence > existing_name[\"confidence\"]:\n",
    "                    existing_name[\"subtype\"] = classification.get(\"subtype\")\n",
    "                    existing_name[\"confidence\"] = confidence\n",
    "                    existing_name[\"explanation\"] = classification.get(\"explanation\", \"\")\n",
    "                    existing_name[\"templates_ids\"] = classification.get(\n",
    "                        \"templates_ids\", []\n",
    "                    )\n",
    "\n",
    "        # Log the final classification for debugging purposes\n",
    "        logger.debug(f\"{self.names_classifications=}\")\n",
    "\n",
    "    def process_facts_classifications(self):\n",
    "        \"\"\"\n",
    "        Processes classification information specifically for facts from 'classify_P2_Definitional_facts'\n",
    "        document and stores the type, subtype, subtype confidence, and subtype explanation.\n",
    "        The type is always set to 'Definitional'.\n",
    "        \"\"\"\n",
    "        # Document identifier we are interested in\n",
    "        doc_classification = \"classify_P2_Definitional_facts\"\n",
    "\n",
    "        # Retrieve document content\n",
    "        doc_content = self.manager.retrieve_document(\n",
    "            doc_classification, \"llm_response_classification\"\n",
    "        ).content\n",
    "\n",
    "        # Iterate over each item in the document content\n",
    "        for item in doc_content:\n",
    "            doc_id = normalize_str(item.get(\"doc_id\"))\n",
    "            statement_id = normalize_str(str(item.get(\"statement_id\")))\n",
    "            classifications = item.get(\"classification\", [])\n",
    "\n",
    "            # Iterate over each classification to extract the highest confidence one for facts\n",
    "            for classification in classifications:\n",
    "                confidence = classification.get(\"confidence\", 0)\n",
    "\n",
    "                # Check if this classification already exists in facts_classifications\n",
    "                existing_fact = next(\n",
    "                    (\n",
    "                        fact\n",
    "                        for fact in self.facts_classifications\n",
    "                        if fact[\"doc_id\"] == doc_id\n",
    "                        and fact[\"statement_id\"] == statement_id\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                # Initialize if not found\n",
    "                if existing_fact is None:\n",
    "                    existing_fact = {\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"statement_id\": statement_id,\n",
    "                        \"type\": \"Definitional\",  # Set type as \"Definitional\"\n",
    "                        \"subtype\": None,\n",
    "                        \"subtype_confidence\": -1,\n",
    "                        \"subtype_explanation\": \"\",\n",
    "                        \"templates_ids\": [],\n",
    "                    }\n",
    "                    self.facts_classifications.append(existing_fact)\n",
    "\n",
    "                # Update subtype information if the confidence is higher than the existing one\n",
    "                if confidence > existing_fact[\"subtype_confidence\"]:\n",
    "                    existing_fact[\"subtype\"] = classification.get(\"subtype\")\n",
    "                    existing_fact[\"subtype_confidence\"] = confidence\n",
    "                    existing_fact[\"subtype_explanation\"] = classification.get(\n",
    "                        \"explanation\", \"\"\n",
    "                    )\n",
    "                    existing_fact[\"templates_ids\"] = classification.get(\n",
    "                        \"templates_ids\", []\n",
    "                    )\n",
    "\n",
    "        # Log the final classification for debugging purposes\n",
    "        logger.debug(f\"{self.facts_classifications=}\")\n",
    "\n",
    "    def process_terms_classifications(self):\n",
    "        \"\"\"\n",
    "        Processes classification information specifically for terms from 'classify_P2_Definitional_terms'\n",
    "        document and stores the type, subtype, subtype confidence, and subtype explanation.\n",
    "        The type is always set to 'Definitional'.\n",
    "        \"\"\"\n",
    "        # Document identifier we are interested in\n",
    "        doc_classification = \"classify_P2_Definitional_terms\"\n",
    "\n",
    "        # Retrieve document content\n",
    "        doc = self.manager.retrieve_document(\n",
    "            doc_classification, \"llm_response_classification\"\n",
    "        )\n",
    "        if not doc or not doc.content:\n",
    "            logger.warning(\n",
    "                f\"Document '{doc_classification}' not found or has empty content.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        doc_content = doc.content\n",
    "\n",
    "        # Iterate over each item in the document content\n",
    "        for item in doc_content:\n",
    "            doc_id = normalize_str(item.get(\"doc_id\"))\n",
    "            statement_id = normalize_str(str(item.get(\"statement_id\")))\n",
    "            classifications = item.get(\"classification\", [])\n",
    "\n",
    "            # Iterate over each classification to extract the highest confidence one for terms\n",
    "            for classification in classifications:\n",
    "                confidence = classification.get(\"confidence\", 0)\n",
    "\n",
    "                # Check if this classification already exists in terms_classifications\n",
    "                existing_term = next(\n",
    "                    (\n",
    "                        term\n",
    "                        for term in self.terms_classifications\n",
    "                        if term[\"doc_id\"] == doc_id\n",
    "                        and term[\"statement_id\"] == statement_id\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                # Initialize if not found\n",
    "                if existing_term is None:\n",
    "                    existing_term = {\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"statement_id\": statement_id,\n",
    "                        \"type\": \"Definitional\",  # Set type as \"Definitional\"\n",
    "                        \"subtype\": None,\n",
    "                        \"confidence\": -1,\n",
    "                        \"explanation\": \"\",\n",
    "                        \"templates_ids\": [],\n",
    "                    }\n",
    "                    self.terms_classifications.append(existing_term)\n",
    "\n",
    "                # Update subtype information if the confidence is higher than the existing one\n",
    "                if confidence > existing_term[\"confidence\"]:\n",
    "                    existing_term[\"subtype\"] = classification.get(\"subtype\")\n",
    "                    existing_term[\"confidence\"] = confidence\n",
    "                    existing_term[\"explanation\"] = classification.get(\"explanation\", \"\")\n",
    "                    existing_term[\"templates_ids\"] = classification.get(\n",
    "                        \"templates_ids\", []\n",
    "                    )\n",
    "\n",
    "        # Log the final classification for debugging purposes\n",
    "        logger.debug(f\"{self.terms_classifications=}\")\n",
    "\n",
    "    def add_definition(self, doc_id, term, definition, isLocalScope):\n",
    "        \"\"\"\n",
    "        Adds a term definition and isLocalScope to the elements_terms_definition dictionary.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): Identifier of the document.\n",
    "            term (str): The term to be defined.\n",
    "            definition (str): The definition of the term.\n",
    "            isLocalScope (bool): The isLocalScope value.\n",
    "        \"\"\"\n",
    "        self.elements_terms_definition.setdefault(doc_id, {})[term] = {\n",
    "            'definition': definition,\n",
    "            'isLocalScope': isLocalScope\n",
    "        }\n",
    "\n",
    "    def process_definitions(self):\n",
    "        \"\"\"\n",
    "        Processes document terms definitions and stores them in elements_terms_definition.\n",
    "        \"\"\"\n",
    "        docs_p2 = [\n",
    "            s\n",
    "            for s in self.manager.list_document_ids(doc_type=\"llm_response\")\n",
    "            if s.endswith(\"_P2\")\n",
    "        ]\n",
    "\n",
    "        for doc in docs_p2:\n",
    "            doc_id = doc.replace(\"_P2\", \"\")\n",
    "            doc_content = self.manager.retrieve_document(\n",
    "                doc, doc_type=\"llm_response\"\n",
    "            ).content\n",
    "            doc_terms = doc_content.get(\"terms\", [])\n",
    "            for term in doc_terms:\n",
    "                self.add_definition(\n",
    "                    doc_id,\n",
    "                    term.get(\"term\"),\n",
    "                    term.get(\"definition\"),\n",
    "                    term.get(\"isLocalScope\")\n",
    "                )\n",
    "\n",
    "    def process_elements(self):\n",
    "        \"\"\"\n",
    "        Processes elements from documents and categorizes them into terms, names, facts, and rules.\n",
    "        \"\"\"\n",
    "        # Get the list of documents that end with '_P1'\n",
    "        docs_p1 = [\n",
    "            s\n",
    "            for s in self.manager.list_document_ids(doc_type=\"llm_response\")\n",
    "            if s.endswith(\"_P1\")\n",
    "        ]\n",
    "\n",
    "        for doc in docs_p1:\n",
    "            doc_content = self.manager.retrieve_document(\n",
    "                doc, doc_type=\"llm_response\"\n",
    "            ).content\n",
    "            doc_id = doc_content.get(\"section\")\n",
    "            doc_elements = doc_content.get(\"elements\", [])\n",
    "            for element in doc_elements:\n",
    "                element_classification = element.get(\"classification\")\n",
    "                element_id = element.get(\"id\")\n",
    "                verb_symbols = element.get(\"verb_symbols\") or element.get(\"verb_symbol\")\n",
    "                if isinstance(verb_symbols, str):\n",
    "                    verb_symbols = [verb_symbols]\n",
    "                elif verb_symbols is None:\n",
    "                    verb_symbols = []\n",
    "                element_dict = {\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"statement_id\": element_id,\n",
    "                    \"statement_title\": element.get(\"title\"),\n",
    "                    \"statement\": element.get(\"statement\"),\n",
    "                    \"sources\": element.get(\"sources\"),\n",
    "                    \"terms\": element.get(\"terms\", []),\n",
    "                    \"verb_symbols\": verb_symbols,\n",
    "                    \"element_name\": element_classification,\n",
    "                }\n",
    "                logger.debug(f\"{element_dict=}\")\n",
    "\n",
    "                match element_classification:\n",
    "                    case \"Fact\" | \"Fact Type\":\n",
    "                        self.elements_facts.append(element_dict)\n",
    "                    case \"Operative Rule\":\n",
    "                        self.elements_rules.append(element_dict)\n",
    "\n",
    "                element_terms = element.get(\"terms\", [])\n",
    "                if element_terms:\n",
    "                    for term in element_terms:\n",
    "                        signifier = term.get(\"term\")\n",
    "                        # Retrieve definition and isLocalScope\n",
    "                        term_info = self.elements_terms_definition.get(doc_id, {}).get(signifier, {})\n",
    "                        definition = term_info.get('definition')\n",
    "                        isLocalScope = term_info.get('isLocalScope')\n",
    "\n",
    "                        term_dict = {\n",
    "                            \"doc_id\": doc_id,\n",
    "                            \"statement_id\": signifier,  # Using signifier as statement_id\n",
    "                            \"definition\": definition,\n",
    "                            \"isLocalScope\": isLocalScope,\n",
    "                            \"sources\": element.get(\"sources\"),\n",
    "                            \"element_name\": \"Term\" if term.get(\"classification\") == \"Common Noun\" else \"Name\",\n",
    "                        }\n",
    "\n",
    "                        # Append new term\n",
    "                        if term_dict[\"element_name\"] == \"Term\":\n",
    "                            self.elements_terms.append(term_dict)\n",
    "                            self.elements_terms_set.add(signifier)\n",
    "                        else:\n",
    "                            self.elements_names.append(term_dict)\n",
    "                            self.elements_names_set.add(signifier)\n",
    "\n",
    "    def process_operative_rules_classifications(self):\n",
    "        \"\"\"\n",
    "        Processes classification information specifically for operative rules from\n",
    "        'classify_P1' and 'classify_P2_Operative_rules' documents, and stores the type, subtype,\n",
    "        confidence, and explanation.\n",
    "        \"\"\"\n",
    "        # Get only the specific documents we are interested in\n",
    "        docs_classification = [\n",
    "            s\n",
    "            for s in self.manager.list_document_ids(\n",
    "                doc_type=\"llm_response_classification\"\n",
    "            )\n",
    "            if s in [\"classify_P1\", \"classify_P2_Operative_rules\"]\n",
    "        ]\n",
    "\n",
    "        # A temporary dictionary to group classifications by (doc_id, statement_id)\n",
    "        classification_dict = {}\n",
    "\n",
    "        # Iterate over each document\n",
    "        for doc in docs_classification:\n",
    "            # Retrieve document content for each classification document\n",
    "            doc_content = self.manager.retrieve_document(\n",
    "                doc, \"llm_response_classification\"\n",
    "            ).content\n",
    "\n",
    "            # Iterate over each item in the document content\n",
    "            for item in doc_content:\n",
    "                doc_id = normalize_str(item.get(\"doc_id\"))\n",
    "                statement_id = normalize_str(str(item.get(\"statement_id\")))\n",
    "                classifications = item.get(\"classification\", [])\n",
    "\n",
    "                # Create a key for grouping classifications\n",
    "                key = (doc_id, statement_id)\n",
    "\n",
    "                # Initialize the classification entry if it doesn't exist\n",
    "                if key not in classification_dict:\n",
    "                    classification_dict[key] = {\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"statement_id\": statement_id,\n",
    "                        \"type\": None,\n",
    "                        \"type_confidence\": -1,  # Initialize with a negative value to ensure first confidence is updated\n",
    "                        \"type_explanation\": \"\",\n",
    "                        \"subtype\": None,\n",
    "                        \"subtype_confidence\": -1,  # Initialize with a negative value to ensure first confidence is updated\n",
    "                        \"subtype_explanation\": \"\",\n",
    "                        \"templates_ids\": [],\n",
    "                    }\n",
    "\n",
    "                # Iterate over each classification to extract the highest confidence one\n",
    "                for classification in classifications:\n",
    "                    # Extract confidence and other classification details\n",
    "                    confidence = classification.get(\"confidence\", 0)\n",
    "                    current_classification = classification_dict[key]\n",
    "\n",
    "                    # Update based on document type and ensure we retain both type and subtype\n",
    "                    if doc == \"classify_P1\":\n",
    "                        # Update type if this document is from classify_P1 and has higher confidence\n",
    "                        if confidence > current_classification[\"type_confidence\"]:\n",
    "                            current_classification[\"type\"] = classification.get(\"type\")\n",
    "                            current_classification[\"type_confidence\"] = confidence\n",
    "                            current_classification[\"type_explanation\"] = (\n",
    "                                classification.get(\"explanation\", \"\")\n",
    "                            )\n",
    "\n",
    "                    elif doc == \"classify_P2_Operative_rules\":\n",
    "                        # Update subtype if this document is from classify_P2_Operative_rules and has higher confidence\n",
    "                        if confidence > current_classification[\"subtype_confidence\"]:\n",
    "                            current_classification[\"subtype\"] = classification.get(\n",
    "                                \"subtype\"\n",
    "                            )\n",
    "                            current_classification[\"subtype_confidence\"] = confidence\n",
    "                            current_classification[\"subtype_explanation\"] = (\n",
    "                                classification.get(\"explanation\", \"\")\n",
    "                            )\n",
    "                            current_classification[\"templates_ids\"] = (\n",
    "                                classification.get(\"templates_ids\", [])\n",
    "                            )\n",
    "\n",
    "        # Convert the classification_dict to a list and assign to operative_rules_classifications\n",
    "        self.operative_rules_classifications = list(classification_dict.values())\n",
    "        logger.debug(f\"{self.operative_rules_classifications=}\")\n",
    "\n",
    "    def merge_terms(self):\n",
    "        \"\"\"\n",
    "        Merges term elements with the same doc_id and statement_id by combining sources lists and \n",
    "        retaining fields from the element with the highest semscore and similarity_score.\n",
    "        \"\"\"\n",
    "        # Build a dictionary to group terms by (doc_id, statement_id)\n",
    "        grouped_terms = {}\n",
    "        for term in self.elements_terms:\n",
    "            key = (term['doc_id'], term['statement_id'])\n",
    "            grouped_terms.setdefault(key, []).append(term)\n",
    "        \n",
    "        # Now process each group to merge elements\n",
    "        merged_terms = []\n",
    "        for key, terms in grouped_terms.items():\n",
    "            # Collect all 'sources' lists into a combined list and remove duplicates\n",
    "            sources = []\n",
    "            for term in terms:\n",
    "                sources.extend(term.get('sources', []))\n",
    "            sources = list(set(sources))  # Remove duplicates\n",
    "            \n",
    "            # Find the term with the highest 'semscore' and 'similarity_score'\n",
    "            best_term = max(terms, key=lambda t: (t.get('similarity_score', 0), t.get('semscore', 0)))\n",
    "            \n",
    "            # Create a new term with merged data\n",
    "            merged_term = best_term.copy()\n",
    "            merged_term['sources'] = sources\n",
    "            \n",
    "            # Add the merged term to the list\n",
    "            merged_terms.append(merged_term)\n",
    "        \n",
    "        # Replace self.elements_terms with the merged_terms\n",
    "        self.elements_terms = merged_terms\n",
    "\n",
    "    def merge_names(self):\n",
    "        \"\"\"\n",
    "        Merges name elements with the same doc_id and statement_id by combining sources lists and \n",
    "        retaining fields from the element with the highest semscore and similarity_score.\n",
    "        \"\"\"\n",
    "        # Build a dictionary to group names by (doc_id, statement_id)\n",
    "        grouped_names = {}\n",
    "        for name in self.elements_names:\n",
    "            key = (name['doc_id'], name['statement_id'])\n",
    "            grouped_names.setdefault(key, []).append(name)\n",
    "        \n",
    "        # Now process each group to merge elements\n",
    "        merged_names = []\n",
    "        for key, names in grouped_names.items():\n",
    "            # Collect all 'sources' lists into a combined list and remove duplicates\n",
    "            sources = []\n",
    "            for name in names:\n",
    "                sources.extend(name.get('sources', []))\n",
    "            sources = list(set(sources))  # Remove duplicates\n",
    "            \n",
    "            # Find the name with the highest 'semscore' and 'similarity_score'\n",
    "            best_name = max(names, key=lambda n: (n.get('similarity_score', 0), n.get('semscore', 0)))\n",
    "            \n",
    "            # Create a new name with merged data\n",
    "            merged_name = best_name.copy()\n",
    "            merged_name['sources'] = sources\n",
    "            \n",
    "            # Add the merged name to the list\n",
    "            merged_names.append(merged_name)\n",
    "        \n",
    "        # Replace self.elements_names with the merged_names\n",
    "        self.elements_names = merged_names\n",
    "\n",
    "\n",
    "    def get_unique_terms(self, doc_id=None):\n",
    "        \"\"\"\n",
    "        Returns the set of unique terms found in the documents. If doc_id is provided,\n",
    "        returns only the unique terms for that specific document.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str, optional): Identifier of the document. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            set: Set of unique terms.\n",
    "        \"\"\"\n",
    "        if doc_id:\n",
    "            return {\n",
    "                term[\"signifier\"]\n",
    "                for term in self.elements_terms\n",
    "                if term[\"doc_id\"] == doc_id\n",
    "            }\n",
    "        return self.elements_terms_set\n",
    "\n",
    "    def get_unique_names(self, doc_id=None):\n",
    "        \"\"\"\n",
    "        Returns the set of unique names found in the documents. If doc_id is provided,\n",
    "        returns only the unique names for that specific document.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str, optional): Identifier of the document. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            set: Set of unique names.\n",
    "        \"\"\"\n",
    "        if doc_id:\n",
    "            return {\n",
    "                name[\"signifier\"]\n",
    "                for name in self.elements_names\n",
    "                if name[\"doc_id\"] == doc_id\n",
    "            }\n",
    "        return self.elements_names_set\n",
    "\n",
    "    def get_terms(self, doc_id=None, term_id=None, definition_filter=\"all\"):\n",
    "        \"\"\"\n",
    "        Returns the list of terms extracted from documents, enriched with type, subtype, confidence, and explanation.\n",
    "        If doc_id and term_id are provided, returns a specific term.\n",
    "        If only doc_id is provided, returns all terms for that specific document.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str, optional): Document identifier to filter a specific term or all terms in the document.\n",
    "            term_id (str, optional): Term identifier to filter a specific term.\n",
    "            definition_filter (str): Filter for terms based on definition presence.\n",
    "                                    \"non_null\" returns terms with definitions,\n",
    "                                    \"null\" returns terms without definitions,\n",
    "                                    \"all\" returns all terms regardless of definition.\n",
    "\n",
    "        Returns:\n",
    "            list or dict: List of enriched terms, or a dictionary with a specific term if both doc_id and term_id are provided.\n",
    "        \"\"\"\n",
    "        # Create a lookup dictionary for terms classifications for efficient access\n",
    "        term_classification_lookup = {\n",
    "            (\n",
    "                normalize_str(classification[\"doc_id\"]),\n",
    "                normalize_str(str(classification[\"statement_id\"])),\n",
    "            ): classification\n",
    "            for classification in self.terms_classifications\n",
    "        }\n",
    "\n",
    "        enriched_terms = []\n",
    "\n",
    "        # Enrich terms with classification information\n",
    "        for term in self.elements_terms:\n",
    "            term_key = (\n",
    "                normalize_str(term[\"doc_id\"]),\n",
    "                normalize_str(term.get(\"statement_id\", \"\")),\n",
    "            )\n",
    "            classification = term_classification_lookup.get(term_key)\n",
    "\n",
    "            if classification:\n",
    "                term.update(\n",
    "                    {\n",
    "                        \"type\": classification.get(\"type\"),\n",
    "                        \"subtype\": classification.get(\"subtype\"),\n",
    "                        \"confidence\": classification.get(\"confidence\"),\n",
    "                        \"explanation\": classification.get(\"explanation\"),\n",
    "                        \"templates_ids\": classification.get(\"templates_ids\", []),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            enriched_terms.append(term)\n",
    "\n",
    "        # Apply filtering based on definition presence\n",
    "        if definition_filter == \"non_null\":\n",
    "            enriched_terms = [term for term in enriched_terms if term.get(\"definition\")]\n",
    "        elif definition_filter == \"null\":\n",
    "            enriched_terms = [\n",
    "                term for term in enriched_terms if not term.get(\"definition\")\n",
    "            ]\n",
    "        else:\n",
    "            logger.info(\"Not filtering terms based on definition presence.\")\n",
    "\n",
    "        # If both doc_id and term_id are provided, return the specific term\n",
    "        if doc_id and term_id:\n",
    "            for term in enriched_terms:\n",
    "                if normalize_str(term[\"doc_id\"]) == normalize_str(\n",
    "                    doc_id\n",
    "                ) and normalize_str(term.get(\"statement_id\", \"\")) == normalize_str(\n",
    "                    term_id\n",
    "                ):\n",
    "                    return term\n",
    "\n",
    "            # Return None if no matching term is found\n",
    "            logger.debug(f\"No term found for doc_id='{doc_id}', term_id='{term_id}'\")\n",
    "            return None\n",
    "\n",
    "        # If only doc_id is provided, return all terms that match the given doc_id\n",
    "        if doc_id:\n",
    "            filtered_terms = [\n",
    "                term\n",
    "                for term in enriched_terms\n",
    "                if normalize_str(term[\"doc_id\"]) == normalize_str(doc_id)\n",
    "            ]\n",
    "\n",
    "            if not filtered_terms:\n",
    "                logger.debug(f\"No terms found for doc_id='{doc_id}'\")\n",
    "                return []\n",
    "\n",
    "            return filtered_terms\n",
    "\n",
    "        # If neither doc_id nor term_id is provided, return all enriched terms\n",
    "        return enriched_terms\n",
    "\n",
    "    def get_names(self, doc_id=None, name_id=None, definition_filter=\"all\"):\n",
    "        \"\"\"\n",
    "        Returns the list of names extracted from documents, enriched with type, subtype, confidence, and explanation.\n",
    "        If doc_id and name_id are provided, returns a specific name.\n",
    "        If only doc_id is provided, returns all names for that specific document.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str, optional): Document identifier to filter a specific name or all names in the document.\n",
    "            name_id (str, optional): Name identifier to filter a specific name.\n",
    "            definition_filter (str, optional): Filter for names based on definition presence.\n",
    "                                            \"non_null\" returns names with definitions,\n",
    "                                            \"null\" returns names without definitions,\n",
    "                                            \"all\" returns all names regardless of definition.\n",
    "\n",
    "        Returns:\n",
    "            list or dict: List of enriched names, or a dictionary with a specific name if both doc_id and name_id are provided.\n",
    "        \"\"\"\n",
    "        # Create a lookup dictionary for names classifications for efficient access\n",
    "        name_classification_lookup = {\n",
    "            (\n",
    "                normalize_str(classification[\"doc_id\"]),\n",
    "                normalize_str(str(classification[\"statement_id\"])),\n",
    "            ): classification\n",
    "            for classification in self.names_classifications\n",
    "        }\n",
    "\n",
    "        enriched_names = []\n",
    "\n",
    "        # Enrich names with classification information\n",
    "        for name in self.elements_names:\n",
    "            name_key = (\n",
    "                normalize_str(name[\"doc_id\"]),\n",
    "                normalize_str(name.get(\"statement_id\", \"\")),\n",
    "            )\n",
    "            classification = name_classification_lookup.get(name_key)\n",
    "\n",
    "            if classification:\n",
    "                name.update(\n",
    "                    {\n",
    "                        \"type\": classification.get(\"type\"),\n",
    "                        \"subtype\": classification.get(\"subtype\"),\n",
    "                        \"confidence\": classification.get(\"confidence\"),\n",
    "                        \"explanation\": classification.get(\"explanation\"),\n",
    "                        \"templates_ids\": classification.get(\"templates_ids\", []),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            enriched_names.append(name)\n",
    "\n",
    "        # Apply filtering based on definition presence\n",
    "        if definition_filter == \"non_null\":\n",
    "            enriched_names = [name for name in enriched_names if name.get(\"definition\")]\n",
    "        elif definition_filter == \"null\":\n",
    "            enriched_names = [\n",
    "                name for name in enriched_names if not name.get(\"definition\")\n",
    "            ]\n",
    "        else:\n",
    "            logger.info(\"Not filtering names based on definition presence.\")\n",
    "\n",
    "        # If both doc_id and name_id are provided, return the specific name\n",
    "        if doc_id and name_id:\n",
    "            for name in enriched_names:\n",
    "                if normalize_str(name[\"doc_id\"]) == normalize_str(\n",
    "                    doc_id\n",
    "                ) and normalize_str(name.get(\"statement_id\", \"\")) == normalize_str(\n",
    "                    name_id\n",
    "                ):\n",
    "                    return name\n",
    "\n",
    "            # Return None if no matching name is found\n",
    "            logger.debug(f\"No name found for doc_id='{doc_id}', name_id='{name_id}'\")\n",
    "            return None\n",
    "\n",
    "        # If only doc_id is provided, return all names that match the given doc_id\n",
    "        if doc_id:\n",
    "            filtered_names = [\n",
    "                name\n",
    "                for name in enriched_names\n",
    "                if normalize_str(name[\"doc_id\"]) == normalize_str(doc_id)\n",
    "            ]\n",
    "\n",
    "            if not filtered_names:\n",
    "                logger.debug(f\"No names found for doc_id='{doc_id}'\")\n",
    "                return []\n",
    "\n",
    "            return filtered_names\n",
    "\n",
    "        # If neither doc_id nor name_id is provided, return all enriched names\n",
    "        return enriched_names\n",
    "\n",
    "    def get_facts(self, doc_id=None, statement_id=None):\n",
    "        \"\"\"\n",
    "        Returns the list of facts extracted from documents, enriched with type, subtype, confidence, and explanation.\n",
    "        If doc_id and statement_id are provided, returns a specific fact.\n",
    "        If only doc_id is provided, returns all facts for that specific document.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str, optional): Document identifier to filter a specific fact or all facts in the document.\n",
    "            statement_id (str, optional): Statement identifier to filter a specific fact.\n",
    "\n",
    "        Returns:\n",
    "            list or dict: List of enriched facts, or a dictionary with a specific fact if both doc_id and statement_id are provided.\n",
    "        \"\"\"\n",
    "        # Create a lookup dictionary for facts classifications for efficient access\n",
    "        fact_classification_lookup = {\n",
    "            (\n",
    "                normalize_str(classification[\"doc_id\"]),\n",
    "                normalize_str(str(classification[\"statement_id\"])),\n",
    "            ): classification\n",
    "            for classification in self.facts_classifications\n",
    "        }\n",
    "\n",
    "        enriched_facts = []\n",
    "\n",
    "        # Enrich facts with classification information\n",
    "        for fact in self.elements_facts:\n",
    "            fact_key = (\n",
    "                normalize_str(fact[\"doc_id\"]),\n",
    "                normalize_str(str(fact[\"statement_id\"])),\n",
    "            )\n",
    "            classification = fact_classification_lookup.get(fact_key)\n",
    "\n",
    "            if classification:\n",
    "                fact.update(\n",
    "                    {\n",
    "                        \"type\": classification.get(\"type\"),\n",
    "                        \"subtype\": classification.get(\"subtype\"),\n",
    "                        \"subtype_confidence\": classification.get(\"subtype_confidence\"),\n",
    "                        \"subtype_explanation\": classification.get(\n",
    "                            \"subtype_explanation\"\n",
    "                        ),\n",
    "                        \"templates_ids\": classification.get(\"templates_ids\", []),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            enriched_facts.append(fact)\n",
    "\n",
    "        # If both doc_id and statement_id are provided, return the specific fact\n",
    "        if doc_id and statement_id:\n",
    "            for fact in enriched_facts:\n",
    "                if normalize_str(fact[\"doc_id\"]) == normalize_str(\n",
    "                    doc_id\n",
    "                ) and normalize_str(str(fact[\"statement_id\"])) == normalize_str(\n",
    "                    str(statement_id)\n",
    "                ):\n",
    "                    return fact\n",
    "\n",
    "            # Return None if no matching fact is found\n",
    "            logger.debug(\n",
    "                f\"No fact found for doc_id='{doc_id}', statement_id='{statement_id}'\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        # If only doc_id is provided, return all facts that match the given doc_id\n",
    "        if doc_id:\n",
    "            filtered_facts = [\n",
    "                fact\n",
    "                for fact in enriched_facts\n",
    "                if normalize_str(fact[\"doc_id\"]) == normalize_str(doc_id)\n",
    "            ]\n",
    "\n",
    "            if not filtered_facts:\n",
    "                logger.debug(f\"No facts found for doc_id='{doc_id}'\")\n",
    "                return []\n",
    "\n",
    "            return filtered_facts\n",
    "\n",
    "        # If neither doc_id nor statement_id is provided, return all enriched facts\n",
    "        return enriched_facts\n",
    "\n",
    "    def get_rules(self, doc_id=None, statement_id=None):\n",
    "        \"\"\"\n",
    "        Returns the list of rules extracted from documents, enriched with type, subtype, confidence, and explanation.\n",
    "        If doc_id and statement_id are provided, returns a specific rule.\n",
    "        If only doc_id is provided, returns all rules for that specific document.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str, optional): Document identifier to filter a specific rule or all rules in the document.\n",
    "            statement_id (str, optional): Statement identifier to filter a specific rule.\n",
    "\n",
    "        Returns:\n",
    "            list or dict: List of enriched rules, or a dictionary with a specific rule if both doc_id and statement_id are provided.\n",
    "        \"\"\"\n",
    "        # Create a lookup dictionary for rules classifications for efficient access\n",
    "        rule_classification_lookup = {\n",
    "            (\n",
    "                normalize_str(classification[\"doc_id\"]),\n",
    "                normalize_str(str(classification[\"statement_id\"])),\n",
    "            ): classification\n",
    "            for classification in self.operative_rules_classifications\n",
    "        }\n",
    "\n",
    "        enriched_rules = []\n",
    "\n",
    "        # Enrich rules with classification information\n",
    "        for rule in self.elements_rules:\n",
    "            rule_key = (\n",
    "                normalize_str(rule[\"doc_id\"]),\n",
    "                normalize_str(str(rule[\"statement_id\"])),\n",
    "            )\n",
    "            classification = rule_classification_lookup.get(rule_key)\n",
    "\n",
    "            if classification:\n",
    "                rule.update(\n",
    "                    {\n",
    "                        \"type\": classification.get(\"type\"),\n",
    "                        \"type_confidence\": classification.get(\"type_confidence\"),\n",
    "                        \"type_explanation\": classification.get(\"type_explanation\"),\n",
    "                        \"subtype\": classification.get(\"subtype\"),\n",
    "                        \"subtype_confidence\": classification.get(\"subtype_confidence\"),\n",
    "                        \"subtype_explanation\": classification.get(\n",
    "                            \"subtype_explanation\"\n",
    "                        ),\n",
    "                        \"templates_ids\": classification.get(\"templates_ids\"),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            enriched_rules.append(rule)\n",
    "\n",
    "        # If both doc_id and statement_id are provided, return the specific rule\n",
    "        if doc_id and statement_id:\n",
    "            for rule in enriched_rules:\n",
    "                if normalize_str(rule[\"doc_id\"]) == normalize_str(\n",
    "                    doc_id\n",
    "                ) and normalize_str(str(rule[\"statement_id\"])) == normalize_str(\n",
    "                    str(statement_id)\n",
    "                ):\n",
    "                    return rule\n",
    "\n",
    "            # Return None if no matching rule is found\n",
    "            logger.debug(\n",
    "                f\"No rule found for doc_id='{doc_id}', statement_id='{statement_id}'\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        # If only doc_id is provided, return all rules that match the given doc_id\n",
    "        if doc_id:\n",
    "            filtered_rules = [\n",
    "                rule\n",
    "                for rule in enriched_rules\n",
    "                if normalize_str(rule[\"doc_id\"]) == normalize_str(doc_id)\n",
    "            ]\n",
    "\n",
    "            if not filtered_rules:\n",
    "                logger.debug(f\"No rules found for doc_id='{doc_id}'\")\n",
    "                return []\n",
    "\n",
    "            return filtered_rules\n",
    "\n",
    "        # If neither doc_id nor statement_id is provided, return all enriched rules\n",
    "        return enriched_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir logging_setup && touch logging_setup/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting logging_setup/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile logging_setup/main.py\n",
    "\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "\n",
    "\n",
    "def setting_logging(log_path: str, log_level: str):\n",
    "    # Ensure the ../logs directory exists\n",
    "    log_directory = Path.cwd() / log_path\n",
    "    log_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Path for the log file\n",
    "    log_file_path = log_directory / \"application.log\"\n",
    "\n",
    "    # Set up TimedRotatingFileHandler to rotate logs every day\n",
    "    file_handler = TimedRotatingFileHandler(\n",
    "        log_file_path,\n",
    "        when=\"midnight\",\n",
    "        interval=1,\n",
    "        backupCount=0,  # Rotate every midnight, keep all backups\n",
    "    )\n",
    "\n",
    "    # Set the file handler's log format\n",
    "    file_handler.setFormatter(\n",
    "        logging.Formatter(\n",
    "            \"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    match log_level:\n",
    "        case \"DEBUG\":\n",
    "            level = logging.DEBUG\n",
    "        case \"INFO\":\n",
    "            level = logging.INFO\n",
    "        case \"WARNING\":\n",
    "            level = logging.WARNING\n",
    "        case \"ERROR\":\n",
    "            level = logging.ERROR\n",
    "        case \"CRITICAL\":\n",
    "            level = logging.CRITICAL\n",
    "        case _:\n",
    "            level = logging.INFO\n",
    "\n",
    "    # Set up logging configuration\n",
    "    logging.basicConfig(\n",
    "        level=level,  # Set to the desired log level\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Console log format\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",  # Custom date format\n",
    "        handlers=[\n",
    "            file_handler,  # Log to the rotating file in ../logs\n",
    "            logging.StreamHandler(),  # Log to console\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Example logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Log a test message to verify\n",
    "    logger.info(\"Logging is set up with daily rotation.\")\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir token_estimator && touch token_estimator/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile token_estimator/main.py\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def estimate_tokens(text, model=\"gpt-4o\"):\n",
    "    \"\"\"\n",
    "    Estimates the number of tokens in a given text using the OpenAI `tiktoken` library, \n",
    "    which closely approximates the tokenization method used by OpenAI language models.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to be tokenized and counted.\n",
    "        model (str): The model to use for tokenization. Defaults to \"gpt-4o\".\n",
    "                     Supported models include \"gpt-3.5-turbo\" and \"gpt-4o\".\n",
    "\n",
    "    Returns:\n",
    "        int: The estimated number of tokens in the text.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the specified model is not supported by `tiktoken`.\n",
    "\n",
    "    Example:\n",
    "        >>> text = \"This is a sample text.\"\n",
    "        >>> estimate_tokens_tiktoken(text)\n",
    "        6\n",
    "    \"\"\"\n",
    "    # Load the appropriate tokenizer\n",
    "    try:\n",
    "        tokenizer = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Model '{model}' is not supported by tiktoken.\")\n",
    "    \n",
    "    # Tokenize the text and return the token count\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir rules_taxonomy_provider && touch rules_taxonomy_provider/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rules_taxonomy_provider/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rules_taxonomy_provider/main.py\n",
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "class RuleInformationProvider:\n",
    "    \"\"\"\n",
    "    A class to provide information about rule classifications and templates based on YAML data.\n",
    "\n",
    "    This class loads and processes rule classification data, template data, and example data from specified YAML files.\n",
    "    It is used to generate markdown documentation for a given rule type, including details such as templates and examples.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    data_path : str\n",
    "        Path to the directory containing the YAML files.\n",
    "    template_dict : dict\n",
    "        Dictionary containing template information loaded from the templates YAML file.\n",
    "    examples_dict : dict\n",
    "        Dictionary containing example information loaded from the examples YAML file.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Initializes the RuleInformationProvider with the specified data path.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_path : str\n",
    "            Path to the directory containing the YAML files with rules, templates, and examples.\n",
    "        \"\"\"\n",
    "        self.data_path = Path(data_path)\n",
    "        self.template_dict = self._load_yaml(self.data_path / 'witt_templates.yaml', 'template_list')\n",
    "        self.examples_dict = self._load_yaml(self.data_path / 'witt_examples.yaml', 'example_list')\n",
    "\n",
    "    def _load_yaml(self, file_path, list_key=None):\n",
    "        \"\"\"\n",
    "        Loads data from a YAML file.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        file_path : Path\n",
    "            Path to the YAML file to be loaded.\n",
    "        list_key : str, optional\n",
    "            Key used to extract a specific list from the YAML data. If provided, returns a dictionary indexed by 'id'.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            If list_key is provided, returns a dictionary with items indexed by 'id'.\n",
    "        Any type\n",
    "            If list_key is not provided, returns the entire data structure from the YAML file.\n",
    "        \"\"\"\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            data = yaml.safe_load(file)\n",
    "            if list_key and list_key in data:\n",
    "                return {item['id']: item for item in data[list_key]}\n",
    "            return data\n",
    "\n",
    "    def get_classification_and_templates(self, section_title, return_forms=\"all\"):\n",
    "        \"\"\"\n",
    "        Retrieves classification information and templates for a specified rule section.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        section_title : str\n",
    "            Title of the section for which to retrieve information.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A markdown formatted string containing the classification details, templates, and examples for the given section.\n",
    "        \"\"\"\n",
    "        data = self._load_yaml(self.data_path / 'classify_subtypes.yaml')\n",
    "        filtered_data = self._filter_sections_by_title(data, section_title)\n",
    "        return self._convert_to_markdown(filtered_data, return_forms)\n",
    "\n",
    "    def _filter_sections_by_title(self, data, title):\n",
    "        \"\"\"\n",
    "        Filters sections based on the given title.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : list\n",
    "            List of sections to filter from.\n",
    "        title : str\n",
    "            Title to filter sections by.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            A list of sections that match the given title.\n",
    "        \"\"\"\n",
    "        #return [section for section in data if section.get('section_title') == title]\n",
    "        # Improve the filtering to include subsections\n",
    "        results = []\n",
    "        for section in data:\n",
    "            if section.get('section_title') == title:\n",
    "                results.append(section)\n",
    "            if 'subsections' in section:\n",
    "                results.extend(self._filter_sections_by_title(section['subsections'], title))\n",
    "        return results\n",
    "\n",
    "    def _convert_to_markdown(self, filtered_data, return_forms):\n",
    "        \"\"\"\n",
    "        Converts filtered rule classification data to markdown format.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        filtered_data : list\n",
    "            List of filtered sections to convert into markdown.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A markdown formatted string representing the filtered sections.\n",
    "        \"\"\"\n",
    "        def process_section(section, level=2):\n",
    "            \"\"\"\n",
    "            Processes a section recursively and converts it to markdown format.\n",
    "\n",
    "            Parameters:\n",
    "            -----------\n",
    "            section : dict\n",
    "                The section to process.\n",
    "            level : int, optional\n",
    "                The heading level for the section title in markdown (default is 2).\n",
    "\n",
    "            Returns:\n",
    "            --------\n",
    "            str\n",
    "                A markdown formatted string for the section and its subsections.\n",
    "            \"\"\"\n",
    "            markdown = \"\"\n",
    "            if 'class' in section:\n",
    "                markdown += f\"{'#' * level} Subtype: {section['section_title']}\\n\\n\"\n",
    "            else:\n",
    "                markdown = f\"{'#' * level} {section['section_title']}\\n\\n\"\n",
    "            \n",
    "            markdown += f\"**ID**: {section['section_id']}\\n\\n\"\n",
    "            markdown += f\"**Definition**: {section['section_definition']}\\n\\n\"\n",
    "\n",
    "            if 'templates' in section:\n",
    "                markdown += self._process_templates(section['templates'], return_forms)\n",
    "\n",
    "            if 'examples' in section:\n",
    "                #print(section, return_forms)\n",
    "                #print(section['examples'])\n",
    "                markdown += self._process_examples(section['examples'], return_forms)\n",
    "\n",
    "            if 'subsections' in section:\n",
    "                for subsection in section['subsections']:\n",
    "                    markdown += process_section(subsection, level + 1)\n",
    "\n",
    "            return markdown\n",
    "\n",
    "        markdown = \"\"\n",
    "        for section in filtered_data:\n",
    "            markdown += process_section(section)\n",
    "        return markdown\n",
    "\n",
    "    def _process_templates(self, templates, return_forms):\n",
    "        \"\"\"\n",
    "        Processes templates and formats them to markdown.\n",
    "        \"\"\"\n",
    "        markdown = \"\"\n",
    "        for template_id in templates:\n",
    "            template = self.template_dict.get(template_id, None)\n",
    "            if template:\n",
    "                markdown += f\"**Template ID**: {template_id}\\n\"\n",
    "                if 'form' in template:\n",
    "                    markdown += f\"**Form**:\\n```form\\n{template['form']}\\n```\\n\\n\"\n",
    "                markdown += f\"**Template Explanation**: {template['explanation']}\\n\\n\"\n",
    "                if return_forms in [\"rule\", \"all\"] and 'rule_form' in template:\n",
    "                    markdown += f\"**Rule Form**:\\n```rule_form\\n{template['rule_form']}\\n```\\n\\n\"\n",
    "                if return_forms in [\"fact_type\", \"all\"] and 'fact_type_form' in template:\n",
    "                    markdown += f\"**Fact Type Form**:\\n```fact_type_form\\n{template['fact_type_form']}\\n```\\n\\n\"\n",
    "            else:\n",
    "                markdown += f\"**Template ID**: {template_id} - No details found.\\n\\n\"\n",
    "        return markdown\n",
    "\n",
    "    def _process_examples(self, examples, return_forms=\"all\"):\n",
    "        \"\"\"\n",
    "        Processes examples and formats them to markdown.\n",
    "        \"\"\"\n",
    "        markdown = \"\"\n",
    "        for example_id in examples:\n",
    "            example = self.examples_dict.get(example_id, None)\n",
    "            if example:\n",
    "                if return_forms == \"rule\" and not example_id.startswith(\"R\"):\n",
    "                    continue\n",
    "                if return_forms == \"fact_type\" and not example_id.startswith(\"F\"):\n",
    "                    continue\n",
    "                markdown += f\"**Example ID**: {example_id}\\n\\n\"\n",
    "                markdown += f\"**Example Text**:\\n\\n```example\\n{example['text']}\\n```\\n\\n\"\n",
    "            else:\n",
    "                markdown += f\"**Example ID**: {example_id} - No details found.\\n\\n\"\n",
    "        return markdown\n",
    "\n",
    "\n",
    "class RulesTemplateProvider:\n",
    "    \"\"\"\n",
    "    A class to provide information about rules templates and their relationships from YAML data.\n",
    "\n",
    "    This class loads and processes template data, subtemplate data, and their relationships from specified YAML files.\n",
    "    It is used to extract information about templates and format them into readable output.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_directory):\n",
    "        \"\"\"\n",
    "        Initializes the RulesTemplateProvider with the specified data directory.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_directory : str or Path\n",
    "            Path to the directory containing the YAML files with templates, subtemplates, and relationships.\n",
    "        \"\"\"\n",
    "        self.data_directory = Path(data_directory)\n",
    "        self.data_dicts = self._load_data()\n",
    "\n",
    "    def _load_yaml(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads data from a YAML file.\n",
    "        \"\"\"\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            return yaml.safe_load(file) or {}\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "        Loads data from multiple YAML files required for template processing.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'witt_template_relationship_data': self._load_yaml(self.data_directory / 'witt_template_subtemplate_relationship.yaml').get('template_subtemplate_relationship', {}),\n",
    "            'witt_templates_data': self._load_yaml(self.data_directory / 'witt_templates.yaml').get('template_list', []),\n",
    "            'witt_subtemplates_data': self._load_yaml(self.data_directory / 'witt_subtemplates.yaml').get('subtemplate_list', [])\n",
    "        }\n",
    "\n",
    "    def get_rules_template(self, template_keys, return_forms=\"all\"):\n",
    "        \"\"\"\n",
    "        Retrieves the formatted rules templates for the specified list of template keys, avoiding duplicate subtemplates.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        template_keys : str or list of str\n",
    "            The key(s) of the template(s) to be retrieved.\n",
    "        return_forms : str\n",
    "            Indicates which forms to return: 'rule_form', 'fact_type_form', or 'both'.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A formatted string representation of each template and its associated subtemplates, without duplicates.\n",
    "        \"\"\"\n",
    "        if isinstance(template_keys, str):\n",
    "            template_keys = [template_keys]\n",
    "\n",
    "        output = \"\"\n",
    "        processed_subtemplates = set()\n",
    "\n",
    "        for template_key in template_keys:\n",
    "            output += self._process_template(template_key, processed_keys=processed_subtemplates, return_forms=return_forms)\n",
    "            output += \"\\n\\n\"\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _process_template(self, template_key, processed_keys=None, return_forms=\"all\"):\n",
    "        if processed_keys is None:\n",
    "            processed_keys = set()\n",
    "\n",
    "        if template_key in processed_keys:\n",
    "            return ''\n",
    "        processed_keys.add(template_key)\n",
    "\n",
    "        template_data = self._get_template_data(template_key)\n",
    "        if not template_data:\n",
    "            return f\"# {template_key}\\n\\nTemplate data not found.\\n\\n\"\n",
    "\n",
    "        output = self._format_template_output(template_key, template_data, return_forms)\n",
    "\n",
    "        if 'usesSubtemplate' in template_data:\n",
    "            subtemplate_keys = template_data['usesSubtemplate']\n",
    "            subtemplate_keys = [subtemplate_keys] if isinstance(subtemplate_keys, str) else subtemplate_keys\n",
    "            for sub_key in subtemplate_keys:\n",
    "                sub_key = sub_key.strip()\n",
    "                output += self._process_template(sub_key, processed_keys, return_forms)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _get_template_data(self, template_key):\n",
    "        if template_key.startswith('T'):\n",
    "            template_data = self._find_data(template_key, self.data_dicts['witt_templates_data'])\n",
    "            uses_subtemplate = self.data_dicts['witt_template_relationship_data'].get(template_key, [])\n",
    "            if uses_subtemplate:\n",
    "                template_data['usesSubtemplate'] = uses_subtemplate\n",
    "        elif template_key.startswith('S'):\n",
    "            template_data = self._find_data(template_key, self.data_dicts['witt_subtemplates_data'])\n",
    "            uses_subtemplate = self.data_dicts['witt_template_relationship_data'].get(template_key, [])\n",
    "            if uses_subtemplate:\n",
    "                template_data['usesSubtemplate'] = uses_subtemplate\n",
    "        else:\n",
    "            template_data = None\n",
    "        return template_data\n",
    "\n",
    "    def _find_data(self, template_key, data_list):\n",
    "        for item in data_list:\n",
    "            if item.get('id', '') == template_key:\n",
    "                return item\n",
    "        return None\n",
    "\n",
    "    def _format_template_output(self, template_key, template_data, return_forms):\n",
    "        title = template_data.get('title', '')\n",
    "        output = f\"## {template_key}: {title}\\n\\n\" if title else f\"## {template_key}\\n\\n\"\n",
    "\n",
    "        if not template_data:\n",
    "            output += \"Template data not found.\\n\\n\"\n",
    "            return output\n",
    "\n",
    "        if 'usesSubtemplate' in template_data:\n",
    "            output += \"### Subtemplate(s) in use\\n\"\n",
    "            subtemplate_list = []\n",
    "            for sub_key in template_data['usesSubtemplate']:\n",
    "                sub_data = self._find_data(sub_key, self.data_dicts['witt_subtemplates_data'])\n",
    "                sub_title = sub_data.get('title', '') if sub_data else \"Unknown\"\n",
    "                subtemplate_list.append(f\"- {sub_key}: {sub_title}\")\n",
    "            output += \"\\n\".join(subtemplate_list) + \"\\n\\n\"\n",
    "        \n",
    "        if return_forms in [\"rule\", \"all\"] and 'rule_form' in template_data:\n",
    "            output += f\"### Rule Form\\n\\n{template_data['rule_form']}\\n\\n\"\n",
    "        if return_forms in [\"fact_type\", \"all\"] and 'fact_type_form' in template_data:\n",
    "            output += f\"### Fact Type Form\\n\\n{template_data['fact_type_form']}\\n\\n\"\n",
    "        if 'form' in template_data:\n",
    "            output += f\"### Form\\n\\n{template_data['form']}\\n\\n\"\n",
    "        if 'explanation' in template_data:\n",
    "            output += f\"### Explanation\\n\\n{template_data['explanation']}\\n\\n\"\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llm_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir llm_query && touch llm_query/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llm_query/main.py\n",
    "\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import instructor\n",
    "import logging\n",
    "from typing import Any\n",
    "\n",
    "# Set up basic logging configuration for the checkpoint module\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to INFO or another level as needed\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Log format\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def measure_time(func):\n",
    "    \"\"\"\n",
    "    Decorator to measure the execution time of a function.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed_time = end_time - start_time\n",
    "        logger.info(f\"Execution time for {func.__name__}: {elapsed_time:.2f} seconds\")\n",
    "        return result[0], result[1], elapsed_time\n",
    "    return wrapper\n",
    "\n",
    "@measure_time\n",
    "def query_instruct_llm(system_prompt: str,\n",
    "                        user_prompt: str,\n",
    "                        llm_model: str,\n",
    "                        document_model: Any,\n",
    "                        temperature: float,\n",
    "                        max_tokens: int) -> Any:\n",
    "    \"\"\"\n",
    "    Queries the LLM with the given system and user prompts.\n",
    "\n",
    "    Args:\n",
    "        system_prompt (str): The system prompt to set the context for the LLM.\n",
    "        user_prompt (str): The user prompt containing the text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        Any: The response from the LLM, parsed into a document_model object.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the API call fails.\n",
    "    \"\"\"\n",
    "    client = instructor.from_openai(OpenAI())\n",
    "    resp, completion = client.chat.completions.create_with_completion(\n",
    "        model=llm_model,\n",
    "        response_model=document_model,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "    )\n",
    "    logger.info(f\"Tokes used: {completion.usage}\")\n",
    "    return resp, completion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipt-cfr2sbvr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
