{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFR2SBVR Modules\n",
    "\n",
    "Supporting modules for the chapters 6, 7 of the dissertation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir configuration && touch configuration/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile configuration/main.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import glob\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "DEFAULT_CONFIG_DIR: str = '../config.yaml'  # Google drive: \"/content/drive/MyDrive/cfr2sbvr/config.yaml\"\n",
    "\n",
    "def _get_sorted_file_info(file_dir: str, file_prefix: str, extension: str):\n",
    "    \"\"\"\n",
    "    Helper function to retrieve and sort file information based on a specific prefix and extension.\n",
    "\n",
    "    Args:\n",
    "        file_dir (str): Directory to search for files.\n",
    "        file_prefix (str): Prefix for the filenames.\n",
    "        extension (str): File extension.\n",
    "\n",
    "    Returns:\n",
    "        list: Sorted list of file information dictionaries containing 'filename', 'date', and 'number' keys.\n",
    "    \"\"\"\n",
    "    path = Path(file_dir)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = list(path.glob(f\"{file_prefix}-*.{extension}\"))\n",
    "    file_info_list = []\n",
    "\n",
    "    pattern = re.compile(rf'^{file_prefix}-(\\d{{4}}-\\d{{2}}-\\d{{2}})-(\\d+)\\.{extension}$')\n",
    "    for filepath in files:\n",
    "        match = pattern.match(filepath.name)\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            number = int(match.group(2))\n",
    "            file_info_list.append({'filename': filepath.name, 'date': date_str, 'number': number})\n",
    "\n",
    "    return sorted(file_info_list, key=lambda x: (x['date'], x['number']), reverse=True)\n",
    "\n",
    "def get_next_filename(file_dir: str, file_prefix: str, extension: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates the next filename in a sequence based on existing files in a directory,\n",
    "    considering the file extension.\n",
    "\n",
    "    The filename format is: `{file_prefix}-{YYYY-MM-DD}-{N}.{extension}`,\n",
    "    where `N` is an incrementing integer for files with the same date.\n",
    "    \"\"\"\n",
    "    today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "    sorted_files = _get_sorted_file_info(file_dir, file_prefix, extension)\n",
    "\n",
    "    if sorted_files and sorted_files[0]['date'] == today_str:\n",
    "        new_number = sorted_files[0]['number'] + 1\n",
    "    else:\n",
    "        new_number = 1\n",
    "\n",
    "    new_filename = f'{file_prefix}-{today_str}-{new_number}.{extension}'\n",
    "    return str(Path(file_dir) / new_filename)\n",
    "\n",
    "def get_last_filename(file_dir: str, file_prefix: str, extension: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves the most recent filename based on the highest date and sequence number\n",
    "    for files with a specific prefix and extension in the specified directory.\n",
    "    \"\"\"\n",
    "    sorted_files = _get_sorted_file_info(file_dir, file_prefix, extension)\n",
    "    if sorted_files:\n",
    "        return str(Path(file_dir) / sorted_files[0]['filename'])\n",
    "    return None\n",
    "\n",
    "# Load the YAML config file\n",
    "def load_config(config_file: str = None):\n",
    "    if config_file is None:\n",
    "        config_file = DEFAULT_CONFIG_DIR\n",
    "    try:\n",
    "        with open(config_file, \"r\") as file:\n",
    "            config = yaml.safe_load(file)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Configuration file {config_file} not found.\")\n",
    "    except yaml.YAMLError as exc:\n",
    "        raise ValueError(f\"Error parsing YAML file {config_file}: {exc}\")\n",
    "\n",
    "    # Ensure config structure is correct\n",
    "    if \"LLM\" not in config or \"DEFAULT_CHECKPOINT_DIR\" not in config:\n",
    "        raise ValueError(\"Required configuration keys are missing in the config file.\")\n",
    "\n",
    "    # Set the OpenAI API key from environment variable if it's not set in config\n",
    "    config[\"LLM\"][\"OPENAI_API_KEY\"] = os.getenv(\n",
    "        \"OPENAI_API_KEY\", config[\"LLM\"].get(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "\n",
    "    # Dynamically set checkpoint and report files using the get_next_filename function\n",
    "    config[\"DEFAULT_CHECKPOINT_FILE\"] = get_next_filename(\n",
    "        config[\"DEFAULT_CHECKPOINT_DIR\"], \"documents\", \"json\"\n",
    "    )\n",
    "    config[\"DEFAULT_EXTRACTION_REPORT_FILE\"] = get_next_filename(\n",
    "        config[\"DEFAULT_OUTPUT_DIR\"], \"extraction_report\", \"html\"\n",
    "    )\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  # backup on Google Drive\n",
    "  !cp -r configuration /content/drive/MyDrive/cfr2sbvr/modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configuration.main as configuration\n",
    "\n",
    "# Development mode\n",
    "import importlib\n",
    "importlib.reload(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configuration.load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "Checkpoints are stored / retrieved at the directory `DEFAULT_CHECKPOINT_FILE` in the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir checkpoint && touch checkpoint/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile checkpoint/main.py\n",
    "\n",
    "from typing import List, Dict, Optional, Any, Tuple, Set\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "\n",
    "# Set up basic logging configuration for the checkpoint module\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to INFO or another level as needed\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Log format\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def convert_set_to_list(data: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Recursively converts sets to lists in the data structure.\n",
    "\n",
    "    Args:\n",
    "        data (Any): The data structure to process, which can be a dict, list, set, or other types.\n",
    "\n",
    "    Returns:\n",
    "        Any: The data structure with all sets converted to lists.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {key: convert_set_to_list(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_set_to_list(item) for item in data]\n",
    "    elif isinstance(data, set):\n",
    "        return list(data)\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n",
    "# Define a model for the Document\n",
    "class Document(BaseModel):\n",
    "    id: str\n",
    "    type: str  # New field to represent the type of the document\n",
    "    content: Any  # Content can be any data type: list, dict, string, etc.\n",
    "\n",
    "# Define the DocumentManager class\n",
    "class DocumentManager(BaseModel):\n",
    "    documents: Dict[Tuple[str, str], Document] = Field(default_factory=dict)  # Keys are tuples (id, type)\n",
    "\n",
    "    def add_document(self, doc: Document) -> None:\n",
    "        \"\"\"\n",
    "        Adds a document to the manager.\n",
    "\n",
    "        Args:\n",
    "            doc (Document): The document to add.\n",
    "        \"\"\"\n",
    "        key = (doc.id, doc.type)\n",
    "        self.documents[key] = doc\n",
    "\n",
    "    def retrieve_document(self, doc_id: str, doc_type: str) -> Optional[Document]:\n",
    "        \"\"\"\n",
    "        Retrieves a document by its id and type.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): The ID of the document.\n",
    "            doc_type (str): The type of the document.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Document]: The retrieved document, or None if not found.\n",
    "        \"\"\"\n",
    "        key = (doc_id, doc_type)\n",
    "        return self.documents.get(key)\n",
    "\n",
    "    def list_document_ids(self, doc_type: Optional[str] = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Lists all document ids, optionally filtered by type.\n",
    "\n",
    "        Args:\n",
    "            doc_type (Optional[str], optional): The type of documents to list. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of document ids.\n",
    "        \"\"\"\n",
    "        if doc_type:\n",
    "            return [doc_id for (doc_id, d_type) in self.documents.keys() if d_type == doc_type]\n",
    "        else:\n",
    "            return [doc_id for (doc_id, _) in self.documents.keys()]\n",
    "\n",
    "    def exclude_document(self, doc_id: str, doc_type: str) -> None:\n",
    "        \"\"\"\n",
    "        Excludes a document by its id and type.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): The ID of the document to exclude.\n",
    "            doc_type (str): The type of the document.\n",
    "        \"\"\"\n",
    "        key = (doc_id, doc_type)\n",
    "        if key in self.documents:\n",
    "            del self.documents[key]\n",
    "\n",
    "    def persist_to_file(self, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Persists the current state to a file, converting tuple keys to strings and sets to lists.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The filename to save the documents.\n",
    "        \"\"\"\n",
    "        serializable_documents = {f\"{doc_id}|{doc_type}\": convert_set_to_list(doc.dict()) for (doc_id, doc_type), doc in self.documents.items()}\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(serializable_documents, file, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def restore_from_file(cls, filename: str) -> 'DocumentManager':\n",
    "        \"\"\"\n",
    "        Restores the state from a file, converting string keys back to tuples.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The filename to restore the documents from.\n",
    "\n",
    "        Returns:\n",
    "            DocumentManager: The restored DocumentManager instance.\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            documents = {(doc_id.split('|')[0], doc_id.split('|')[1]): Document(**doc_data) for doc_id, doc_data in data.items()}\n",
    "            return cls(documents=documents)\n",
    "\n",
    "def restore_checkpoint(filename: Optional[str]) -> DocumentManager:\n",
    "    \"\"\"\n",
    "    Restores the document manager from a checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        filename (str, optional): The path to the checkpoint file. Defaults to DEFAULT_CHECKPOINT_FILE.\n",
    "\n",
    "    Returns:\n",
    "        DocumentManager: The restored DocumentManager instance.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the checkpoint file does not exist.\n",
    "\n",
    "    See Also:\n",
    "        - Reset the values delete the documents.json file and run: manager = DocumentManager()\n",
    "        - Restore the state from the documents.json file, run: DocumentManager.restore_from_file(\"documents.json\")\n",
    "        - Exclue a document: manager.exclude_document(doc_id=\"§ 275.0-2\", doc_type=\"section\")\n",
    "        - List documents: manager.list_document_ids(doc_type=\"section\")\n",
    "        - Get a document: manager.retrieve_document(doc_id=doc, doc_type=\"section\")\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        restored_docs = DocumentManager.restore_from_file(filename)\n",
    "        logger.info(f\"Checkpoint restored from {filename}.\")\n",
    "    except (FileNotFoundError, JSONDecodeError):\n",
    "        restored_docs = DocumentManager()\n",
    "        logger.error(f\"Checkpoint file '{filename}' not found or is empty, initializing new checkpoint.\")\n",
    "    return restored_docs\n",
    "\n",
    "def save_checkpoint(filename: Optional[str], manager: DocumentManager) -> None:\n",
    "    \"\"\"\n",
    "    Saves the current state of the DocumentManager to a checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        manager (DocumentManager): The DocumentManager instance to save.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error saving the checkpoint.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        manager.persist_to_file(filename=filename)\n",
    "        logger.info(\"Checkpoint saved.\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(\"Error saving checkpoint. Check the directory path and permissions.\")\n",
    "\n",
    "def get_all_checkpoints(checkpoint_dir, prefix=\"documents\", extension=\"json\"):\n",
    "    managers = []\n",
    "\n",
    "    path = Path(checkpoint_dir)\n",
    "\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = list(path.glob(f\"{file_prefix}-*.{extension}\"))\n",
    "    file_info_list = []\n",
    "\n",
    "    pattern = re.compile(rf'^{file_prefix}-(\\d{{4}}-\\d{{2}}-\\d{{2}})-(\\d+)\\.{extension}$')\n",
    "    for filepath in files:\n",
    "        match = pattern.match(filepath.name)\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            number = int(match.group(2))\n",
    "            file_info_list.append({'filename': filepath.name, 'date': date_str, 'number': number})\n",
    "            \n",
    "            print(filepath)\n",
    "            managers.append(manager.restore_from_file(filepath))\n",
    "    \n",
    "    return managers, file_info_list\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    DocumentProcessor is responsible for processing documents and categorizing elements such as terms, names, facts, and rules.\n",
    "\n",
    "    Attributes:\n",
    "        manager: Object used to manage document retrieval.\n",
    "        elements_terms_set (set): Set of unique terms found in the documents.\n",
    "        elements_names_set (set): Set of unique names found in the documents.\n",
    "        elements_terms (list): List of detailed information about terms.\n",
    "        elements_names (list): List of detailed information about names.\n",
    "        elements_facts (list): List of facts extracted from documents.\n",
    "        elements_rules (list): List of rules extracted from documents.\n",
    "        elements_terms_definition (dict): Dictionary to store terms definitions by document ID.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, manager):\n",
    "        \"\"\"\n",
    "        Initializes the DocumentProcessor instance and processes the documents.\n",
    "\n",
    "        Args:\n",
    "            manager: Object used to manage document retrieval.\n",
    "        \"\"\"\n",
    "        self.manager = manager\n",
    "        self.elements_terms_set = set()\n",
    "        self.elements_names_set = set()\n",
    "        self.elements_terms = []\n",
    "        self.elements_names = []\n",
    "        self.elements_facts = []\n",
    "        self.elements_rules = []\n",
    "        self.elements_terms_definition = {}\n",
    "        \n",
    "        # Automatically process definitions and elements when instantiated\n",
    "        self.process_definitions()\n",
    "        self.process_elements()\n",
    "\n",
    "    def add_definition(self, doc_id, term, definition):\n",
    "        \"\"\"\n",
    "        Adds a term definition to the elements_terms_definition dictionary.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): Identifier of the document.\n",
    "            term (str): The term to be defined.\n",
    "            definition (str): The definition of the term.\n",
    "        \"\"\"\n",
    "        self.elements_terms_definition.setdefault(doc_id, {})[term] = definition\n",
    "\n",
    "    def process_definitions(self):\n",
    "        \"\"\"\n",
    "        Processes document terms definitions and stores them in elements_terms_definition.\n",
    "        \"\"\"\n",
    "        docs_p2 = [s for s in self.manager.list_document_ids(doc_type=\"llm_response\") if s.endswith(\"_P2\")]\n",
    "\n",
    "        for doc in docs_p2:\n",
    "            doc_id = doc.replace(\"_P2\", \"\")\n",
    "            doc_content = self.manager.retrieve_document(doc, doc_type=\"llm_response\").content\n",
    "            doc_terms = doc_content.get(\"terms\", [])\n",
    "            for term in doc_terms:\n",
    "                self.add_definition(doc_id, term.get(\"term\"), term.get(\"definition\"))\n",
    "\n",
    "    def process_elements(self):\n",
    "        \"\"\"\n",
    "        Processes elements from documents and categorizes them into terms, names, facts, and rules.\n",
    "        \"\"\"\n",
    "        docs_p1 = [s for s in self.manager.list_document_ids(doc_type=\"llm_response\") if s.endswith(\"_P1\")]\n",
    "\n",
    "        for doc in docs_p1:\n",
    "            doc_content = self.manager.retrieve_document(doc, doc_type=\"llm_response\").content\n",
    "            doc_id = doc_content.get(\"section\")\n",
    "            doc_elements = doc_content.get(\"elements\", [])\n",
    "            for element in doc_elements:\n",
    "                element_classification = element.get(\"classification\")\n",
    "                element_id = element.get(\"id\")\n",
    "                verb_symbols = element.get(\"verb_symbols\") or element.get(\"verb_symbol\")\n",
    "                if isinstance(verb_symbols, str):\n",
    "                    verb_symbols = [verb_symbols]\n",
    "                elif verb_symbols is None:\n",
    "                    verb_symbols = []\n",
    "                element_dict = {\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"statement_id\": element_id,\n",
    "                    \"statement\": element.get(\"statement\"),\n",
    "                    \"source\": element.get(\"source\"),\n",
    "                    \"terms\": element.get(\"terms\", []),\n",
    "                    \"verb_symbols\": verb_symbols\n",
    "                }\n",
    "\n",
    "                match element_classification:\n",
    "                    case \"Fact\" | \"Fact Type\":\n",
    "                        self.elements_facts.append(element_dict)\n",
    "                    case \"Operative Rule\":\n",
    "                        self.elements_rules.append(element_dict)\n",
    "\n",
    "                element_terms = element.get(\"terms\", [])\n",
    "                if element_terms:\n",
    "                    for term in element_terms:\n",
    "                        signifier = term.get(\"term\")\n",
    "                        term_dict = {\n",
    "                            \"doc_id\": doc_id,\n",
    "                            \"signifier\": signifier,\n",
    "                            \"statement_id\": element_id,\n",
    "                            \"definition\": self.elements_terms_definition.get(doc_id, {}).get(signifier),\n",
    "                            \"source\": element.get(\"source\")\n",
    "                        }\n",
    "                        if term.get(\"classification\") == \"Common Noun\":\n",
    "                            self.elements_terms.append(term_dict)\n",
    "                            self.elements_terms_set.add(signifier)\n",
    "                        else:\n",
    "                            self.elements_names.append(term_dict)\n",
    "                            self.elements_names_set.add(signifier)\n",
    "\n",
    "    def get_unique_terms(self, doc_id=None):\n",
    "        \"\"\"\n",
    "        Returns the set of unique terms found in the documents. If doc_id is provided,\n",
    "        returns only the unique terms for that specific document.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str, optional): Identifier of the document. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            set: Set of unique terms.\n",
    "        \"\"\"\n",
    "        if doc_id:\n",
    "            return {term[\"signifier\"] for term in self.elements_terms if term[\"doc_id\"] == doc_id}\n",
    "        return self.elements_terms_set\n",
    "\n",
    "    def get_unique_names(self, doc_id=None):\n",
    "        \"\"\"\n",
    "        Returns the set of unique names found in the documents. If doc_id is provided,\n",
    "        returns only the unique names for that specific document.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str, optional): Identifier of the document. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            set: Set of unique names.\n",
    "        \"\"\"\n",
    "        if doc_id:\n",
    "            return {name[\"signifier\"] for name in self.elements_names if name[\"doc_id\"] == doc_id}\n",
    "        return self.elements_names_set\n",
    "\n",
    "    # def get_terms(self):\n",
    "    #     \"\"\"\n",
    "    #     Returns the list of terms with detailed information.\n",
    "\n",
    "    #     Returns:\n",
    "    #         list: List of terms.\n",
    "    #     \"\"\"\n",
    "    #     return self.elements_terms\n",
    "\n",
    "    # def get_names(self):\n",
    "    #     \"\"\"\n",
    "    #     Returns the list of names with detailed information.\n",
    "\n",
    "    #     Returns:\n",
    "    #         list: List of names.\n",
    "    #     \"\"\"\n",
    "    #     return self.elements_names\n",
    "\n",
    "    def get_terms(self, definition_filter=\"all\"):\n",
    "        \"\"\"\n",
    "        Returns the list of terms with detailed information, filtered by the presence of a definition.\n",
    "\n",
    "        Args:\n",
    "            definition_filter (str): Filter for terms based on definition presence. \n",
    "                                    \"non_null\" returns terms with definitions,\n",
    "                                    \"null\" returns terms without definitions,\n",
    "                                    \"all\" returns all terms regardless of definition.\n",
    "\n",
    "        Returns:\n",
    "            list: List of terms.\n",
    "        \"\"\"\n",
    "        if definition_filter == \"non_null\":\n",
    "            return [term for term in self.elements_terms if term.get(\"definition\") is not None]\n",
    "        elif definition_filter == \"null\":\n",
    "            return [term for term in self.elements_terms if term.get(\"definition\") is None]\n",
    "        return self.elements_terms\n",
    "\n",
    "    def get_names(self, definition_filter=\"all\"):\n",
    "        \"\"\"\n",
    "        Returns the list of names with detailed information, filtered by the presence of a definition.\n",
    "\n",
    "        Args:\n",
    "            definition_filter (str): Filter for names based on definition presence. \n",
    "                                    \"non_null\" returns names with definitions,\n",
    "                                    \"null\" returns names without definitions,\n",
    "                                    \"all\" returns all names regardless of definition.\n",
    "\n",
    "        Returns:\n",
    "            list: List of names.\n",
    "        \"\"\"\n",
    "        if definition_filter == \"non_null\":\n",
    "            return [name for name in self.elements_names if name.get(\"definition\") is not None]\n",
    "        elif definition_filter == \"null\":\n",
    "            return [name for name in self.elements_names if name.get(\"definition\") is None]\n",
    "        return self.elements_names\n",
    "\n",
    "\n",
    "    def get_facts(self):\n",
    "        \"\"\"\n",
    "        Returns the list of facts extracted from documents.\n",
    "\n",
    "        Returns:\n",
    "            list: List of facts.\n",
    "        \"\"\"\n",
    "        return self.elements_facts\n",
    "\n",
    "    def get_rules(self):\n",
    "        \"\"\"\n",
    "        Returns the list of rules extracted from documents.\n",
    "\n",
    "        Returns:\n",
    "            list: List of rules.\n",
    "        \"\"\"\n",
    "        return self.elements_rules\n",
    "\n",
    "    def get_term_info(self, doc_id, term):\n",
    "        \"\"\"\n",
    "        Retrieves information about a specific term from elements.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): Document identifier.\n",
    "            term (str): Term to retrieve information for.\n",
    "\n",
    "        Returns:\n",
    "            dict or None: A dictionary containing term information if found, otherwise None.\n",
    "        \"\"\"\n",
    "        definition = self.elements_terms_definition.get(doc_id, {}).get(term)\n",
    "        if definition:\n",
    "            for term_dict in self.elements_terms + self.elements_names:\n",
    "                if term_dict[\"doc_id\"] == doc_id and term_dict[\"signifier\"] == term:\n",
    "                    return {\n",
    "                        \"definition\": definition,\n",
    "                        \"source\": term_dict[\"source\"],\n",
    "                        \"statement_id\": term_dict[\"statement_id\"]\n",
    "                    }\n",
    "        return None\n",
    "\n",
    "    def get_name_info(self, doc_id, name):\n",
    "        \"\"\"\n",
    "        Retrieves information about a specific name from elements.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): Document identifier.\n",
    "            name (str): Name to retrieve information for.\n",
    "\n",
    "        Returns:\n",
    "            dict or None: A dictionary containing name information if found, otherwise None.\n",
    "        \"\"\"\n",
    "        for name_dict in self.elements_names:\n",
    "            if name_dict[\"doc_id\"] == doc_id and name_dict[\"signifier\"] == name:\n",
    "                return {\n",
    "                    \"definition\": name_dict.get(\"definition\"),\n",
    "                    \"source\": name_dict[\"source\"],\n",
    "                    \"statement_id\": name_dict[\"statement_id\"]\n",
    "                }\n",
    "        return None\n",
    "\n",
    "    def get_fact_info(self, doc_id, statement_id):\n",
    "        \"\"\"\n",
    "        Retrieves information about a specific fact from elements.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): Document identifier.\n",
    "            statement_id (str): statement identifier of the fact.\n",
    "\n",
    "        Returns:\n",
    "            dict or None: A dictionary containing fact information if found, otherwise None.\n",
    "        \"\"\"\n",
    "        for fact_dict in self.elements_facts:\n",
    "            if fact_dict[\"doc_id\"] == doc_id and fact_dict[\"statement_id\"] == statement_id:\n",
    "                terms = [term.get(\"term\") for term in fact_dict.get(\"terms\", []) if term.get(\"classification\") == \"Common Noun\"]\n",
    "                names = [term.get(\"term\") for term in fact_dict.get(\"terms\", []) if term.get(\"classification\") == \"Proper Noun\"]\n",
    "                return {\n",
    "                    \"statement\": fact_dict[\"statement\"],\n",
    "                    \"source\": fact_dict[\"source\"],\n",
    "                    \"terms\": terms,\n",
    "                    \"names\": names,\n",
    "                    \"verb_symbols\": fact_dict.get(\"verb_symbols\", [])\n",
    "                }\n",
    "        return None\n",
    "\n",
    "    def get_rule_info(self, doc_id, statement_id):\n",
    "        \"\"\"\n",
    "        Retrieves information about a specific rule from elements.\n",
    "\n",
    "        Args:\n",
    "            doc_id (str): Document identifier.\n",
    "            statement_id (str): statement identifier of the rule.\n",
    "\n",
    "        Returns:\n",
    "            dict or None: A dictionary containing rule information if found, otherwise None.\n",
    "        \"\"\"\n",
    "        for rule_dict in self.elements_rules:\n",
    "            if rule_dict[\"doc_id\"] == doc_id and rule_dict[\"statement_id\"] == statement_id:\n",
    "                terms = [term.get(\"term\") for term in rule_dict.get(\"terms\", []) if term.get(\"classification\") == \"Common Noun\"]\n",
    "                names = [term.get(\"term\") for term in rule_dict.get(\"terms\", []) if term.get(\"classification\") == \"Proper Noun\"]\n",
    "                return {\n",
    "                    \"statement\": rule_dict.get(\"statement\"),\n",
    "                    \"source\": rule_dict.get(\"source\"),\n",
    "                    \"terms\": terms,\n",
    "                    \"names\": names,\n",
    "                    \"verb_symbols\": rule_dict.get(\"verb_symbols\", [])\n",
    "                }\n",
    "        return None\n",
    "\n",
    "# # Example usage\n",
    "# processor = DocumentProcessor(manager)\n",
    "\n",
    "# # Access processed data\n",
    "# unique_terms = processor.get_unique_terms()\n",
    "# unique_names = processor.get_unique_names()\n",
    "# terms = processor.get_terms()\n",
    "# names = processor.get_names()\n",
    "# facts = processor.get_facts()\n",
    "# rules = processor.get_rules()\n",
    "\n",
    "# print(f\"Unique terms: {len(unique_terms)}\")\n",
    "# print(f\"Unique names: {len(unique_names)}\")\n",
    "\n",
    "# print(f'Rules from § 275.0-2: {processor.get_rule_info(\"§ 275.0-2\", 3)}')\n",
    "# print(f'Facts from § 275.0-2: {processor.get_fact_info(\"§ 275.0-2\", 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir logging_setup && touch logging_setup/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile logging_setup/main.py\n",
    "\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "\n",
    "\n",
    "def setting_logging(log_path: str, log_level: str):\n",
    "    # Ensure the ../logs directory exists\n",
    "    log_directory = Path.cwd() / log_path\n",
    "    log_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Path for the log file\n",
    "    log_file_path = log_directory / \"application.log\"\n",
    "\n",
    "    # Set up TimedRotatingFileHandler to rotate logs every day\n",
    "    file_handler = TimedRotatingFileHandler(\n",
    "        log_file_path,\n",
    "        when=\"midnight\",\n",
    "        interval=1,\n",
    "        backupCount=0,  # Rotate every midnight, keep all backups\n",
    "    )\n",
    "\n",
    "    # Set the file handler's log format\n",
    "    file_handler.setFormatter(\n",
    "        logging.Formatter(\n",
    "            \"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Set up logging configuration\n",
    "    logging.basicConfig(\n",
    "        level=log_level,  # Set to the desired log level\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Console log format\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",  # Custom date format\n",
    "        handlers=[\n",
    "            file_handler,  # Log to the rotating file in ../logs\n",
    "            logging.StreamHandler(),  # Log to console\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Example logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Log a test message to verify\n",
    "    logger.info(\"Logging is set up with daily rotation.\")\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir token_estimator && touch token_estimator/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile token_estimator/main.py\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def estimate_tokens(text, model=\"gpt-4o\"):\n",
    "    \"\"\"\n",
    "    Estimates the number of tokens in a given text using the OpenAI `tiktoken` library, \n",
    "    which closely approximates the tokenization method used by OpenAI language models.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to be tokenized and counted.\n",
    "        model (str): The model to use for tokenization. Defaults to \"gpt-4o\".\n",
    "                     Supported models include \"gpt-3.5-turbo\" and \"gpt-4o\".\n",
    "\n",
    "    Returns:\n",
    "        int: The estimated number of tokens in the text.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the specified model is not supported by `tiktoken`.\n",
    "\n",
    "    Example:\n",
    "        >>> text = \"This is a sample text.\"\n",
    "        >>> estimate_tokens_tiktoken(text)\n",
    "        6\n",
    "    \"\"\"\n",
    "    # Load the appropriate tokenizer\n",
    "    try:\n",
    "        tokenizer = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Model '{model}' is not supported by tiktoken.\")\n",
    "    \n",
    "    # Tokenize the text and return the token count\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir rules_taxonomy_provider && touch rules_taxonomy_provider/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rules_taxonomy_provider/main.py\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "class RuleInformationProvider:\n",
    "    \"\"\"\n",
    "    A class to provide information about rule classifications and templates based on YAML data.\n",
    "\n",
    "    This class loads and processes rule classification data, template data, and example data from specified YAML files.\n",
    "    It is used to generate markdown documentation for a given rule type, including details such as templates and examples.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    data_path : str\n",
    "        Path to the directory containing the YAML files.\n",
    "    template_dict : dict\n",
    "        Dictionary containing template information loaded from the templates YAML file.\n",
    "    examples_dict : dict\n",
    "        Dictionary containing example information loaded from the examples YAML file.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Initializes the RuleInformationProvider with the specified data path.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_path : str\n",
    "            Path to the directory containing the YAML files with rules, templates, and examples.\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.template_dict = self._load_yaml(f'{data_path}/witt_templates.yaml', 'template_list')\n",
    "        self.examples_dict = self._load_yaml(f'{data_path}/witt_examples.yaml', 'example_list')\n",
    "\n",
    "    def _load_yaml(self, file_path, list_key=None):\n",
    "        \"\"\"\n",
    "        Loads data from a YAML file.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        file_path : str\n",
    "            Path to the YAML file to be loaded.\n",
    "        list_key : str, optional\n",
    "            Key used to extract a specific list from the YAML data. If provided, returns a dictionary indexed by 'id'.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            If list_key is provided, returns a dictionary with items indexed by 'id'.\n",
    "        Any type\n",
    "            If list_key is not provided, returns the entire data structure from the YAML file.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = yaml.safe_load(file)\n",
    "            if list_key:\n",
    "                return {item['id']: item for item in data[list_key]}\n",
    "            return data\n",
    "\n",
    "    def get_classification_and_templates(self, section_title):\n",
    "        \"\"\"\n",
    "        Retrieves classification information and templates for a specified rule section.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        section_title : str\n",
    "            Title of the section for which to retrieve information.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A markdown formatted string containing the classification details, templates, and examples for the given section.\n",
    "        \"\"\"\n",
    "        data = self._load_yaml(f'{self.data_path}/classify_subtypes.yaml')\n",
    "        filtered_data = self._filter_sections_by_title(data, section_title)\n",
    "        return self._convert_to_markdown(filtered_data)\n",
    "\n",
    "    def _filter_sections_by_title(self, data, title):\n",
    "        \"\"\"\n",
    "        Filters sections based on the given title.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : list\n",
    "            List of sections to filter from.\n",
    "        title : str\n",
    "            Title to filter sections by.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            A list of sections that match the given title.\n",
    "        \"\"\"\n",
    "        return [section for section in data if section['section_title'] == title]\n",
    "\n",
    "    def _convert_to_markdown(self, filtered_data):\n",
    "        \"\"\"\n",
    "        Converts filtered rule classification data to markdown format.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        filtered_data : list\n",
    "            List of filtered sections to convert into markdown.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A markdown formatted string representing the filtered sections.\n",
    "        \"\"\"\n",
    "        def process_section(section, level=2):\n",
    "            \"\"\"\n",
    "            Processes a section recursively and converts it to markdown format.\n",
    "\n",
    "            Parameters:\n",
    "            -----------\n",
    "            section : dict\n",
    "                The section to process.\n",
    "            level : int, optional\n",
    "                The heading level for the section title in markdown (default is 1).\n",
    "\n",
    "            Returns:\n",
    "            --------\n",
    "            str\n",
    "                A markdown formatted string for the section and its subsections.\n",
    "            \"\"\"\n",
    "            markdown = f\"{'#' * level} {section['section_title']}\\n\\n\"\n",
    "            markdown += f\"**ID**: {section['section_id']}\\n\\n\"\n",
    "            markdown += f\"**Definition**: {section['section_definition']}\\n\\n\"\n",
    "\n",
    "            if 'templates' in section and section['templates']:\n",
    "                for template_id in section['templates']:\n",
    "                    if template_id in self.template_dict:\n",
    "                        template = self.template_dict[template_id]\n",
    "                        markdown += f\"**Template ID**: {template_id}\\n\\n\"\n",
    "                        markdown += f\"**Template Explanation**: {template['explanation']}\\n\\n\"\n",
    "                        markdown += f\"**Template Text**:\\n\\n```template\\n{template['text']}```\\n\\n\"\n",
    "                    else:\n",
    "                        markdown += f\"**Template ID**: {template_id} - No details found.\\n\\n\"\n",
    "            else:\n",
    "                markdown += \"**Templates**: Look in the subsection(s).\\n\\n\"\n",
    "\n",
    "            if 'examples' in section and section['examples']:\n",
    "                for example_id in section['examples']:\n",
    "                    if example_id in self.examples_dict:\n",
    "                        example = self.examples_dict[example_id]\n",
    "                        markdown += f\"**Example ID**: {example_id}\\n\\n\"\n",
    "                        markdown += f\"**Example Text**:\\n\\n```example\\n{example['text']}```\\n\\n\"\n",
    "                    else:\n",
    "                        markdown += f\"**Example ID**: {example_id} - No details found.\\n\\n\"\n",
    "\n",
    "            if 'subsections' in section:\n",
    "                for subsection in section['subsections']:\n",
    "                    markdown += process_section(subsection, level + 1)\n",
    "\n",
    "            return markdown\n",
    "\n",
    "        markdown = \"\"\n",
    "        for section in filtered_data:\n",
    "            markdown += process_section(section)\n",
    "        return markdown\n",
    "\n",
    "class RulesTemplateProvider:\n",
    "    \"\"\"\n",
    "    A class to provide information about rules templates and their relationships from YAML data.\n",
    "\n",
    "    This class loads and processes template data, subtemplate data, and their relationships from specified YAML files.\n",
    "    It is used to extract information about templates and format them into readable output.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    data_directory : Path\n",
    "        Path to the directory containing the YAML files.\n",
    "    data_dicts : dict\n",
    "        Dictionary containing data loaded from YAML files, including templates, subtemplates, and relationships.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_directory):\n",
    "        \"\"\"\n",
    "        Initializes the RulesTemplateProvider with the specified data directory.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_directory : str or Path\n",
    "            Path to the directory containing the YAML files with templates, subtemplates, and relationships.\n",
    "        \"\"\"\n",
    "        self.data_directory = Path(data_directory)\n",
    "        self.data_dicts = self._load_data()\n",
    "\n",
    "    def _load_yaml(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads data from a YAML file.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        file_path : Path\n",
    "            Path to the YAML file to be loaded.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            A dictionary containing the data from the YAML file.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r') as file:\n",
    "            return yaml.safe_load(file) or {}\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "        Loads data from multiple YAML files required for template processing.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            A dictionary containing data from templates, subtemplates, and template relationships YAML files.\n",
    "        \"\"\"\n",
    "        witt_template_relationship_file = self.data_directory / 'witt_template_subtemplate_relationship.yaml'\n",
    "        witt_templates_file = self.data_directory / 'witt_templates.yaml'\n",
    "        witt_subtemplates_file = self.data_directory / 'witt_subtemplates.yaml'\n",
    "\n",
    "        witt_template_relationship_data = self._load_yaml(witt_template_relationship_file).get('template_subtemplate_relationship', {})\n",
    "        witt_templates_data = self._load_yaml(witt_templates_file).get('template_list', [])\n",
    "        witt_subtemplates_data = self._load_yaml(witt_subtemplates_file).get('subtemplate_list', [])\n",
    "\n",
    "        return {\n",
    "            'witt_template_relationship_data': witt_template_relationship_data,\n",
    "            'witt_templates_data': witt_templates_data,\n",
    "            'witt_subtemplates_data': witt_subtemplates_data\n",
    "        }\n",
    "\n",
    "    def _get_template_data(self, template_key, data):\n",
    "        \"\"\"\n",
    "        Retrieves data for a specific template or subtemplate based on its key.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        template_key : str\n",
    "            The key of the template or subtemplate to be retrieved.\n",
    "        data : list or dict\n",
    "            The data to search in, which can be a list of templates or a dictionary of relationships.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict or None\n",
    "            The data corresponding to the specified template key, or None if not found.\n",
    "        \"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            return data.get(template_key, None)\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                if isinstance(item, dict) and item.get('id', '') == template_key:\n",
    "                    return item\n",
    "        return None\n",
    "\n",
    "    def _format_template_output(self, template_key, template_data):\n",
    "        \"\"\"\n",
    "        Formats the output for a given template or subtemplate.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        template_key : str\n",
    "            The key of the template or subtemplate.\n",
    "        template_data : dict\n",
    "            The data of the template or subtemplate to be formatted.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A formatted string representation of the template data.\n",
    "        \"\"\"\n",
    "        output = f\"# {template_key}\\n\\n\"\n",
    "        if not template_data:\n",
    "            output += \"Template data not found.\\n\\n\"\n",
    "            return output\n",
    "        if 'usesSubtemplate' in template_data:\n",
    "            uses_subtemplate = template_data['usesSubtemplate']\n",
    "            if isinstance(uses_subtemplate, list):\n",
    "                uses_subtemplate = ', '.join(uses_subtemplate)\n",
    "            output += f\"## usesSubtemplate\\n{uses_subtemplate}\\n\\n\"\n",
    "        if 'text' in template_data:\n",
    "            output += f\"## text\\n\\n{template_data['text']}\\n\\n\"\n",
    "        if 'explanation' in template_data:\n",
    "            output += f\"## explanation\\n\\n{template_data['explanation']}\\n\\n\"\n",
    "        return output\n",
    "\n",
    "    def _process_template(self, template_key, processed_keys=None):\n",
    "        \"\"\"\n",
    "        Processes a template or subtemplate recursively, including any subtemplates used.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        template_key : str\n",
    "            The key of the template or subtemplate to be processed.\n",
    "        processed_keys : set, optional\n",
    "            A set of keys that have already been processed to prevent circular references.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A formatted string representation of the template and its subtemplates.\n",
    "        \"\"\"\n",
    "        if processed_keys is None:\n",
    "            processed_keys = set()\n",
    "\n",
    "        if template_key in processed_keys:\n",
    "            return ''\n",
    "        processed_keys.add(template_key)\n",
    "\n",
    "        template_data = None\n",
    "\n",
    "        if template_key.startswith('T'):\n",
    "            template_data = self._get_template_data(template_key, self.data_dicts['witt_templates_data']) or {}\n",
    "            uses_subtemplate = self._get_template_data(template_key, self.data_dicts['witt_template_relationship_data'])\n",
    "            if uses_subtemplate:\n",
    "                template_data['usesSubtemplate'] = uses_subtemplate if isinstance(uses_subtemplate, list) else [uses_subtemplate]\n",
    "        elif template_key.startswith('S'):\n",
    "            template_data = self._get_template_data(template_key, self.data_dicts['witt_subtemplates_data']) or {}\n",
    "            uses_subtemplate = self._get_template_data(template_key, self.data_dicts['witt_template_relationship_data'])\n",
    "            if uses_subtemplate:\n",
    "                template_data['usesSubtemplate'] = uses_subtemplate if isinstance(uses_subtemplate, list) else [uses_subtemplate]\n",
    "\n",
    "        if not template_data:\n",
    "            return f\"# {template_key}\\n\\nTemplate data not found.\\n\\n\"\n",
    "\n",
    "        output = self._format_template_output(template_key, template_data)\n",
    "\n",
    "        if 'usesSubtemplate' in template_data:\n",
    "            subtemplate_keys = template_data['usesSubtemplate']\n",
    "            subtemplate_keys = [subtemplate_keys] if isinstance(subtemplate_keys, str) else subtemplate_keys\n",
    "            for sub_key in subtemplate_keys:\n",
    "                sub_key = sub_key.strip()\n",
    "                output += self._process_template(sub_key, processed_keys)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_rules_template(self, template_key):\n",
    "        \"\"\"\n",
    "        Retrieves the formatted rules template for the specified template key.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        template_key : str\n",
    "            The key of the template to be retrieved.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A formatted string representation of the template and its associated subtemplates.\n",
    "        \"\"\"\n",
    "        return self._process_template(template_key)\n",
    "\n",
    "# Example usage:\n",
    "# rule_information_provider = RuleInformationProvider(\"../data\")\n",
    "# markdown_data = rule_provider.get_classification_and_templates(\"Data rules\")\n",
    "# print(markdown_data)\n",
    "\n",
    "# rule_template_provider = RulesTemplateProvider(\"../data\")\n",
    "# markdown_data = processor.get_rules_template(\"T7\")\n",
    "# print(markdown_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llm_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir llm_query && touch llm_query/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llm_query/main.py\n",
    "\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import instructor\n",
    "import logging\n",
    "from typing import Any\n",
    "\n",
    "# Set up basic logging configuration for the checkpoint module\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to INFO or another level as needed\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Log format\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def measure_time(func):\n",
    "    \"\"\"\n",
    "    Decorator to measure the execution time of a function.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed_time = end_time - start_time\n",
    "        logger.info(f\"Execution time for {func.__name__}: {elapsed_time:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@measure_time\n",
    "def query_instruct_llm(system_prompt: str,\n",
    "                        user_prompt: str,\n",
    "                        llm_model: str,\n",
    "                        document_model: Any,\n",
    "                        temperature: float,\n",
    "                        max_tokens: int) -> Any:\n",
    "    \"\"\"\n",
    "    Queries the LLM with the given system and user prompts.\n",
    "\n",
    "    Args:\n",
    "        system_prompt (str): The system prompt to set the context for the LLM.\n",
    "        user_prompt (str): The user prompt containing the text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        Any: The response from the LLM, parsed into a document_model object.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the API call fails.\n",
    "    \"\"\"\n",
    "    client = instructor.from_openai(OpenAI())\n",
    "    resp = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        response_model=document_model,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "    )\n",
    "    return resp\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipt-cfr2sbvr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
