{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/asantos2000/master-degree-santos-anderson/blob/main/code/src/chap_6_nlp2sbvr_transform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lwF6l0TKfK0"
   },
   "source": [
    "# nlp2sbvr - Transformação para SBVR\n",
    "\n",
    "Chapter 6. Ferramentas de suporte\n",
    "- Section 6.2 Implementação dos principais componentes\n",
    "  - Section 6.2.4 nlp2sbvr\n",
    "    - Section Algoritmo \"Transformação para SBVR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  !rm -rf cfr2sbvr configuration checkpoint\n",
    "  !git clone https://github.com/asantos2000/master-degree-santos-anderson.git cfr2sbvr\n",
    "  %pip install -r cfr2sbvr/code/requirements.txt\n",
    "  !cp -r cfr2sbvr/code/src/configuration .\n",
    "  !cp -r cfr2sbvr/code/src/checkpoint .\n",
    "  !cp -r cfr2sbvr/code/config.colab.yaml config.yaml\n",
    "  DEFAULT_CONFIG_FILE=\"config.yaml\"\n",
    "else:\n",
    "  DEFAULT_CONFIG_FILE=\"../config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "\n",
    "# Third-party libraries\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Any\n",
    "\n",
    "# Local application/library-specific imports\n",
    "import checkpoint.main as checkpoint\n",
    "from checkpoint.main import (\n",
    "    save_checkpoint,\n",
    "    restore_checkpoint,\n",
    "    DocumentProcessor,\n",
    "    Document,\n",
    ")\n",
    "import configuration.main as configuration\n",
    "import logging_setup.main as logging_setup\n",
    "import rules_taxonomy_provider.main as rules_taxonomy_provider\n",
    "from rules_taxonomy_provider.main import RulesTemplateProvider\n",
    "import llm_query.main as llm_query\n",
    "from llm_query.main import query_instruct_llm\n",
    "\n",
    "DEV_MODE = True\n",
    "\n",
    "if DEV_MODE:\n",
    "    # Development mode\n",
    "    import importlib\n",
    "\n",
    "    importlib.reload(configuration)\n",
    "    importlib.reload(logging_setup)\n",
    "    importlib.reload(checkpoint)\n",
    "    importlib.reload(rules_taxonomy_provider)\n",
    "    importlib.reload(llm_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Default settings, check them before run the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "config = configuration.load_config(DEFAULT_CONFIG_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated files for analysis in this run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config[\"DEFAULT_CHECKPOINT_FILE\"],\n",
    "config[\"DEFAULT_EXCEL_FILE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging_setup.setting_logging(config[\"DEFAULT_LOG_DIR\"], config[\"LOG_LEVEL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints\n",
    "\n",
    "Documents, annoted datasets, statistics and metrics about the execution of the notebook are stored by checkpoint module.\n",
    "\n",
    "Checkpoints are stored / retrieved at the directory `DEFAULT_CHECKPOINT_FILE` in the configuration file.\n",
    "\n",
    "During the execution, it will restore the checkpoint at the beginning of the section and saved at the end. We can run and restore the checkpoint several times. If the run fails, check the closest checkpoint and restore it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run after classification\n",
    "last_checkpoint = configuration.get_last_filename(config[\"DEFAULT_CHECKPOINT_DIR\"], \"documents\", \"json\")\n",
    "\n",
    "logger.info(f\"{last_checkpoint=}\")\n",
    "\n",
    "config[\"DEFAULT_CHECKPOINT_FILE\"] = last_checkpoint\n",
    "\n",
    "manager = restore_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General functions and data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prompts_samples(system_prompts, user_prompts, element_name, manager):\n",
    "\n",
    "    if system_prompts:\n",
    "        manager.add_document(\n",
    "            Document(\n",
    "                id=f\"prompt-system-transform_rules_{element_name.replace(' ', '_')}\",\n",
    "                type=\"prompt\",\n",
    "                content=system_prompts[0],\n",
    "            )\n",
    "        )\n",
    "        logger.info(f\"System prompts for {element_name}s: {len(system_prompts)}\")\n",
    "        save_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"], manager=manager)\n",
    "\n",
    "    if user_prompts:\n",
    "        manager.add_document(\n",
    "            Document(\n",
    "                id=f\"prompt-user-transform_rules_{element_name.replace(' ', '_')}\",\n",
    "                type=\"prompt\",\n",
    "                content=user_prompts[0],\n",
    "            )\n",
    "        )\n",
    "        logger.info(f\"User prompts for {element_name}s: {len(user_prompts)}\")\n",
    "        save_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"], manager=manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM response model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedStatement(BaseModel):\n",
    "    doc_id: str = Field(..., description=\"Document ID associated with the statement.\")\n",
    "    statement_id: str = Field(..., description=\"A provided string that identifies the statement. e.g., '1', 'Person'.\")\n",
    "    statement_title: str = Field(..., description=\"Title of the statement.\") \n",
    "    statement: str = Field(..., description=\"The statement to be transformed.\")\n",
    "    statement_sources: List[str] = Field(..., description=\"Sources of the statement.\")\n",
    "    templates_ids: List[str] = Field(..., description=\"List of template IDs.\")\n",
    "    transformed: str = Field(..., description=\"The transformed statement.\")\n",
    "    confidence: float = Field(..., description=\"Confidence of the transformation.\")\n",
    "    reason: str = Field(..., description=\"Reason for confidence score of the transformation.\")\n",
    "\n",
    "class TransformedStatements(BaseModel):\n",
    "    TransformedStatements: List[TransformedStatement] = Field(..., description=\"List of transformed statements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_statement(element_name, user_prompts, system_prompts, manager):\n",
    "    # Check if there are prompts for the element\n",
    "    if not user_prompts or not system_prompts:\n",
    "        logger.info(f\"No prompts for {element_name}s.\")\n",
    "        return []\n",
    "    \n",
    "    # Initialize an empty list to accumulate all responses\n",
    "    all_responses = []\n",
    "    elapse_times = []\n",
    "    completions = []\n",
    "\n",
    "    # Loop through each pair of user and system prompts with a counter\n",
    "    for index, (user_prompt, system_prompt) in enumerate(\n",
    "        zip(user_prompts, system_prompts), start=1\n",
    "    ):\n",
    "        logger.info(f\"Processing transformation prompt {index} for {element_name}.\")\n",
    "        logger.debug(system_prompt)\n",
    "        logger.debug(user_prompt)\n",
    "\n",
    "        # Query the language model\n",
    "        response, completion, elapse_time = query_instruct_llm(\n",
    "            system_prompt=system_prompt,\n",
    "            user_prompt=user_prompt,\n",
    "            document_model=TransformedStatements,\n",
    "            llm_model=config[\"LLM\"][\"MODEL\"],\n",
    "            temperature=config[\"LLM\"][\"TEMPERATURE\"],\n",
    "            max_tokens=config[\"LLM\"][\"MAX_TOKENS\"],\n",
    "        )\n",
    "\n",
    "        logger.debug(response)\n",
    "\n",
    "        # Accumulate the responses in the list\n",
    "        all_responses.extend(response.TransformedStatements)\n",
    "        elapse_times.append(elapse_time)\n",
    "        completions.append(completion.dict())\n",
    "\n",
    "        logger.info(f\"Finished processing classification and templates prompt {index}.\")\n",
    "\n",
    "    # After the loop, create a single Document with all the accumulated responses\n",
    "    doc = Document(\n",
    "        id=f\"transform_{element_name.replace(' ', '_')}s\",\n",
    "        type=\"llm_response_transform\",\n",
    "        content=all_responses,\n",
    "        elapsed_times=elapse_times,\n",
    "        completions=completions,\n",
    "    )\n",
    "    manager.add_document(doc)\n",
    "\n",
    "    logger.info(f\"{element_name}s: {len(all_responses)}\")\n",
    "\n",
    "    return all_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_prompts_for_rule(rules, rule_template_formulation, data_dir):\n",
    "    rule_template_provider = RulesTemplateProvider(data_dir)\n",
    "\n",
    "    system_prompts = []\n",
    "    user_prompts = []\n",
    "    element_name = None\n",
    "\n",
    "    for rule in rules:\n",
    "        element_name = rule.get(\"element_name\")\n",
    "\n",
    "        # to-from mapping for elements\n",
    "        if element_name == [\"Term\", \"Name\"]:\n",
    "            statement_key = \"definition\"\n",
    "            statement_id_key = \"signifier\"\n",
    "        else:\n",
    "            statement_key = \"statement\"\n",
    "            statement_id_key = \"statement_id\"\n",
    "\n",
    "        # # Return templates and examples for fact types or all\n",
    "        # if element_name == \"Fact Type\":\n",
    "        #     return_forms = \"fact_type\"\n",
    "        # else:\n",
    "        #     return_forms = \"rule\"\n",
    "        # logger.info(f\"Processing {element_name} with return forms {return_forms}.\")\n",
    "\n",
    "        input_rule = {\n",
    "            \"doc_id\": rule[\"doc_id\"],\n",
    "            f\"{statement_id_key}\": rule[\"statement_id\"],\n",
    "            \"statement_title\": rule.get(\"statement_title\", rule.get(\"statement_id\")),\n",
    "            \"sources\": rule[\"sources\"],\n",
    "            f\"{statement_key}\": rule.get(\"statement\", rule.get(\"definition\")),\n",
    "            \"templates_ids\": rule[\"templates_ids\"],\n",
    "        }\n",
    "        user_prompt = get_user_prompt_transform(element_name, input_rule)\n",
    "        user_prompts.append(user_prompt)\n",
    "        rule_templates_subtemplates = rule_template_provider.get_rules_template(rule[\"templates_ids\"])\n",
    "        system_prompt = get_system_prompt_transform(element_name,rule_template_formulation, rule_templates_subtemplates)\n",
    "        system_prompts.append(system_prompt)\n",
    "        logger.debug(system_prompt)\n",
    "        logger.debug(user_prompt)\n",
    "    \n",
    "    \n",
    "    logger.info(f\"System prompts for {element_name}s: {len(system_prompts)}\")\n",
    "    logger.info(f\"User prompts for {element_name}s: {len(user_prompts)}\")\n",
    "\n",
    "    return system_prompts, user_prompts, element_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Datasets used in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True tables\n",
    "\n",
    "There are no true tables to evaluate the transformation, the evaluation depends on the algorithms SEMSCORE and \"LLM as a Judge\", see `chap_7_validation_rules_transformation.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements to transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get expressions to transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DocumentProcessor(manager, merge=True)\n",
    "\n",
    "pred_operative_rules = processor.get_rules()\n",
    "pred_facts = processor.get_facts()\n",
    "pred_terms = processor.get_terms(definition_filter=\"non_null\")\n",
    "pred_names = processor.get_names(definition_filter=\"non_null\")\n",
    "\n",
    "logger.debug(f\"Rules: {pred_operative_rules}\")\n",
    "logger.debug(f\"Facts: {pred_facts}\")\n",
    "logger.debug(f\"Terms: {pred_terms}\")\n",
    "logger.debug(f\"Names: {pred_names}\")\n",
    "logger.info(f\"Rules to evaluate: {len(pred_operative_rules)}\")\n",
    "logger.info(f\"Facts to evaluate: {len(pred_facts)}\")\n",
    "logger.info(f\"Terms to evaluate: {len(pred_terms)}\")\n",
    "logger.info(f\"Names to evaluate: {len(pred_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt engeneering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System prompt\n",
    "\n",
    "Formulation is expressed using a template (WITT, 2012, p. 162)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_template_formulation = \"\"\"\n",
    "# How to interpret the templates and subtemplates\n",
    "\n",
    "Each formulation is expressed using a template, in which the various symbols have the following meanings:\n",
    "\n",
    "1. Each item enclosed in \"angle brackets\" (\"<\" and \">\") is a placeholder, in place of which any suitable text may be substituted. For example, any of the following may be substituted in place of <operative rule statement subject> (subtemplate):\n",
    "    a. a term: for example, \"flight booking request\",\n",
    "    b. a term followed by a qualifying clause: for example, \"flight booking request for a one-way journey\",\n",
    "    c. a reference to a combination of items: for example, \"combination of enrollment date and graduation date\", with or without a qualifying clause,\n",
    "    d. a reference to a set of items: for example, \"set of passengers\", with or without a qualifying clause.\n",
    "2. Each pair of braces (\"{\" and \"}\") encloses a set of options (separated from each other by the bar symbol: \"|\"), one of which is included in the rule statement. For example,\n",
    "3. If a pair of braces includes a bar symbol immediately before the closing brace, the null option is allowed: that is, you can, if necessary, include none of the options at that point in the rule statement.\n",
    "4. Sets of options may be nested. For example, in each of the templates above\n",
    "    a. a conditional clause may be included or omitted,\n",
    "    b. if included, the conditional clause should be preceded by either \"if\" or \"unless\".\n",
    "5. A further notation, introduced later in this section, uses square brackets to indicate that a syntactic element may be repeated indefinitely.\n",
    "6. Any text not enclosed in either \"angle brackets\" or braces (i.e., \"must\", \"not\", \"may\", and \"only\") is included in every rule statement conforming to the relevant template.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_prompt_transform(element_name, rule_template_formulation, rule_templates_subtemplates):\n",
    "    statement_name = \"definition\" if element_name in [\"Term\", \"Name\"] else \"statement\"\n",
    "    return f\"\"\"\n",
    "Transform each given {element_name} {statement_name} into a structured format by matching it to the specified templates and subtemplates.\n",
    "\n",
    "# Steps\n",
    "\n",
    "1. **Summarize {statement_name}**: Summarize the given {element_name} {statement_name} to understand its structure and content.\n",
    "\n",
    "2. **Use Template**:\n",
    "   - For given expression, use the templates and subtemplates ({\"Fact Type Form\" if element_name in [\"Fact Type\", \"Fact\"] else \"Rule Form\"}) provided for transformation.\n",
    "   - Determine the appropriate template or subtemplate based on the structure of the expression.\n",
    "   \n",
    "3. **Replace Placeholders**:\n",
    "   - Substitute placeholders, such as `<term>`, `<verb phrase>`, `<conditional clause>`, etc., with suitable values as per the expression.\n",
    "   - For terms and names, the statement_id is the term defined by the statement.\n",
    "   \n",
    "4. **Include Qualifying Details**:\n",
    "   - Where placeholders, such as `<qualifying clause>`, require additional details (e.g., attributes or qualifiers to distinguish meaning), ensure that these are included appropriately as per the respective subtemplate.\n",
    "\n",
    "5. **Transform into Structured Format**:\n",
    "   - Once the transformation is complete, ensure it's in the correct template format.\n",
    "\n",
    "6. **Output as Structured JSON**:\n",
    "   - For every transformed expression generate a JSON object as per the specified output format.\n",
    "\n",
    "7. **Review and Validate**:\n",
    "   - Ensure accuracy in grammar and compliance with logical constructs when performing substitutions.\n",
    "   - Ensure the generated JSON is in the correct template format.\n",
    "\n",
    "8. **Assess the Transformation**:\n",
    "   - Record the confidence level and reason for the confidence score in the JSON object.\n",
    "\n",
    "{rule_template_formulation}\n",
    "\n",
    "# Provided templates and subtemplates for transformation\n",
    "\n",
    "{rule_templates_subtemplates}\n",
    "\n",
    "# Output Format\n",
    "\n",
    "[\n",
    "    {{\n",
    "      \"doc_id\": <doc_id>,\n",
    "      \"statement_id\": <statement_id or signifier>,\n",
    "      \"statement_title\": <statement_title>,\n",
    "      \"sources\": [<source>],\n",
    "      \"statement\": <statement or definition>,\n",
    "      \"templates_ids\": [<templates_id>],\n",
    "      \"transformed\": <transformed_statement>,\n",
    "      \"confidence\": <confidence_level>,\n",
    "      \"reason\": <reason_for_confidence>\n",
    "    }},\n",
    "    ...\n",
    "]\n",
    "\n",
    "- **`doc_id`**: A original identifier of the given document.\n",
    "- **`statement_id or signifier`**: The original identifier of the given {statement_name}. e.g., '1', 'Person'\".\n",
    "- **`statement_title`**: The title of the given {statement_name}.\n",
    "- **`sources`**: The original sources of the given {statement_name}.\n",
    "- **`statement or definition`**: The original text of the given {statement_name}.\n",
    "- **`templates_ids`**: The template(s) used for the transformation (e.g., T1, T2, etc.)\n",
    "- **`transformed`**: The transformed statement according to template.\n",
    "- **`confidence`**: The confidence level of the transformation range from 0 to 1.\n",
    "- **`reason`**: The reason for the confidence score.\n",
    "\n",
    "# Notes\n",
    "- Use only the provided templates and subtemplates for transformation.\n",
    "- If a placeholder within an expression is not applicable or optional, consider whether it should be omitted or replaced by a suitable value.\n",
    "- Each expression may involve nested levels of substitution as indicated by the subtemplate hierarchy (e.g., a qualifying clause that contains sub-elements).\n",
    "- Ensure accuracy in grammar and compliance with logical constructs when performing substitutions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt_transform(element_name, rule):\n",
    "\n",
    "    return f\"\"\"\n",
    "# Here's the {element_name} {\"definition\" if element_name in [\"Term\", \"Name\"] else \"statement\"} you need to transform using template {rule.get(\"templates_ids\")}.\n",
    "\n",
    "{json.dumps(rule, indent=2)}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the state to a file\n",
    "save_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"], manager=manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = restore_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operative rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get prompts for operative rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompts_operative_rules, user_prompts_operative_rules, element_name = (\n",
    "    get_prompts_for_rule(\n",
    "        rules=pred_operative_rules,\n",
    "        rule_template_formulation=rule_template_formulation,\n",
    "        data_dir=config[\"DEFAULT_DATA_DIR\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a sample of the system prompt and user prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prompts_samples(\n",
    "    system_prompts_operative_rules, user_prompts_operative_rules, element_name, manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call LLM to transform operative rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_operative_rules = transform_statement(\n",
    "    element_name=element_name,\n",
    "    user_prompts=user_prompts_operative_rules,\n",
    "    system_prompts=system_prompts_operative_rules,\n",
    "    manager=manager,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{responses_operative_rules=}\")\n",
    "\n",
    "# Persist the state to a file\n",
    "save_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"], manager=manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average execution time 5s per prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fact Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get prompts for facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompts_facts, user_prompts_facts, element_name = get_prompts_for_rule(\n",
    "    rules=pred_facts,\n",
    "    rule_template_formulation=rule_template_formulation,\n",
    "    data_dir=config[\"DEFAULT_DATA_DIR\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a sample of the system prompt and user prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prompts_samples(\n",
    "    system_prompts_facts, user_prompts_facts, element_name, manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call LLM to transform facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_facts = transform_statement(\n",
    "    element_name=element_name,\n",
    "    user_prompts=user_prompts_facts,\n",
    "    system_prompts=system_prompts_facts,\n",
    "    manager=manager,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{responses_facts=}\")\n",
    "\n",
    "# Persist the state to a file\n",
    "save_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"], manager=manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average execution time 5s per prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get prompts for facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompts_terms, user_prompts_terms, element_name = get_prompts_for_rule(\n",
    "    rules=pred_terms,\n",
    "    rule_template_formulation=rule_template_formulation,\n",
    "    data_dir=config[\"DEFAULT_DATA_DIR\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a sample of the system prompt and user prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prompts_samples(\n",
    "    system_prompts_terms, user_prompts_terms, element_name, manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call LLM to transform terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_terms = transform_statement(\n",
    "    element_name=element_name,\n",
    "    user_prompts=user_prompts_terms,\n",
    "    system_prompts=system_prompts_terms,\n",
    "    manager=manager,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{responses_terms=}\")\n",
    "\n",
    "# Persist the state to a file\n",
    "save_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"], manager=manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average execution time 4s per prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get prompts for names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompts_names, user_prompts_names, element_name = get_prompts_for_rule(\n",
    "    rules=pred_names,\n",
    "    rule_template_formulation=rule_template_formulation,\n",
    "    data_dir=config[\"DEFAULT_DATA_DIR\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a sample of the system prompt and user prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prompts_samples(\n",
    "    system_prompts_names, user_prompts_names, element_name, manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call LLM to transform names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_names = transform_statement(\n",
    "    element_name=element_name,\n",
    "    user_prompts=user_prompts_names,\n",
    "    system_prompts=system_prompts_names,\n",
    "    manager=manager,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{responses_names=}\")\n",
    "\n",
    "# Persist the state to a file\n",
    "save_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"], manager=manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average execution time 5s per prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check missing transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = restore_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate elements for missing transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DocumentProcessor(manager, merge=True)\n",
    "\n",
    "pred_operative_rules = processor.get_rules()\n",
    "pred_facts = processor.get_facts()\n",
    "pred_terms = processor.get_terms(definition_filter=\"non_null\")\n",
    "pred_names = processor.get_names(definition_filter=\"non_null\")\n",
    "\n",
    "logger.debug(f\"Rules: {pred_operative_rules}\")\n",
    "logger.debug(f\"Facts: {pred_facts}\")\n",
    "logger.debug(f\"Terms: {pred_terms}\")\n",
    "logger.debug(f\"Names: {pred_names}\")\n",
    "\n",
    "data = [pred_facts, pred_terms, pred_names, pred_operative_rules]\n",
    "data_names = [\"pred_facts\", \"pred_terms\", \"pred_names\", \"pred_operative_rules\"]\n",
    "\n",
    "logger.info(f\"Checkpoint: {config['DEFAULT_CHECKPOINT_FILE']}\")\n",
    "for element_list, element_name in zip(data, data_names):\n",
    "    empty_transformed_elements = []\n",
    "    for element in element_list:\n",
    "        if not element.get(\"transformed\"):\n",
    "            logger.debug(f\"{element_name} - {element.get('statement_id')}: {element.get('transformed')}\")\n",
    "            empty_transformed_elements.append(element)\n",
    "\n",
    "    logger.info(f\"Empty transformed: From {len(element_list)} {element_name} {len(empty_transformed_elements)} empty\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "For the the first parte (prompt_classify_p1), the assigned confidence levels reflect a calibrated approach to expressions involving multiple classifications where a dominant rule type is not explicitly evident. For instance, when an expression primarily constrains data (Data rule) but also includes specific parties (Party rule), a high confidence level is attributed to Data while a moderate confidence level is applied to Party, acknowledging its secondary relevance.\n",
    "\n",
    "Similarly, expressions referencing roles such as “Secretary” or “interested person” without explicit party restrictions are assigned moderate confidence for Party classification due to interpretive ambiguity. Procedural elements that impact data handling, such as document forwarding, receive high confidence for Data rules; however, a moderate confidence level is assigned for Activity rules when procedural references are indirect. This methodology prioritizes primary rule types while accounting for the interpretive limits of secondary classifications."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ipt-cfr2sbvr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
