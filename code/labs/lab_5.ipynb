{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lwF6l0TKfK0"
   },
   "source": [
    "# Lab 5 - nlp2sbvr\n",
    "\n",
    "Lab 5 - Initial version.\n",
    "\n",
    "Chapter 6. Ferramentas de suporte\n",
    "- Section 6.2 Implementação dos principais componentes\n",
    "  - Section 6.2.4 Transformação para SBVR\n",
    "    - Section Algoritmo \"nlp2sbvr\"\n",
    "    - Section Algoritmo \"elements association and creation\"\n",
    "    - Section Algoritmo \"define vocabular namespace\"\n",
    "    - Section Algoritmo \"similarity search\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  !rm -rf cfr2sbvr configuration checkpoint\n",
    "  !git clone https://github.com/asantos2000/master-degree-santos-anderson.git cfr2sbvr\n",
    "  %pip install -r cfr2sbvr/code/requirements.txt\n",
    "  !cp -r cfr2sbvr/code/src/configuration .\n",
    "  !cp -r cfr2sbvr/code/src/checkpoint .\n",
    "  !cp -r cfr2sbvr/code/config.colab.yaml config.yaml\n",
    "  DEFAULT_CONFIG_FILE=\"config.yaml\"\n",
    "else:\n",
    "  DEFAULT_CONFIG_FILE=\"../config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for labs\n",
    "import sys\n",
    "sys.path.append(r'../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from typing import List, Dict, Optional, Any, Tuple, Set\n",
    "\n",
    "# Local application/library-specific imports\n",
    "import checkpoint.main as checkpoint\n",
    "from checkpoint.main import (\n",
    "    normalize_str,\n",
    "    save_checkpoint,\n",
    "    restore_checkpoint,\n",
    "    get_all_checkpoints,\n",
    "    get_elements_from_checkpoints,\n",
    "    get_elements_from_true_tables,\n",
    "    get_true_table_files,\n",
    "    DocumentProcessor,\n",
    "    Document,\n",
    ")\n",
    "import configuration.main as configuration\n",
    "import logging_setup.main as logging_setup\n",
    "import token_estimator.main as token_estimator\n",
    "from token_estimator.main import estimate_tokens\n",
    "import rules_taxonomy_provider.main as rules_taxonomy_provider\n",
    "from rules_taxonomy_provider.main import RuleInformationProvider, RulesTemplateProvider\n",
    "import llm_query.main as llm_query\n",
    "from llm_query.main import query_instruct_llm\n",
    "\n",
    "DEV_MODE = True\n",
    "\n",
    "if DEV_MODE:\n",
    "    # Development mode\n",
    "    import importlib\n",
    "\n",
    "    importlib.reload(configuration)\n",
    "    importlib.reload(logging_setup)\n",
    "    importlib.reload(checkpoint)\n",
    "    importlib.reload(token_estimator)\n",
    "    importlib.reload(rules_taxonomy_provider)\n",
    "    importlib.reload(llm_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Default settings, check them before run the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "DEFAULT_CONFIG_FILE = \"../config.yaml\"\n",
    "config = configuration.load_config(DEFAULT_CONFIG_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated files for analysis in this run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/checkpoints/documents-2024-11-10-1.json ../outputs/extraction_report-2024-11-10-1.html ../outputs/compare_items_metrics.xlsx\n"
     ]
    }
   ],
   "source": [
    "print(config[\"DEFAULT_CHECKPOINT_FILE\"],\n",
    "config[\"DEFAULT_EXTRACTION_REPORT_FILE\"],\n",
    "config[\"DEFAULT_EXCEL_FILE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 17:42:57 - INFO - Logging is set up with daily rotation.\n"
     ]
    }
   ],
   "source": [
    "logger = logging_setup.setting_logging(config[\"DEFAULT_LOG_DIR\"], config[\"LOG_LEVEL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints\n",
    "\n",
    "Documents, annoted datasets, statistics and metrics about the execution of the notebook are stored by checkpoint module.\n",
    "\n",
    "Checkpoints are stored / retrieved at the directory `DEFAULT_CHECKPOINT_FILE` in the configuration file.\n",
    "\n",
    "During the execution, it will restore the checkpoint at the beginning of the section and saved at the end. We can run and restore the checkpoint several times. If the run fails, check the closest checkpoint and restore it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 17:54:14 - INFO - last_checkpoint='../data/checkpoints/documents-2024-11-01-3.json'\n",
      "2024-11-10 17:54:14 - INFO - DocumentManager restored from file: ../data/checkpoints/documents-2024-11-01-3.json\n",
      "2024-11-10 17:54:14 - INFO - Checkpoint restored from ../data/checkpoints/documents-2024-11-01-3.json.\n"
     ]
    }
   ],
   "source": [
    "# Restore the checkpoint\n",
    "\n",
    "# To run after classification\n",
    "last_checkpoint = configuration.get_last_filename(config[\"DEFAULT_CHECKPOINT_DIR\"], \"documents\", \"json\")\n",
    "\n",
    "logger.info(f\"{last_checkpoint=}\")\n",
    "\n",
    "config[\"DEFAULT_CHECKPOINT_FILE\"] = last_checkpoint\n",
    "\n",
    "manager = restore_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Datasets used in the notebook. They are divided into sections and true tables. The sections are the documents from CFR and true tables are annoted  or \"golden\" datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General functions and data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section_from_kg(conn: Any, section_num: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves a section from the Knowledge Graph based on the section number.\n",
    "\n",
    "    Args:\n",
    "        conn: The connection object to the Knowledge Graph.\n",
    "        section_num (str): The section number to query.\n",
    "\n",
    "    Returns:\n",
    "        str: The retrieved section content as a string.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error executing the query.\n",
    "    \"\"\"\n",
    "    # Query section number from KG\n",
    "    query = \"\"\"\n",
    "    PREFIX fro-cfr: <http://finregont.com/fro/cfr/Code_Federal_Regulations.ttl#>\n",
    "    PREFIX fro-leg-ref: <http://finregont.com/fro/ref/LegalReference.ttl#>\n",
    "\n",
    "    SELECT ?section ?section_seq ?section_num ?section_subject ?section_citation ?section_notes ?divide ?divide_seq ?paragraph_enum ?paragraph_text\n",
    "    WHERE {\n",
    "      ?section a fro-cfr:CFR_Section ;\n",
    "        fro-leg-ref:hasSequenceNumber ?section_seq ;\n",
    "        fro-cfr:hasSectionNumber ?section_num ;\n",
    "        fro-cfr:hasSectionSubject ?section_subject .\n",
    "      OPTIONAL {?section fro-leg-ref:refers_toNote ?section_notes} .\n",
    "      OPTIONAL {?section fro-cfr:hasSectionCitation ?section_citation} .\n",
    "\n",
    "      ?divide fro-leg-ref:divides ?section ; # rdf:type fro-cfr:CFR_Parapraph\n",
    "        fro-leg-ref:hasSequenceNumber ?divide_seq ;\n",
    "        fro-cfr:hasParagraphText ?paragraph_text ;\n",
    "        fro-leg-ref:hasSequenceNumber ?paragraph_seq .\n",
    "      OPTIONAL {?divide fro-cfr:hasParagraphEnumText ?paragraph_enum} .\n",
    "    \"\"\" + f\"\"\"\n",
    "      FILTER(\"{section_num}\" = ?section_num)\n",
    "    \"\"\" + \"\"\"\n",
    "    }\n",
    "    ORDER BY ?section_num ?section ?divide_seq\n",
    "    \"\"\"\n",
    "    tuple_query = conn.prepareTupleQuery(QueryLanguage.SPARQL, query)\n",
    "    result = tuple_query.evaluate()\n",
    "\n",
    "    logger.debug(f\"result.metadata: {result.metadata}\")\n",
    "    logger.debug(f\"result.variable_names: {result.variable_names}\")\n",
    "\n",
    "    body_text = \"\"\n",
    "    previous_section = None\n",
    "    previous_paragraph_id = None\n",
    "    with result:\n",
    "      for binding_set in result:\n",
    "          section = binding_set.getValue(\"section\")\n",
    "          section_seq = str(binding_set.getValue(\"section_seq\")).replace('\"', '')\n",
    "          section_num = str(binding_set.getValue(\"section_num\")).replace('\"', '')\n",
    "          section_subject = str(binding_set.getValue(\"section_subject\")).replace('\"', '')\n",
    "          section_citation = str(binding_set.getValue(\"section_citation\")).replace('\"', '')\n",
    "          section_notes = str(binding_set.getValue(\"section_notes\")).replace('\"', '')\n",
    "          divide = binding_set.getValue(\"divide\")\n",
    "          divide_seq = str(binding_set.getValue(\"divide_seq\")).replace('\"', '')\n",
    "          paragraph_enum = str(binding_set.getValue(\"paragraph_enum\")).replace('\"', '')\n",
    "          paragraph_text = str(binding_set.getValue(\"paragraph_text\")).replace('\"', '')\n",
    "          # Header\n",
    "          if previous_section != section:\n",
    "            previous_section = section\n",
    "            header = f\"\"\"\n",
    "    section_number: {section_num}\n",
    "    section_subject: {section_subject}\n",
    "    section_id: {section}\n",
    "    citations: {section_citation}\n",
    "    notes: {section_notes}\n",
    "            \"\"\"\n",
    "          # Body\n",
    "          if paragraph_enum != \"None\":\n",
    "            body_text += f\"\"\"\n",
    "    paragraph_enumeration: {paragraph_enum}\n",
    "    paragraph_text: {paragraph_text}\n",
    "    \"\"\"\n",
    "          else:\n",
    "            body_text += f\"\"\"\n",
    "    paragraph_text: {paragraph_text}\n",
    "    \"\"\"\n",
    "\n",
    "    return header + body_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prompts_samples(system_prompts, user_prompts, element_name, manager):\n",
    "    manager.add_document(\n",
    "        Document(\n",
    "            id=f\"prompt-system-transform_rules_{element_name.replace(' ', '_')}\",\n",
    "            type=\"prompt\",\n",
    "            content=system_prompts[0],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    manager.add_document(\n",
    "        Document(\n",
    "            id=f\"prompt-user-transform_rules_{element_name.replace(' ', '_')}\",\n",
    "            type=\"prompt\",\n",
    "            content=user_prompts[0],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    logger.info(f\"System prompts for {element_name}s: {len(system_prompts)}\")\n",
    "    logger.info(f\"User prompts for {element_name}s: {len(user_prompts)}\")\n",
    "\n",
    "    save_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"], manager=manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM response model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedStatement(BaseModel):\n",
    "    doc_id: str = Field(..., description=\"Document ID associated with the statement.\")\n",
    "    statement_id: str = Field(..., description=\"A provided string that identifies the statement. e.g., '1', 'Person'.\")\n",
    "    statement: str = Field(..., description=\"The statement to be transformed.\")\n",
    "    statement_source: str = Field(..., description=\"Source of the statement.\")\n",
    "    templates_ids: List[str] = Field(..., description=\"List of template IDs.\")\n",
    "    transformed: str = Field(..., description=\"The transformed statement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_statement(element_name, user_prompts, system_prompts, manager):\n",
    "    # Initialize an empty list to accumulate all responses\n",
    "    all_responses = []\n",
    "\n",
    "    # Loop through each pair of user and system prompts with a counter\n",
    "    for index, (user_prompt, system_prompt) in enumerate(\n",
    "        zip(user_prompts, system_prompts), start=1\n",
    "    ):\n",
    "        logger.info(f\"Processing transformation prompt {index} for {element_name}.\")\n",
    "        logger.debug(system_prompt)\n",
    "        logger.debug(user_prompt)\n",
    "\n",
    "        # Query the language model\n",
    "        response = query_instruct_llm(\n",
    "            system_prompt=system_prompt,\n",
    "            user_prompt=user_prompt,\n",
    "            document_model=List[TransformedStatement],\n",
    "            llm_model=config[\"LLM\"][\"MODEL\"],\n",
    "            temperature=config[\"LLM\"][\"TEMPERATURE\"],\n",
    "            max_tokens=config[\"LLM\"][\"MAX_TOKENS\"],\n",
    "        )\n",
    "\n",
    "        logger.debug(response)\n",
    "\n",
    "        # Accumulate the responses in the list\n",
    "        all_responses.extend(response)\n",
    "\n",
    "        logger.info(f\"Finished processing classification and templates prompt {index}.\")\n",
    "\n",
    "    # After the loop, create a single Document with all the accumulated responses\n",
    "    doc = Document(\n",
    "        id=f\"transform_{element_name.replace(' ', '_')}s\",\n",
    "        type=\"llm_response_transform\",\n",
    "        content=all_responses,\n",
    "    )\n",
    "    manager.add_document(doc)\n",
    "\n",
    "    logger.info(f\"{element_name}s: {len(all_responses)}\")\n",
    "\n",
    "    return all_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_prompts_for_rule(rules, rule_template_formulation, data_dir):\n",
    "    rule_template_provider = RulesTemplateProvider(data_dir)\n",
    "\n",
    "    system_prompts = []\n",
    "    user_prompts = []\n",
    "\n",
    "    for rule in rules:\n",
    "        element_name = rule.get(\"element_name\")\n",
    "\n",
    "        if element_name == [\"Term\", \"Name\"]:\n",
    "            statement_key = \"definition\"\n",
    "            statement_id_key = \"signifier\"\n",
    "        else:\n",
    "            statement_key = \"statement\"\n",
    "            statement_id_key = \"statement_id\"\n",
    "\n",
    "        input_rule = {\n",
    "            \"doc_id\": rule[\"doc_id\"],\n",
    "            f\"{statement_id_key}\": rule[\"statement_id\"],\n",
    "            \"source\": rule[\"source\"],\n",
    "            f\"{statement_key}\": rule.get(\"statement\", rule.get(\"definition\")),\n",
    "            \"templates_ids\": rule[\"templates_ids\"],\n",
    "        }\n",
    "        user_prompt = get_user_prompt_transform(element_name, input_rule)\n",
    "        user_prompts.append(user_prompt)\n",
    "        rule_templates_subtemplates = rule_template_provider.get_rules_template(rule[\"templates_ids\"])\n",
    "        system_prompt = get_system_prompt_transform(element_name,rule_template_formulation, rule_templates_subtemplates)\n",
    "        system_prompts.append(system_prompt)\n",
    "        logger.debug(system_prompt)\n",
    "        logger.debug(user_prompt)\n",
    "\n",
    "    logger.info(f\"System prompts for {element_name}s: {len(system_prompts)}\")\n",
    "    logger.info(f\"User prompts for {element_name}s: {len(user_prompts)}\")\n",
    "\n",
    "    return system_prompts, user_prompts, element_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True tables\n",
    "\n",
    "True tables are annotated or \"golden\" datasets in which entities have been manually identified and labeled within the original source data.\n",
    "\n",
    "True tables for sectiona 275.0-2, 275.0-5 and 275.0-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification true tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 14:36:42 - INFO - Adding classify_P1|true_table true table to the manager\n",
      "2024-11-10 14:36:42 - INFO - Adding classify_P2_Definitional_facts|true_table true table to the manager\n",
      "2024-11-10 14:36:42 - INFO - Adding classify_P2_Definitional_names|true_table true table to the manager\n",
      "2024-11-10 14:36:42 - INFO - Adding classify_P2_Definitional_terms|true_table true table to the manager\n",
      "2024-11-10 14:36:42 - INFO - Adding classify_P2_Operative_rules|true_table true table to the manager\n"
     ]
    }
   ],
   "source": [
    "true_table_files = get_true_table_files(config[\"DEFAULT_DATA_DIR\"])\n",
    "\n",
    "for item in true_table_files:\n",
    "    with open(item[\"path\"], 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "        logger.debug(data[item[\"id\"]])\n",
    "        logger.info(f\"Adding {item['id']} true table to the manager\")\n",
    "\n",
    "        # manager.add_document(\n",
    "        #     Document.model_validate(data[item[\"id\"]])\n",
    "        # )\n",
    "\n",
    "# # Persist the state to a file\n",
    "# save_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"], manager=manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements to transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get expressions to transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 17:33:27 - INFO - Rules to evaluate: 6\n",
      "2024-11-10 17:33:27 - INFO - Facts to evaluate: 17\n",
      "2024-11-10 17:33:27 - INFO - Terms to evaluate: 91\n",
      "2024-11-10 17:33:27 - INFO - Names to evaluate: 15\n"
     ]
    }
   ],
   "source": [
    "processor = DocumentProcessor(manager)\n",
    "\n",
    "pred_operative_rules = processor.get_rules()\n",
    "pred_facts = processor.get_facts()\n",
    "pred_terms = processor.get_terms(definition_filter=\"non_null\")\n",
    "pred_names = processor.get_names(definition_filter=\"non_null\")\n",
    "\n",
    "logger.debug(f\"Rules: {pred_operative_rules}\")\n",
    "logger.debug(f\"Facts: {pred_facts}\")\n",
    "logger.debug(f\"Terms: {pred_terms}\")\n",
    "logger.debug(f\"Names: {pred_names}\")\n",
    "logger.info(f\"Rules to evaluate: {len(pred_operative_rules)}\")\n",
    "logger.info(f\"Facts to evaluate: {len(pred_facts)}\")\n",
    "logger.info(f\"Terms to evaluate: {len(pred_terms)}\")\n",
    "logger.info(f\"Names to evaluate: {len(pred_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for duplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries found in true_terms based on specified fields:\n",
      "{'doc_id': '§ 275.0-5', 'statement_id': 'Matter', 'definition': \"The subject of the proceeding initiated by the application or Commission's motion.\", 'source': '(a)', 'element_name': 'Term', 'transformed': \"A Matter is by definition the subject of the proceeding that is initiated by the application or Commission's motion.\", 'type': 'Definitional', 'subtype': 'Formal intensional definitions', 'confidence': 0.9, 'explanation': \"The statement defines 'matter' by specifying it as the subject of a proceeding, which fits the formal intensional definition structure by using a hypernym and a distinguishing characteristic.\", 'templates_ids': ['T7']}\n"
     ]
    }
   ],
   "source": [
    "# Track seen items and duplicates\n",
    "seen_terms = set()\n",
    "duplicates = []\n",
    "\n",
    "# Check for duplicates by specified fields\n",
    "for term in pred_terms:\n",
    "    # Create a unique key from the specified fields\n",
    "    key = (term['doc_id'], term['statement_id'], term['definition'], term['source'])\n",
    "    if key in seen_terms:\n",
    "        duplicates.append(term)\n",
    "    else:\n",
    "        seen_terms.add(key)\n",
    "\n",
    "# Output results\n",
    "if duplicates:\n",
    "    print(\"Duplicate entries found in true_terms based on specified fields:\")\n",
    "    for dup in duplicates:\n",
    "        print(dup)\n",
    "else:\n",
    "    print(\"No duplicate entries found in true_terms based on specified fields.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt engeneering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nlp2sbvr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System prompt\n",
    "\n",
    "Formulation is expressed using a template (WITT, 2012, p. 162)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_template_formulation = \"\"\"\n",
    "# How to interpret the templates and subtemplates\n",
    "\n",
    "Each formulation is expressed using a template, in which the various symbols have the following meanings:\n",
    "\n",
    "1. Each item enclosed in \"angle brackets\" (\"<\" and \">\") is a placeholder, in place of which any suitable text may be substituted. For example, any of the following may be substituted in place of <operative rule statement subject> (subtemplate):\n",
    "    a. a term: for example, \"flight booking request\",\n",
    "    b. a term followed by a qualifying clause: for example, \"flight booking request for a one-way journey\",\n",
    "    c. a reference to a combination of items: for example, \"combination of enrollment date and graduation date\", with or without a qualifying clause,\n",
    "    d. a reference to a set of items: for example, \"set of passengers\", with or without a qualifying clause.\n",
    "2. Each pair of braces (\"{\" and \"}\") encloses a set of options (separated from each other by the bar symbol: \"|\"), one of which is included in the rule statement. For example,\n",
    "3. If a pair of braces includes a bar symbol immediately before the closing brace, the null option is allowed: that is, you can, if necessary, include none of the options at that point in the rule statement.\n",
    "4. Sets of options may be nested. For example, in each of the templates above\n",
    "    a. a conditional clause may be included or omitted,\n",
    "    b. if included, the conditional clause should be preceded by either \"if\" or \"unless\".\n",
    "5. A further notation, introduced later in this section, uses square brackets to indicate that a syntactic element may be repeated indefinitely.\n",
    "6. Any text not enclosed in either \"angle brackets\" or braces (i.e., \"must\", \"not\", \"may\", and \"only\") is included in every rule statement conforming to the relevant template.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_prompt_transform(element_name, rule_template_formulation, rule_templates_subtemplates):\n",
    "    return f\"\"\"\n",
    "Transform each given {element_name} {\"definition\" if element_name in [\"Term\", \"Name\"] else \"statement\"} into a structured format by matching it to the specified templates and subtemplates.\n",
    "\n",
    "# Steps\n",
    "\n",
    "1. **Use Template**:\n",
    "   - For given expression, use the templates and subtemplates provided for transformation.\n",
    "   - Determine the appropriate template or subtemplate based on the structure of the expression.\n",
    "   \n",
    "2. **Replace Placeholders**:\n",
    "   - Substitute placeholders, such as `<term>`, `<verb phrase>`, `<conditional clause>`, etc., with suitable values as per the expression.\n",
    "   - For terms and names, the statement_id is the term defined by the statement.\n",
    "   \n",
    "3. **Include Qualifying Details**:\n",
    "   - Where placeholders, such as `<qualifying clause>`, require additional details (e.g., attributes or qualifiers to distinguish meaning), ensure that these are included appropriately as per the respective subtemplate.\n",
    "\n",
    "4. **Transform into Structured Format**:\n",
    "   - Once the transformation is complete, ensure it's in the correct template format.\n",
    "\n",
    "5. **Output as Structured JSON**:\n",
    "   - For every transformed expression generate a JSON object as per the specified output format.\n",
    "\n",
    "{rule_template_formulation}\n",
    "\n",
    "# Provided templates and subtemplates for transformation\n",
    "\n",
    "{rule_templates_subtemplates}\n",
    "\n",
    "# Output Format\n",
    "\n",
    "[\n",
    "    {{\n",
    "      \"doc_id\": <doc_id>,\n",
    "      \"statement_id\": <statement_id or signifier>,\n",
    "      \"source\": <source>,\n",
    "      \"statement\": <statement or definition>,\n",
    "      \"templates_ids\": [<templates_id>],\n",
    "      \"transformed\": <transformed_statement>,\n",
    "    }},\n",
    "    ...\n",
    "]\n",
    "\n",
    "- **`doc_id`**: A original document identifier.\n",
    "- **`statement_id or signifier`**: The original statement or definition identifier. e.g., '1', 'Person'\".\n",
    "- **`source`**: The original source of the expression.\n",
    "- **`statement or definition`**: The original text of the given expression.\n",
    "- **`templates_ids`**: The exact template(s) to be used for transformation (e.g., T1, T2, etc.)\n",
    "- **`transformed`**: The transformation result, formatted according to the matching template and subtemplates.\n",
    "\n",
    "# Notes\n",
    "- Use only the provided templates and subtemplates for transformation.\n",
    "- If a placeholder within an expression is not applicable or optional, consider whether it should be omitted or replaced by a suitable value.\n",
    "- Each expression may involve nested levels of substitution as indicated by the subtemplate hierarchy (e.g., a qualifying clause that contains sub-elements).\n",
    "- Ensure accuracy in grammar and compliance with logical constructs when performing substitutions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt_transform(element_name, rule):\n",
    "\n",
    "    return f\"\"\"\n",
    "# Here's the {element_name} {\"definition\" if element_name in [\"Term\", \"Name\"] else \"statement\"} you need to transform using template {rule.get(\"templates_ids\")}.\n",
    "\n",
    "{json.dumps(rule, indent=2)}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 14:37:37 - INFO - DocumentManager state persisted to file: ../data/checkpoints/documents-2024-11-01-3.json\n",
      "2024-11-10 14:37:37 - INFO - Checkpoint saved.\n"
     ]
    }
   ],
   "source": [
    "# Persist the state to a file\n",
    "save_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"], manager=manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nlp2sbvr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 17:33:47 - INFO - DocumentManager restored from file: ../data/checkpoints/documents-2024-11-01-3.json\n",
      "2024-11-10 17:33:47 - INFO - Checkpoint restored from ../data/checkpoints/documents-2024-11-01-3.json.\n"
     ]
    }
   ],
   "source": [
    "manager = restore_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operative rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get prompts for operative rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 17:33:52 - INFO - System prompts for Operative Rules: 6\n",
      "2024-11-10 17:33:52 - INFO - User prompts for Operative Rules: 6\n"
     ]
    }
   ],
   "source": [
    "system_prompts_operative_rules, user_prompts_operative_rules, element_name = (\n",
    "    get_prompts_for_rule(\n",
    "        rules=pred_operative_rules,\n",
    "        rule_template_formulation=rule_template_formulation,\n",
    "        data_dir=config[\"DEFAULT_DATA_DIR\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a sample of the system prompt and user prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 14:37:53 - INFO - System prompts for Operative Rules: 6\n",
      "2024-11-10 14:37:53 - INFO - User prompts for Operative Rules: 6\n",
      "2024-11-10 14:37:53 - INFO - DocumentManager state persisted to file: ../data/checkpoints/documents-2024-11-01-3.json\n",
      "2024-11-10 14:37:53 - INFO - Checkpoint saved.\n"
     ]
    }
   ],
   "source": [
    "save_prompts_samples(\n",
    "    system_prompts_operative_rules, user_prompts_operative_rules, element_name, manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call LLM to transform operative rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_operative_rules = transform_statement(\n",
    "    element_name=element_name,\n",
    "    user_prompts=user_prompts_operative_rules,\n",
    "    system_prompts=system_prompts_operative_rules,\n",
    "    manager=manager,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{responses_operative_rules=}\")\n",
    "\n",
    "# Persist the state to a file\n",
    "save_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"], manager=manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average execution time 5s per prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get prompts for facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 14:38:05 - INFO - System prompts for Fact Types: 17\n",
      "2024-11-10 14:38:05 - INFO - User prompts for Fact Types: 17\n"
     ]
    }
   ],
   "source": [
    "system_prompts_facts, user_prompts_facts, element_name = get_prompts_for_rule(\n",
    "    rules=pred_facts,\n",
    "    rule_template_formulation=rule_template_formulation,\n",
    "    data_dir=config[\"DEFAULT_DATA_DIR\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a sample of the system prompt and user prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 14:38:07 - INFO - System prompts for Fact Types: 6\n",
      "2024-11-10 14:38:07 - INFO - User prompts for Fact Types: 6\n",
      "2024-11-10 14:38:07 - INFO - DocumentManager state persisted to file: ../data/checkpoints/documents-2024-11-01-3.json\n",
      "2024-11-10 14:38:07 - INFO - Checkpoint saved.\n"
     ]
    }
   ],
   "source": [
    "save_prompts_samples(\n",
    "    system_prompts_operative_rules, user_prompts_operative_rules, element_name, manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call LLM to transform facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_facts = transform_statement(\n",
    "    element_name=element_name,\n",
    "    user_prompts=user_prompts_facts,\n",
    "    system_prompts=system_prompts_facts,\n",
    "    manager=manager,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{responses_facts=}\")\n",
    "\n",
    "# Persist the state to a file\n",
    "save_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"], manager=manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average execution time 5s per prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get prompts for facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 14:39:29 - INFO - System prompts for Terms: 91\n",
      "2024-11-10 14:39:29 - INFO - User prompts for Terms: 91\n"
     ]
    }
   ],
   "source": [
    "system_prompts_terms, user_prompts_terms, element_name = get_prompts_for_rule(\n",
    "    rules=pred_terms,\n",
    "    rule_template_formulation=rule_template_formulation,\n",
    "    data_dir=config[\"DEFAULT_DATA_DIR\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a sample of the system prompt and user prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 14:39:35 - INFO - System prompts for Terms: 6\n",
      "2024-11-10 14:39:35 - INFO - User prompts for Terms: 6\n",
      "2024-11-10 14:39:35 - INFO - DocumentManager state persisted to file: ../data/checkpoints/documents-2024-11-01-3.json\n",
      "2024-11-10 14:39:35 - INFO - Checkpoint saved.\n"
     ]
    }
   ],
   "source": [
    "save_prompts_samples(\n",
    "    system_prompts_operative_rules, user_prompts_operative_rules, element_name, manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call LLM to transform facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_terms = transform_statement(\n",
    "    element_name=element_name,\n",
    "    user_prompts=user_prompts_terms,\n",
    "    system_prompts=system_prompts_terms,\n",
    "    manager=manager,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{responses_terms=}\")\n",
    "\n",
    "# Persist the state to a file\n",
    "save_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"], manager=manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average execution time 4s per prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get prompts for names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 14:39:45 - INFO - System prompts for Names: 15\n",
      "2024-11-10 14:39:45 - INFO - User prompts for Names: 15\n"
     ]
    }
   ],
   "source": [
    "system_prompts_names, user_prompts_names, element_name = get_prompts_for_rule(\n",
    "    rules=pred_names,\n",
    "    rule_template_formulation=rule_template_formulation,\n",
    "    data_dir=config[\"DEFAULT_DATA_DIR\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a sample of the system prompt and user prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 14:39:48 - INFO - System prompts for Names: 6\n",
      "2024-11-10 14:39:48 - INFO - User prompts for Names: 6\n",
      "2024-11-10 14:39:48 - INFO - DocumentManager state persisted to file: ../data/checkpoints/documents-2024-11-01-3.json\n",
      "2024-11-10 14:39:48 - INFO - Checkpoint saved.\n"
     ]
    }
   ],
   "source": [
    "save_prompts_samples(\n",
    "    system_prompts_operative_rules, user_prompts_operative_rules, element_name, manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call LLM to transform facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_names = transform_statement(\n",
    "    element_name=element_name,\n",
    "    user_prompts=user_prompts_names,\n",
    "    system_prompts=system_prompts_names,\n",
    "    manager=manager,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{responses_names=}\")\n",
    "\n",
    "# Persist the state to a file\n",
    "save_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"], manager=manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average execution time 5s per prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check missing transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 18:01:44 - INFO - DocumentManager restored from file: ../data/checkpoints/documents-2024-11-01-3.json\n",
      "2024-11-10 18:01:44 - INFO - Checkpoint restored from ../data/checkpoints/documents-2024-11-01-3.json.\n"
     ]
    }
   ],
   "source": [
    "manager = restore_checkpoint(filename=config[\"DEFAULT_CHECKPOINT_FILE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate elements for missing transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 18:04:06 - INFO - Empty transformed pred_facts: 0/17\n",
      "2024-11-10 18:04:06 - INFO - Empty transformed pred_terms: 0/91\n",
      "2024-11-10 18:04:06 - INFO - Empty transformed pred_names: 0/15\n",
      "2024-11-10 18:04:06 - INFO - Empty transformed pred_operative_rules: 0/6\n"
     ]
    }
   ],
   "source": [
    "processor = DocumentProcessor(manager)\n",
    "\n",
    "pred_operative_rules = processor.get_rules()\n",
    "pred_facts = processor.get_facts()\n",
    "pred_terms = processor.get_terms(definition_filter=\"non_null\")\n",
    "pred_names = processor.get_names(definition_filter=\"non_null\")\n",
    "\n",
    "logger.debug(f\"Rules: {pred_operative_rules}\")\n",
    "logger.debug(f\"Facts: {pred_facts}\")\n",
    "logger.debug(f\"Terms: {pred_terms}\")\n",
    "logger.debug(f\"Names: {pred_names}\")\n",
    "\n",
    "data = [pred_facts, pred_terms, pred_names, pred_operative_rules]\n",
    "data_names = [\"pred_facts\", \"pred_terms\", \"pred_names\", \"pred_operative_rules\"]\n",
    "\n",
    "for element_list, element_name in zip(data, data_names):\n",
    "    empty_transformed_elements = [element for element in element_list if not element['transformed']]\n",
    "    logger.info(f\"Empty transformed {element_name}: {len(empty_transformed_elements)}/{len(element_list)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "For the the first parte (prompt_classify_p1), the assigned confidence levels reflect a calibrated approach to expressions involving multiple classifications where a dominant rule type is not explicitly evident. For instance, when an expression primarily constrains data (Data rule) but also includes specific parties (Party rule), a high confidence level is attributed to Data while a moderate confidence level is applied to Party, acknowledging its secondary relevance. Similarly, expressions referencing roles such as “Secretary” or “interested person” without explicit party restrictions are assigned moderate confidence for Party classification due to interpretive ambiguity. Procedural elements that impact data handling, such as document forwarding, receive high confidence for Data rules; however, a moderate confidence level is assigned for Activity rules when procedural references are indirect. This methodology prioritizes primary rule types while accounting for the interpretive limits of secondary classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### elements association and creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define vocabular namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ipt-cfr2sbvr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
